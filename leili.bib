% Encoding: UTF-8

@Unpublished{bai2020self,
  author = {Hongxiao Bai and Mingxuan Wang and Hai Zhao and Lei Li},
  title  = {Self-Training with Heterogeneous Teachers Improves Neural Machine Translation},
  year   = {2020},
}

@Unpublished{bai2020triangular,
  author   = {Hongxiao Bai and Mingxuan Wang and Hai Zhao and Lei Li},
  title    = {Triangular Unsupervised Neural Machine Translation},
  year     = {2020},
  addendum = {in submission},
}

@Unpublished{bao2020pnat,
  author = {Yu Bao and Hao Zhou and Jiangtao Feng and Mingxuan Wang and Shujian Huang and Jiajun Chen and Lei Li},
  note   = {in submission},
  title  = {{PNAT}: Non-autoregressive Transformer by Position Learning},
  year   = {2020},
}

@Unpublished{dong2020doubly,
  author   = {Qianqian Dong and Mingxuan Wang and Hao Zhou and Zhen Yang and Bo Xu and Lei Li},
  title    = {Doubly Supervised Encoder for End-to-end Speech Translation},
  year     = {2020},
  addendum = {in submission},
}

@Unpublished{huang2020acutum,
  author = {Xunpeng Huang and Zhengyang Liu and Zhe Wang and Yue Yu and Lei Li},
  title  = {Acutum: an Adaptive Angular Regularized Optimization Algorithm},
  year   = {2020},
}

@Article{kong2020foveabox,
  author  = {Tao {Kong} and Fuchun {Sun} and Huaping {Liu} and Yuning {Jiang} and Lei {Li} and Jianbo {Shi}},
  journal = {IEEE Transactions on Image Processing},
  title   = {{FoveaBox}: Beyound Anchor-based Object Detection},
  year    = {2020},
  issn    = {1057-7149},
  pages   = {1-10},
  doi     = {10.1109/TIP.2020.3002345},
}

@Unpublished{li2020bidirectional,
  author = {Mingwei Li and Qingyuan Jiang and Yi He and Lei Li and Wujun Li},
  title  = {Bidirectional Attentive Convolutional Neural Network for Near-Duplicate Video Retrieval},
  year   = {2020},
}

@Unpublished{pan2020pretraining,
  author = {Xiao Pan and Mingxuan Wang and Jiangtao Feng and Hao Zhou and Lei Li},
  title  = {Pretraining Neural Machine Translation by Pretrained Neural Machine Translation},
  year   = {2020},
}

@Unpublished{ru2020active,
  author = {Dongyu Ru and Yating Luo and Lin Qiu and Hao Zhou and Lei Li and Weinan Zhang and Yong Yu},
  title  = {Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space},
  year   = {2020},
}

@Unpublished{sun2020diving,
  author = {Zewei Sun and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Shujian Huang and Jiajun Chen and Lei Li},
  title  = {Diving into Document-Level Neural Machine Translation},
  year   = {2020},
}

@Unpublished{tian2020conversational,
  author = {Youzhi Tian and Zhou Yu and Cheng Yang and Hang Li and Lei Li},
  title  = {Conversational Contextualized Multimodal Representation Learning},
  year   = {2020},
}

@Unpublished{wu2020textgail,
  author = {Qingyang Wu and Lei Li and Zhou Yu},
  title  = {TextGAIL: Generative Adversarial Imitation Learning for Text Generation},
  year   = {2020},
}

@Unpublished{yan2020cross,
  author = {An Yan and Xin Wang and Jiangtao Feng and Lei Li and William Yang Wang},
  title  = {Cross-Lingual Vision-Language Navigation},
  year   = {2020},
}

@InProceedings{song2020improving,
  author    = {Yuxuan Song and Ning Miao and Hao Zhou and Lantao Yu and Mingxuan Wang and Lei Li},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation},
  year      = {2020},
  month     = aug,
  timestamp = {2020-01-07},
}

@InProceedings{wang2020solo,
  author    = {Xinlong Wang and Tao Kong and Chunhua Shen and Yuning Jiang and Lei Li},
  booktitle = {The European Conference on Computer Vision (ECCV)},
  title     = {{SOLO}: {S}egmenting {O}bjects by {L}ocations},
  year      = {2020},
  month     = aug,
  abstract  = {We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.},
  url       = {https://arxiv.org/abs/1912.04488},
}

@InProceedings{shi2020dispersed,
  author    = {Wenxian Shi and Hao Zhou and Ning Miao and Lei Li},
  booktitle = {Proceedings of the 37th International Conference on Machine learning (ICML)},
  title     = {Dispersing Exponential Family Mixture {VAE}s for Interpretable Text Generation},
  year      = {2020},
  month     = jul,
}

@InProceedings{ru2020quachie,
  author       = {Dongyu Ru and Zhenghui Wang and Lin Qiu and Hao Zhou and Lei Li and Weinan Zhang and Yong Yu},
  booktitle    = {the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) - System Demonstrations},
  title        = {{QuAChIE}: Question Answering based {Chinese} Information Extraction System},
  year         = {2020},
  month        = jul,
  entrysubtype = {demo},
  owner        = {lilei.02},
}

@InProceedings{miao2020do,
  author    = {Ning Miao and Yuxuan Song and Hao Zhou and Lei Li},
  booktitle = {the 58th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers},
  title     = {Do you have the right scissors? Tailoring Pre-trained Language Models via {Monte}-{Carlo} Methods},
  year      = {2020},
  month     = jul,
  abstract  = {It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.  In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to  over- and/or under-estimation problem.  In this paper, we propose MC-Taylor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Taylor consistently and significantly outperforms the fine-tuning approach. Our code is available at \url{https://github.com/NingMiao/MC-tailor}.},
  timestamp = {2020-05-01},
}

@InProceedings{xu2020xiaomingbot,
  author       = {Runxin Xu and Jun Cao and Mingxuan Wang and Jiaze Chen and Hao Zhou and Ying Zeng and Yuping Wang and Li Chen and Xiang Yin and Xijin Zhang and Songcheng Jiang and Yuxuan Wang and Lei Li},
  booktitle    = {the 58th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations},
  title        = {Xiaomingbot: A Multilingual Robot News Reporter},
  year         = {2020},
  month        = jul,
  abstract     = {This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multi-modal software robot equipped with four integral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multilingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person’s voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.},
  entrysubtype = {demo},
  timestamp    = {2020-05-01},
  url          = {https://xiaomingbot.github.io},
}

@Article{wu2020towards,
  author  = {Wu, Fei and Lu, Cewu and Zhu, Mingjie and Chen, Hao and Zhu, Jun and Yu, Kai and Li, Lei and Li, Ming and Chen, Qianfeng and Li, Xi and Cao, Xudong and Wang, Zhongyuan and Zha, Zhengjun and Zhuang, Yueting and Pan, Yunhe},
  journal = {Nature Machine Intelligence},
  title   = {Towards a new generation of artificial intelligence in {China}},
  year    = {2020},
  month   = jun,
  pages   = {312-316},
  volume  = {2},
  doi     = {10.1038/s42256-020-0183-4},
}

@InProceedings{ye2020variational,
  author    = {Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Variational Template Machine for Data-to-Text Generation},
  year      = {2020},
  month     = apr,
  abstract  = {How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able generate more diversely while keeping a good fluency and quality.},
  url       = {https://openreview.net/forum?id=HkejNgBtPB},
}

@InProceedings{zheng2020mirror,
  author    = {Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xinyu Dai and Jiajun Chen},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Mirror Generative Models for Neural Machine Translation},
  year      = {2020},
  month     = apr,
  abstract  = {Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in all a variety of scenarios and language pairs, including resource-rich and low-resource languages.},
  addendum  = {Oral, 1.9\% acceptance rate},
  owner     = {lilei.02},
  url       = {https://openreview.net/forum?id=HkxQRTNYPH},
}

@InProceedings{huang2020span,
  author    = {Xunpeng Huang and Xianfeng Liang and Zhengyang Liu and Yue Yu and Lei Li},
  booktitle = {the 34th {AAAI} Conference on Artificial Intelligence (AAAI)},
  title     = {{SPAN}: A Stochastic Projected Approximate Newton Method},
  year      = {2020},
  month     = feb,
  abstract  = {Second-order optimization methods have desirable convergence properties. However, the exact Newton method requires expensive computation for the Hessian and its inverse. In this paper, we propose SPAN, a novel approximate and fast Newton method. SPAN computes the inverse of the Hessian matrix via low-rank approximation and stochastic Hessian-vector products. Our experiments on multiple benchmark datasets demonstrate that SPAN outperforms existing first-order and second-order optimization methods in terms of the convergence wall-clock time. Furthermore, we provide a theoretical analysis of the per-iteration complexity, the approximation error, and the convergence rate. Both the theoretical analysis and experimental results show that our proposed method achieves a better trade-off between the convergence rate and the per-iteration efficiency.},
}

@InProceedings{wang2020task,
  author    = {Xinlong Wang and Wei Yin and Tao Kong and Yuning Jiang and Lei Li and Chunhua Shen},
  booktitle = {the 34th {AAAI} Conference on Artificial Intelligence (AAAI)},
  title     = {Task-Aware Monocular Depth Estimation for {3D} Object Detection},
  year      = {2020},
  month     = feb,
  abstract  = {Monocular depth estimation enables 3D perception from a single 2D image, thus attracting much research attention for years. Almost all methods treat foreground and background regions (``things and stuff'') in an image equally. However, not all pixels are equal. Depth of foreground objects plays a crucial role in 3D object recognition and localization. To date how to boost the depth prediction accuracy of foreground objects is rarely discussed. In this paper, we first analyse the data distributions and interaction of foreground and background, then propose the foreground-background separated monocular depth estimation (ForeSeE) method, to estimate the foreground depth and background depth using separate optimization objectives and depth decoders. Our method significantly improves the depth estimation performance on foreground objects. Applying ForeSeE to 3D object detection, we achieve 7.5 AP gains and set new state-of-the-art results among other monocular methods.},
  addendum  = {Oral},
}

@InProceedings{wu2020importance,
  author    = {Qingyang Wu and Lei Li and Hao Zhou and Ying Zeng and Zhou Yu},
  booktitle = {the 34th {AAAI} Conference on Artificial Intelligence (AAAI)},
  title     = {Importance-Aware Learning for Neural Headline Editing},
  year      = {2020},
  month     = feb,
  abstract  = {Many social media news writers are not professionally trained. Therefore, social media platforms have to hire professional editors to adjust amateur headlines to attract more readers. We propose to automate this headline editing process through neural network models to provide more immediate writing support for these social media news writers. To train such a neural headline editing model, we collected a dataset which contains articles with original headlines and professionally edited headlines. However, it is expensive to collect a large number of professionally edited headlines. To solve this low-resource problem, we design an encoder-decoder model which leverages large scale pre-trained language models. We further improve the pre-trained model's quality by introducing a headline generation task as an intermediate task before the headline editing task. Also, we propose Self Importance-Aware (SIA) loss to address the different levels of editing in the dataset by down-weighting the importance of easily classified tokens and sentences. With the help of Pre-training, Adaptation, and SIA, the model learns to generate headlines in the professional editor's style. Experimental results show that our method significantly improves the quality of headline editing comparing against previous methods.},
  url       = {https://arxiv.org/abs/1912.01114},
}

@InProceedings{yang2020towards,
  author    = {Jiacheng Yang and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Weinan Zhang and Yong Yu and Lei Li},
  booktitle = {the 34th {AAAI} Conference on Artificial Intelligence ({AAAI})},
  title     = {Towards Making the Most of {BERT} in Neural Machine Translation},
  year      = {2020},
  month     = feb,
  abstract  = {GPT-2 and BERT demonstrate the effectiveness of using pre-trained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks.
In this work, we introduce a concerted training framework (\method) that is the key to integrate the pre-trained LMs to neural machine translation (NMT).
Our proposed \method consists of three techniques:
\begin{inparaenum}[\it a)]
\item asymptotic distillation to ensure that the NMT model can retain the previous pre-trained knowledge;
\item a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and
\item a strategy to adjust the learning paces according to a scheduled policy.
\end{inparaenum}
Our experiments in machine translation show \method gains of up to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pre-training aided NMT by 1.4 BLEU score.
While for the large WMT14 English-French task with 40 millions of sentence-pairs, our base model still significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score.},
}

@InProceedings{miao2019kernelized,
  author    = {Miao, Ning and Zhou, Hao and Zhao, Chengqi and Shi, Wenxian and Li, Lei},
  booktitle = {the 33rd Conference on Neural Information Processing Systems (NeurIPS)},
  title     = {Kernelized {Bayesian} Softmax for Text Generation},
  year      = {2019},
  month     = dec,
}

@InProceedings{zhao2019what,
  author    = {Zhao, Zhichen and Li, Lei and Zhang, Bowen and Wang, Meng and Jiang, Yuning and Xu, Li and Wang, Fengkun and Ma, Weiying},
  booktitle = {the 28th ACM International Conference on Information and Knowledge Management (CIKM)},
  title     = {What You Look Matters: Offline Evaluation of Advertising Creatives for Cold Start Problem},
  year      = {2019},
  month     = nov,
  abstract  = {Modern online-auction-based advertising systems utilize user and item features to automatically place ads. In order to train a model to rank the most profitable ads, new ad creatives have to be placed online for hours to receive sufficient user-click data. This corresponds to the cold-start stage. Random strategy lead to inefficiency and inferior selections of potential ads. In this paper, we analyze the effectiveness of content-based selection during the cold-start stage. Specifically, we propose Pre Evaluation of Ad Creative Model (PEAC), a novel method to evaluate and select ad creatives offline before being placed online. Our proposed PEAC utilizes the automatically extracted deep feature from ad content to predict and rank their potential online placement performance. It does not rely on any user-click data, which is scarce during the cold-starting phase. A large-scale system based on our method has been deployed in a real online advertising platform. The online A/B testing shows the ads system with PEAC pre-ranking obtains significant improvement in revenue gain compared to the prior system. Furthermore, we provide detailed analyses on what the model learned, which gives further suggestions to improve ad creative design.},
}

@InProceedings{wang2019towards,
  author    = {Wang, Mingxuan and Xie, Jun and Tan, Zhixing and Su, Jinsong and Xiong, Deyi and Li, Lei},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Towards Linear Time Neural Machine Translation with {Capsule} Networks},
  year      = {2019},
  month     = nov,
  abstract  = {In this study, we first investigate a novel capsule network with dynamic routing for linear time Neural Machine Translation (NMT), referred as CAPSNMT. CAPSNMT uses an aggregation mechanism to map the source sentence into a matrix with pre-determined size, and then applys a deep LSTM network to decode the target sequence from the source representation. Unlike the previous work (Sutskever et al., 2014) to store the source sentence with a passive and bottom-up way, the dynamic routing policy encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CAPSNMT has two core properties: it runs in time that is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, CAPSNMT achieves comparable results with the Transformer system. We also devise new hybrid architectures intended to combine the strength of CAPSNMT and the RNMT model. Our hybrid models obtain state-of-the-arts results on both benchmark datasets. To the best of our knowledge, this is the first work that capsule networks have been empirically inves- tigated for sequence to sequence problems},
}

@InProceedings{jiang2019svd,
  author    = {Jiang, Qing-Yuan and He, Yi and Li, Gen and Lin, Jian and Li, Lei and Li., Wu-Jun},
  booktitle = {International Conference on Computer Vision (ICCV)},
  title     = {{SVD}: A Large-Scale Short Video Dataset for Near Duplicate Video Retrieval.},
  year      = {2019},
  month     = oct,
  abstract  = {With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difficult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io.},
  url       = {https://svdbase.github.io},
}

@InProceedings{wang2019vatex,
  author    = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle = {International Conference on Computer Vision (ICCV)},
  title     = {{VATEX}: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},
  year      = {2019},
  month     = oct,
  abstract  = {We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.},
  addendum  = {Oral},
  url       = {https://vatex.org/main/index.html},
}

@InProceedings{fu2019rethinking,
  author    = {Fu, Yao and Zhou, Hao and Chen, Jiaze and Li, Lei},
  booktitle = {the 12th International Conference on Natural Language Generation (INLG)},
  title     = {Rethinking Text Attribute Transfer: A Lexical Analysis},
  year      = {2019},
  month     = oct,
  abstract  = {Text attribute transfer is modifying certain linguistic attributes (e.g. sentiment, style, authorship, etc.) of a sentence and transforming them from one type to another. In this paper, we aim to analyze and interpret what is changed during the transfer process. We start from the observation that in many existing models and datasets, certain words within a sentence play important roles in determining the sentence attribute class. These words are referred to as the Pivot Words. Based on these pivot words, we propose a lexical analysis framework, the Pivot Analysis, to quantitatively analyze the effects of these words in text attribute classification and transfer. We apply this framework to existing datasets and models, and show that: (1) the pivot words are strong features for the classification of sentence attributes; (2) to change the attribute of a sentence, many datasets only requires to change certain pivot words; (3) consequently, many transfer models only perform the lexical-level modification, while leaving higher-level sentence structures unchanged. Our work provides an in-depth understanding of linguistic attribute transfer and further identifies the future requirements and challenges of this task.},
}

@InProceedings{lu2019uncovering,
  author    = {Lu, Yunfei and Yu, Linyun and Cui, Peng and Zang, Chengxi and Xu, Renzhe and Liu, Yihao and Li, Lei and Zhu, Wenwu},
  booktitle = {the 25th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {Uncovering the Co-driven Mechanism of Social and Content Links in User Churn Phenomena},
  year      = {2019},
  address   = {New York, NY, USA},
  month     = aug,
  publisher = {ACM},
  abstract  = {Recent years witness the merge of social networks and user-generatedcontent (UGC) platforms. In these new platforms, users establishlinks to others not only driven by their social relationships in thephysical world but also driven by the contents published by others.During this merging process, social networks gradually integrateboth social and content links and become unprecedentedly complicated,with the motivation to exploit both the advantages of socialviscosity and content attractiveness to reach the best customerretention situation. However, due to the lack of fine-grained datarecording such merging phenomena, the co-driven mechanism ofsocial and content links in churn phenomena remains unexplored.How do social and content factors jointly influence customers’churn? What is the best ratio of social and content links for a user’sretention? Is there a model to capture this co-driven mechanism inusers’ churn phenomena?In this paper, we collect a real-world dataset with more than 5.77million users and 925 million links, with each link being tagged asa social one or a content one. We find that both social and contentlinks have a significant impact on users’ churn and theywork jointlyas a complicated mixture effect. As a result, we propose a novelsurvival model, which incorporates both social and content factors,to predict churn probability over time. Our model successfully fitsthe churn distribution in reality and accurately predicts the churnrate of different subpopulations in the future. By analyzing themodeling parameters, we try to strike a balance between socialdrivenand content-driven links in a user’s social network to reachthe lowest churn rate. Our model and findings may have potentialimplications for the design of future social media.},
  file      = {:pubs/lu-kdd2019-social-content-links.pdf:PDF},
  owner     = {leili},
  timestamp = {2019-04-30},
}

@InProceedings{sun2019graspsnooker,
  author       = {Sun, Zhaoyue and Chen, Jiaze and Zhou, Hao and Zhou, Deyu and Li, Lei and Jiang, Mingmin},
  booktitle    = {the 28th International Joint Conference on Artificial Intelligence ({IJCAI})},
  title        = {{GraspSnooker}: Automatic {Chinese} Commentary Generation for Snooker Videos},
  year         = {2019},
  month        = aug,
  pages        = {6569--6571},
  addendum     = {Demo},
  doi          = {10.24963/ijcai.2019/959},
  entrysubtype = {demo},
  url          = {https://doi.org/10.24963/ijcai.2019/959},
}

@InProceedings{weng2019correct,
  author    = {Weng, Rongxiang and Zhou, Hao and Huang, Shujian and Xia, Yifan and Li, Lei and Chen, Jiajun},
  booktitle = {the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
  title     = {{Correct-and-Memorize}: Learning to Translate from Interactive Revisions},
  year      = {2019},
  month     = aug,
  abstract  = {State-of-the-art machine translation models are stillnot on a par with human translators. Previous worktakes human interactions into the neural machine translation process to obtain improved results in target languages. However, not all model-translation errors are equal some are critical while others are minor. In the mean while, same translation mistakes occur repeatedly in similar context. To solve bothissues, we propose CAMIT, a novel method for translating in an interactive environment. Our proposed method works with critical revision instructions,therefore allows human to correct arbitrary words in model-translated sentences. In addition,CAMIT learns from and softly memorizes revision actions based on the context, alleviating the issue of repeating mistakes. Experiments in both ideal and real interactive translation settings demonstrate that our proposed CAMIT enhances machine translation results significantly while requires fewer revision instructions from human compared to previous methods.},
  addendum  = {Oral},
}

@InProceedings{bao2019generating,
  author    = {Bao, Yu and Zhou, Hao and Huang, Shujian and Li, Lei and Mou, Lili and Vechtomova, Olga and Dai, Xinyu and Chen, Jiajun},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  title     = {Generating Sentences from Disentangled Syntactic and Semantic Spaces},
  year      = {2019},
  month     = jul,
  abstract  = {Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE’s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax-transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.},
}

@InProceedings{qiu2019dynamically,
  author    = {Qiu, Lin and Xiao, Yunxuan and Qu, Yanru and Zhou, Hao and Li, Lei and Zhang, Weinan and Yu, Yong},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  title     = {Dynamically Fused Graph Network for Multi-hop Reasoning},
  year      = {2019},
  month     = jul,
  abstract  = {Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text across two or more documents. In this paper, we propose the Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analy- sis shows DFGN could produce interpretable reasoning chains.},
  file      = {:pubs/qiu-acl2019-dfgn.pdf:PDF},
}

@InProceedings{zhang2019generating,
  author    = {Zhang, Huangzhao and Miao, Ning and Zhou, Hao and Li, Lei},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers},
  title     = {Generating Fluent Adversarial Examples for Natural Languages},
  year      = {2019},
  month     = jul,
  abstract  = {Efficiently building an adversarial attacker fornatural language processing (NLP) tasks is areal challenge. Firstly, as the sentence spaceis discrete, it is difficult to make small perturbations along the direction of gradients. Secondly,the fluency of the generated examples can not be guaranteed. In this paper, we propose MHA, which addresses both problemsby performing Metropolis-Hastings sampling,whose proposal is designed with the guidanceof gradients. Experiments on IMDB and SNLIshow that our proposed MHA outperforms thebaseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.},
  file      = {:pubs/zhang-acl2019generation-cgmh-adversarial.pdf:PDF},
}

@InProceedings{wu2019unified,
  author    = {Wu, Hao and Mao, Jiayuan and Zhang, Yufeng and Sun, Weiwei and Jiang, Yuning and Li, Lei and Ma, Wei-Ying},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations},
  year      = {2019},
  month     = jun,
  abstract  = {We propose Unified Visual-Semantic Embeddings (VSE)
for learning a joint space for scene representation and textual
semantics. It unifies the embeddings of concepts at different
levels: objects, attributes, relations and full scenes. We
view the sentential semantics as a combination of different
semantic components such as object or relational descriptors,
and align their embeddings with different regions of a
scene. A contrastive learning approach is proposed for the
effective learning of such fine-grained alignment from only
image-caption pairs. We also present a simple yet effective
approach that enforces the coverage of caption embeddings
on the semantic components that appear in the sentence. We
demonstrate that the Unified VSE outperforms other baselines
on cross-modal retrieval tasks and the enforcement
of the semantic coverage improves models’ robustness in
defending text-domain adversarial attacks. Moreover, such
robustness empowers the use of visual cues to accurately
resolve word dependencies in novel sentences.},
  addendum  = {Oral},
  file      = {:pubs/wu-cvpr2019-vse.pdf:PDF},
}

@InProceedings{miao2019cgmh,
  author    = {Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
  booktitle = {the 33rd {AAAI} Conference on Artificial Intelligence (AAAI)},
  title     = {{CGMH}: Constrained Sentence Generation by Metropolis-Hastings Sampling},
  year      = {2019},
  month     = jan,
  abstract  = {In real-world applications of natural language generation,
there are often constraints on the target sentences in addition
to fluency and naturalness requirements. Existing language
generation techniques are usually based on recurrent
neural networks (RNNs). However, it is non-trivial to impose
constraints on RNNs while maintaining generation quality,
since RNNs generate sentences sequentially (or with beam
search) from the first word to the last. In this paper, we propose
CGMH, a novel approach using Metropolis-Hastings
sampling for constrained sentence generation. CGMH allows
complicated constraints such as the occurrence of multiple
keywords in the target sentences, which cannot be handled in
traditional RNN-based approaches. Moreover, CGMH works
in the inference stage, and does not require parallel corpora
for training.We evaluate our method on a variety of tasks, including
keywords-to-sentence generation, unsupervised sentence
paraphrasing, and unsupervised sentence error correction.
CGMH achieves high performance compared with previous
supervised methods for sentence generation. Our code
is released at https://github.com/NingMiao/CGMH},
  addendum  = {Oral},
  file      = {:pubs/miao-aaai2019-cgmh.pdf:PDF;Software:https\:/github.com/NingMiao/CGMH:URL;PPT:pubs/miao2019cgmh - {CGMH_} Constrained Sentence Generation by Metropolis-Hastings Sampling.pdf:PDF},
  url       = {http://arxiv.org/abs/1811.10996},
}

@InProceedings{cao2018brits,
  author    = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Yitan and Li, Lei},
  booktitle = {the 32nd Conference on Neural Information Processing Systems (NeurIPS)},
  title     = {{BRITS}: Bidirectional Recurrent Imputation for Time Series},
  year      = {2018},
  month     = dec,
  abstract  = {Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation.BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.},
  url       = {https://arxiv.org/abs/1805.10572},
}

@InProceedings{shi2018tree,
  author    = {Shi, Haoyue and Zhou, Hao and Chen, Jiaze and Li, Lei},
  title     = {On Tree-Based Neural Sentence Modeling},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2018},
  month     = oct,
  abstract  = {Neural networks with tree-based sentence encoders have shown better results on many downstream tasks. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (i.e., binary balanced tree, left-branching tree and right-branching tree) in the encoders. Though trivial trees contain no syntactic information, those encoders get competitive or even better results on all of the ten downstream tasks we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that tree modeling gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder.},
  file      = {:pubs/shi2018tree - On Tree-Based Neural Sentence Modeling.pdf:PDF},
  owner     = {lilei.02},
  url       = {https://arxiv.org/abs/1808.09644},
}

@InProceedings{li2018jersey,
  author       = {Li, Gen and Xu, Shikun and Liu, Xiang and Li, Lei and Wang, Changhu},
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition workshops, Computer Vision in Sports},
  title        = {Jersey Number Recognition with Semi-Supervised Spatial Transformer Network},
  year         = {2018},
  month        = jun,
  pages        = {1864 --1871},
  abstract     = {It is still a challenging task to recognize the jersey number 
of players on the court in soccer match videos, as the
jersey numbers are very small in the object detection task
and annotated data are not easy to collect. Based on the
object detection results of all the players on the court, a
CNN model is first introduced to classify these numbers on
the deteced players’ images. To localize the jersey number
more precisely without involving another digit detector and
extra consumption, we then improve the former network to
an end-to-end framework by fusing with the spatial transformer
network (STN). To further improve the accuracy, we
bring extra supervision to STN and upgrade the model to
a semi-supervised multi-task learning system, by labeling a
small portion of the number areas in the dataset by quadrangle.
Extensive experiments illustrate the effectiveness of
the proposed framework.},
  entrysubtype = {workshop},
}

@InProceedings{wu2018reinforced,
  author    = {Wu, Jiawei and Li, Lei and Wang, William Yang},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  title     = {Reinforced Co-Training},
  year      = {2018},
  month     = jun,
  pages     = {1252--1262},
  publisher = {Association for Computational Linguistics},
  abstract  = {Co-training is a popular semi-supervised learning framework to utilize a
large amount of unlabeled data in addition to a small labeled set. Co-training
methods exploit predicted labels on the unlabeled data and select samples based
on prediction confidence to augment the training. However, the selection of
samples in existing co-training methods is based on a predetermined policy,
which ignores the sampling bias between the unlabeled and the labeled subsets,
and fails to explore the data space. In this paper, we propose a novel method,
Reinforced Co-Training, to select high-quality unlabeled samples to better
co-train on. More specifically, our approach uses Q-learning to learn a data
selection policy with a small labeled dataset, and then exploits this policy to
train the co-training classifiers automatically. Experimental results on
clickbait detection and generic text classification tasks demonstrate that our
proposed method can obtain more accurate text classification results.},
  location  = {New Orleans, Louisiana},
  timestamp = {2018-05-07T10:09:16.000+0200},
  url       = {http://arxiv.org/abs/1804.06035},
}

@InProceedings{erol2017nearly,
  author    = {Erol, Yusuf B. and Wu, Yi and Li, Lei and Russell, Stuart},
  booktitle = {the 31st AAAI Conference on Artificial Intelligence (AAAI)},
  title     = {A Nearly-Black-Box Online Algorithm for Joint Parameter and State Estimation in Temporal Models},
  year      = {2017},
  month     = jul,
  abstract  = {Online joint parameter and state estimation is a core problem for temporal models. Most existing methods are either restricted to a particular class of models (e.g., the Storvik filter) or computationally expensive (e.g., particle MCMC). We propose a novel nearly-black-box algorithm, the Assumed Parameter Filter (APF), a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has the following advantages: (a) it is online and computationally efficient; (b) it is applicable to both discrete and continuous parameter spaces with arbitrary transition dynamics. On a variety of toy and real models, APF generates more accurate results within a fixed computation budget compared to several standard algorithms from the literature.},
  comment   = {The earlier version appeared in NIPS 2016 workshop on Advances in Approximate Bayesian Inference.},
  file      = {:pubs/erol2017nearly - A Nearly-Black-Box Online Algorithm for Joint Parameter and State Estimation in Temporal Models.pdf:PDF;Appendix:pubs/erol-aaai2017-apf-appendix.pdf:PDF},
  owner     = {lilei.02},
  timestamp = {2017-01-20},
}

@Article{matsubara2017non,
  author   = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
  journal  = {ACM Transactions on the Web},
  title    = {Non-linear Dynamics of Information Diffusion in Social Networks},
  year     = {2017},
  month    = feb,
  number   = {1},
  volume   = {11},
  abstract = {The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this paper, we propose SPIKEM, a concise yet flexible analytical model of the rise and fall patterns of information diffusion. Our model has the following advantages: (a) unification power: it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models. We provide the threshold of the take-off vs. die-out conditions for SPIKEM, and discuss the generality of our model, by applying it to an arbitrary graph topology; (b) practicality: it matches the observed behavior of diverse sets of real data; (c) parsimony: it requires only a handful of parameters; and (d) usefulness: it makes it possible to perform analytic tasks such as forecasting, spotting anomalies, and interpretation by reverse engineering the system parameters of interest (e.g. quality of news, number of interested bloggers, etc.). We also introduce an efficient and effective algorithm for the real-time monitoring of information diffusion, namely, SPIKESTREAM, which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets demonstrate that SPIKEM accurately and succinctly describes all the patterns of the rise-and-fall spikes in social networks.},
  comment  = {The earlier version of the paper appeared in KDD'12. This version includes significant extension.},
}

@InProceedings{wu2016swift,
  author    = {Wu, Yi and Li, Lei and Russell, Stuart J. and Bodik, Rastislav},
  title     = {Swift: Compiled Inference for Probabilistic Programming Languages},
  booktitle = {25th International Joint Conference on Artificial Intelligence (IJCAI)},
  year      = {2016},
  month     = jul,
  abstract  = {A probabilistic program defines a probability measure over its semantic structures. One common goal of probabilistic programming languages (PPLs) is to compute posterior probabilities for arbitrary models and queries, given observed evidence, using a generic inference engine. Most PPL inference engines—even the compiled ones—incur significant runtime interpretation overhead, especially for contingent and open-universe models. This paper describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates optimizations that eliminate interpretation overhead, maintain dynamic dependencies efficiently, and handle memory management for possible worlds of varying sizes. Experiments comparing Swift with other PPL engines on a variety of inference problems demonstrate speedups ranging from 12x to 326x.},
  file      = {:pubs/wu2016swift - Swift_ Compiled Inference for Probabilistic Programming Languages.pdf:PDF;Arxiv:http\:/arxiv.org/abs/1606.09242:URL;Software:https\:/github.com/lileicc/swift:URL},
  owner     = {leili},
  timestamp = {2016.07.04},
}

@InProceedings{dai2016cfo,
  author    = {Dai, Zihang and Li, Lei and Xu, Wei},
  booktitle = {the 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
  title     = {{CFO}: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases},
  year      = {2016},
  month     = jul,
  abstract  = {How can we enable computers to automatically answer questions like ``Who created the character Harry Potter''? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions --- ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neural-network-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7\% on a dataset of 108k questions - the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8\%.},
  file      = {:pubs/dai2016cfo - CFO_ Conditional Focused Neural Question Answering with Large-scale Knowledge Bases.pdf:PDF;Arxiv:https\:/arxiv.org/abs/1606.01994:URL;Source repo:https\:/github.com/zihangdai/CFO:URL},
  owner     = {leili},
  timestamp = {2016.07.04},
}

@Misc{lu2015twisted,
  author    = {Lu, Zefu and Li, Lei and Xu, Wei},
  title     = {Twisted Recurrent Network for Named Entity Recognition},
  year      = {2015},
  booktitle = {Bay Area Machine Learning Symposium},
  owner     = {leili},
  timestamp = {2015.11.03},
}

@Misc{pham2015optimization,
  author    = {Pham, Hieu and Dai, Zihang and Li, Lei},
  title     = {On Optimization Algorithms for Recurrent Networks with Long Short-Term Memory},
  year      = {2015},
  booktitle = {Bay Area Machine Learning Symposium},
  owner     = {leili},
  timestamp = {2015.11.03},
}

@InProceedings{wu2014bfit,
  author       = {Wu, Yi and Li, Lei and Russell, Stuart J.},
  booktitle    = {Neural Information Processing Systems, Probabilistic Programming workshop},
  title        = {{BFiT}: From Possible-World Semantics to Random-Evaluation Semantics in Open Universe},
  year         = {2014},
  abstract     = {In recent years, several probabilistic programming languages (PPLs) have emerged, such as Bayesian Logic (BLOG), Church, and Figaro. These languages can be classified into two categories: PPLs interpreted using possible-world se- mantics and ones using random-evaluation semantics. In this paper, we explic- itly analyze the equivalence between these two semantics in the context of open- universe probability models (OUPMs). We propose a novel dynamic memoization technique to construct OUPMs using procedural instructions in random-evaluation based PPLs. We implemented a translator named BFiT, which converts code in BLOG (possible-world based) to Figaro (random-evaluation based). The trans- lated program in Figaro exhibits a merely constant blowup factor in program size while yielding the same inference results as the original model in BLOG.},
  entrysubtype = {workshop},
  file         = {:pubs/wu2014bfit - BFiT_ From Possible-World Semantics to Random-Evaluation Semantics in Open Universe.pdf:PDF},
  owner        = {leili},
  slides       = {pubs/wu-2014-bfit-poster.pdf},
  timestamp    = {2014.12.16},
}

@InProceedings{du2014maxios,
  author       = {Du, Simon Shaolei and Liu, Yilin and Chen, Boyi and Li, Lei},
  booktitle    = {Neural Information Processing Systems, workshop on Distributed Machine Learning and Matrix Computations},
  title        = {Maxios: Large Scale Nonnegative Matrix Factorization for Collaborative Filtering},
  year         = {2014},
  abstract     = {Nonnegative matrix factorization proved useful in many applications, including collaborative filtering – from existing ratings data one would like to predict new product ratings by users. However, factorizing a user-product score matrix is computation and memory intensive. We propose Maxios, a novel approach to fill missing values for large scale and highly sparse matrices efficiently and ac- curately. We formulate the matrix-completion problem as weighted nonnegative matrix factorization. In addition, we develop distributed update rules using alter- nating direction method of multipliers. We have implemented the Maxios system on top of Spark, a distributed in-memory computation framework. Experiments on commercial clusters show that Maxios is competitive in terms of scalability and accuracy against the existing solutions on a variety of datasets.},
  entrysubtype = {workshop},
  file         = {:pubs/du2014maxios - Maxios_ Large Scale Nonnegative Matrix Factorization for Collaborative Filtering.pdf:PDF},
  owner        = {leili},
  slides       = {pubs/du-2014-maxios-poster.pdf},
  timestamp    = {2014.12.16},
}

@InProceedings{juan2014poisson,
  author    = {Juan, Da-Cheng and Li, Lei and Peng, Huan-Kai and Marculescu, Diana and Faloutsos, Christos},
  booktitle = {The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)},
  title     = {Beyond {Poisson}: Modeling Inter-Arrival Times of Requests in a Datacenter},
  year      = {2014},
  file      = {:pubs/juan2014poisson - Beyond Poisson_ Modeling Inter-Arrival Times of Requests in a Datacenter.pdf:PDF},
  owner     = {leili},
  timestamp = {2014.01.07},
}

@InProceedings{li2013dynamic,
  author    = {Li, Lei and Ramsundar, Bharath and Russell, Stuart},
  booktitle = {16th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Dynamic Scaled Sampling for Deterministic Constraints},
  year      = {2013},
  abstract  = {Deterministic and near-deterministic relationships among subsets of random variables in multivariate systems are known to cause serious problems for Monte Carlo algorithms. We examine the case in which the relationship Z = f(X1,...,Xk) holds, where each Xi has a continuous prior pdf and we wish to obtain samples from the conditional distribution P(X1,...,Xk | Z = s). When f is addition, the problem is NP-hard even when the Xi are independent. In more restricted cases—for example, i.i.d. Boolean or categorical Xi—efficient exact samplers have been obtained previously. For the general continuous case, we propose a dynamic scaling algorithm (DYSC), and prove that it has O(k) expected running time and finite variance. We discuss generalizations of DYSC to functions f described by binary operation trees. We evaluate the algorithm on several examples.},
  file      = {:pubs/li2013dynamic - Dynamic Scaled Sampling for Deterministic Constraints.pdf:PDF},
  owner     = {leili},
  timestamp = {2013.03.05},
}

@InProceedings{vikram2013handwriting,
  author    = {Vikram, Sharad and Li, Lei and Russell, Stuart},
  title     = {Handwriting and Gestures in the Air, Recognizing on the Fly},
  booktitle = {ACM Conference on Human Factors in Computing Systems (CHI) Extended Abstracts},
  year      = {2013},
  abstract  = {Recent technologies in vision sensors are capable of capturing 3D finger positions and movements. We propose a novel way to control and interact with computers by moving fingers in the air. The positions of fingers are precisely captured by a computer vision device. By tracking the moving patterns of fingers, we can then recognize users’ intended control commands or input information. We demonstrate this human input approach through an example application of handwriting recognition. By treating the input as a time series of 3D positions, we propose a fast algorithm using dynamic time warping to recognize characters in online fashion. We employ various optimization techniques to recognize in real time as one writes. Experiments show promising recognition performance and speed.},
  file      = {:pubs/vikram2013handwriting - Handwriting and Gestures in the Air, Recognizing on the Fly.pdf:PDF},
  owner     = {leili},
  timestamp = {2013.03.05},
}

@InProceedings{liu2013hibernating,
  author    = {Liu, Siyuan and Li, Lei and Krishnan, Ramayya},
  booktitle = {IEEE International Conference on Data Mining (ICDM)},
  title     = {Hibernating Process: Modelling Mobile Calls at Multiple Scales},
  year      = {2013},
  file      = {:pubs/liu2013hibernating - Hibernating Process_ Modelling Mobile Calls at Multiple Scales.pdf:PDF},
  owner     = {leili},
  timestamp = {2013.11.26},
}

@InProceedings{erol2013extended,
  author    = {Erol, Yusuf and Li, Lei and Ramsundar, Bharath and Russell, Stuart J.},
  booktitle = {Proceedings of the 30th International Conference on Machine learning (ICML)},
  title     = {The Extended Parameter Filter},
  year      = {2013},
  abstract  = {The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.},
  comment   = {The full version appeared as Tech. Rep. UCB/EECS-2013-48.},
  file      = {:pubs/erol2013extended - The Extended Parameter Filter.pdf:PDF},
  owner     = {leili},
  timestamp = {2013.05.08},
}

@InProceedings{fu2013why,
  author    = {Fu, Bin and Lin, Jialiu and Li, Lei and Faloutsos, Christos and Hong, Jason and Sadeh, Norman},
  booktitle = {the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {Why People Hate Your App - Making Sense of User Feedback in a Mobile App Store},
  year      = {2013},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {User review is a crucial component of open mobile app mar- kets such as the Google Play Store. How do we automatically summarize millions of user reviews and make sense out of them? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose WisCom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users’ reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying users’ major concerns and preferences of different types of apps. Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users.},
  file      = {:pubs/fu2013why - Why People Hate Your App - Making Sense of User Feedback in a Mobile App Store.pdf:PDF},
  owner     = {leili},
  timestamp = {2013.06.18},
}

@InProceedings{rogers2013multilinear,
  author    = {Rogers, Mark and Li, Lei and Russell, Stuart J.},
  booktitle = {the 27th Conference on Neural Information Processing Systems(NeurIPS)},
  title     = {Multilinear Dynamical Systems for Tensor Time Series},
  year      = {2013},
  file      = {:pubs/rogers2013multilinear - Multilinear Dynamical Systems for Tensor Time Series.pdf:PDF},
  owner     = {leili},
  software  = {software/mlds-r662.zip},
  timestamp = {2013.09.11},
}

@InProceedings{matsubara2013f,
  author    = {Matsubara, Yasuko and Li, Lei and Papalexakis, Evangelos E. and Lo, David and Sakurai, Yasushi and Faloutsos, Christos},
  booktitle = {The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)},
  title     = {{F-Trail}: Finding Patterns in Taxi Trajectories},
  year      = {2013},
  pages     = {86--98},
  file      = {:pubs/matsubara-The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)13-ftrail.pdf:PDF},
}

@TechReport{li2013blog,
  author      = {Li, Lei and Russell, Stuart J.},
  title       = {The BLOG Language Reference},
  institution = {EECS Department, University of California, Berkeley},
  year        = {2013},
  number      = {UCB/EECS-2013-51},
  month       = may,
  abstract    = {This document introduces the syntax of BLOG, a probabilistic programming language, for describing random variables and their probabilistic dependencies. BLOG defines probabilistic generative models over first-order structures. For example, all Bayesian networks can be easily described by BLOG. BLOG has the following features: (a) it employs open-universe semantics; (b) it can describe relational uncertainty; (c) it can handle identity uncertainty; and (d) it is empowered by first-order logic. The syntax as described in this document corresponds to BLOG version 0.6. The current version represents a significant redesign and extension to previous versions of BLOG, based on the principles of usability and implementation efficiency.},
  file        = {:pubs/li2013blog - The BLOG Language Reference.pdf:PDF;Tech report:http\:/www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-51.html:URL},
}

@InProceedings{henderson2012rolx,
  author    = {Henderson, Keith and Gallagher, Brian and Eliassi-Rad, Tina and Tong, Hanghang and Basu, Sugato and Akoglu, Leman and Koutra, Danai and Faloutsos, Christos and Li, Lei},
  booktitle = {Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {{RolX}: Structural Role Extraction and Mining in Large Graphs},
  year      = {2012},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {:pubs/henderson2012rolx - RolX_ Structural Role Extraction and Mining in Large Graphs.pdf:PDF},
  owner     = {leili},
  timestamp = {2012.07.18},
}

@InProceedings{matsubara2012rise,
  author    = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
  booktitle = {Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {Rise and Fall Patterns of Information Diffusion: Model and Implications},
  year      = {2012},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {:pubs/matsubara2012rise - Rise and Fall Patterns of Information Diffusion_ Model and Implications.pdf:PDF},
  owner     = {leili},
  timestamp = {2012.07.18},
}

@InProceedings{liu2011mobile,
  author       = {Liu, Siyuan and Li, Lei and Faloutsos, Christos and Ni, Lionel},
  booktitle    = {IEEE International Conference on Data Mining, workshop on Data Mining Technologies for Computational Collective Intelligence},
  title        = {Mobile Phone Graph Evolution: Findings, Model and Interpretation},
  year         = {2011},
  entrysubtype = {workshop},
  file         = {:pubs/liu2011mobile - Mobile Phone Graph Evolution_ Findings, Model and Interpretation.pdf:PDF},
  owner        = {leili},
  timestamp    = {2011.11.30},
}

@InProceedings{henderson2011its,
  author    = {Henderson, Keith and Gallagher, Brian and Li, Lei and Akoglu, Leman and Eliassi-Rad, Tina and Tong, Hanghang and Faloutsos, Christos},
  booktitle = {Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {It's Who You Know: Graph Mining Using Recursive Structural Features},
  year      = {2011},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {:pubs/henderson2011its - It's Who You Know_ Graph Mining Using Recursive Structural Features.pdf:PDF},
  isbn      = {978-1-4503-0813-7/11/08},
  location  = {San Diego, California},
  owner     = {leili},
  timestamp = {2011.07.05},
}

@InProceedings{li2011thermocast,
  author    = {Li, Lei and Liang, Chieh-Jan Mike and Liu, Jie and Nath, Suman and Terzis, Andreas and Faloutsos, Christos},
  booktitle = {Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  title     = {{ThermoCast}: A Cyber-Physical Forecasting Model for Data Centers},
  year      = {2011},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Efficient thermal management is important in modern data centers as cooling consumes up to 50% of the total energy. Unlike previous work, we consider proactive thermal management, whereby servers can predict potential overheating events due to dynamics in data center configuration and workload, giving operators enough time to react. However, such forecasting is very challenging due to data center scales and complexity. Moreover, such a physical system is influenced by cyber effects, including workload scheduling in servers. We propose ThermoCast, a novel thermal forecasting model to predict the temperatures surrounding the servers in a data center, based on continuous streams of temperature and airflow measurements. Our approach is (a) capable of capturing cyber- physical interactions and automatically learning them from data; (b) computationally and physically scalable to data center scales; (c) able to provide online prediction with real-time sensor mea- surements. The paper’s main contributions are: (i) We provide a systematic approach to integrate physical laws and sensor observa- tions in a data center; (ii) We provide an algorithm that uses sensor data to learn the parameters of a data center’s cyber-physical sys- tem. In turn, this ability enables us to reduce model complexity compared to full-fledged fluid dynamics models, while maintain- ing forecast accuracy; (iii) Unlike previous simulation-based stud- ies, we perform experiments in a production data center. Using real data traces, we show that ThermoCast forecasts temperature 2× better than a machine learning approach solely driven by data, and can successfully predict thermal alarms 4.2 minutes ahead of time.},
  file      = {:pubs/li2011thermocast - ThermoCast_ A Cyber-Physical Forecasting Model for Data Centers.pdf:PDF},
  isbn      = {978-1-4503-0813-7/11/08},
  location  = {San Diego, California},
  owner     = {leili},
  slides    = {pubs/li2011thermocast-poster.pdf},
  timestamp = {2011.07.05},
}

@InProceedings{li2011time,
  author    = {Li, Lei and Prakash, B. Aditya},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML)},
  title     = {Time Series Clustering: Complex is Simpler!},
  year      = {2011},
  comment   = {Please see for updated and additional experiments in Chap 5 of the thesis "Fast algorithms for mining co-evolving time series".},
  file      = {:pubs/li2011time - Time Series Clustering_ Complex is Simpler!.pdf:PDF;Dataset1-mocap16:software/li2011time - Time Series Clustering_ Complex is Simpler!.zip:URL;Dataset2-mocap35:software/mocap35-rfoot_amc.zip:URL},
  location  = {Bellevue, Washington},
  numpages  = {8},
  owner     = {leili},
  slides    = {pubs/li2011time-slides.pdf},
  software  = {software/clds-r347.zip},
  timestamp = {2011.07.05},
}

@InProceedings{sakurai2011windmine,
  author    = {Sakurai, Yasushi and Li, Lei and Matsubara, Yasuko and Faloutsos, Christos},
  booktitle = {SIAM International Conference on Data Mining (SDM)},
  title     = {{WindMine}: Fast and Effective Mining of Web-click Sequences},
  year      = {2011},
  file      = {:pubs/sakurai2011windmine - WindMine_ Fast and Effective Mining of Web-click Sequences.pdf:PDF},
  owner     = {leili},
  timestamp = {2011.03.14},
}

@PhdThesis{li2011fast,
  author       = {Li, Lei},
  title        = {Fast algorithms for mining co-evolving time series},
  school       = {Carnegie Mellon University},
  year         = {2011},
  howpublished = {Available as technical report CMU-CS-11-127},
  owner        = {leili},
  slides       = {pubs/leili-talk2011-defense.pdf},
  timestamp    = {2011.09.01},
}

@InProceedings{li2010fast,
  author       = {Li, Lei},
  booktitle    = {26th IEEE International Conference on Data Engineering, PHD Workshop},
  title        = {Fast Algorithms for Time Series Mining},
  year         = {2010},
  pages        = {341--344},
  entrysubtype = {workshop},
  file         = {:pubs/li2010fast - Fast Algorithms for Time Series Mining.pdf:PDF},
  owner        = {leili},
  slides       = {pubs/li-icde10-slides.pdf},
  timestamp    = {2010.03.21},
}

@InProceedings{henderson2010metric,
  author    = {Henderson, Keith and Eliassi-Rad, Tina and Faloutsos, Christos and Akoglu, Leman and Li, Lei and Maruhashi, Koji and Prakash, B. Aditya and Tong, Hanghang},
  booktitle = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  title     = {Metric forensics: a multi-level approach for mining volatile graphs},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {163--172},
  publisher = {ACM},
  series    = {KDD '10},
  doi       = {http://doi.acm.org/10.1145/1835804.1835828},
  file      = {:pubs/henderson2010metric - Metric forensics_ a multi-level approach for mining volatile graphs.pdf:PDF},
  isbn      = {978-1-4503-0055-1},
  keywords  = {graph mining, temporal analysis, volatile graphs},
  location  = {Washington, DC, USA},
  numpages  = {10},
}

@InProceedings{li2010bolero,
  author    = {Li, Lei and McCann, James and Pollard, Nancy and Faloutsos, Christos},
  booktitle = {Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)},
  title     = {{BoLeRO}: a principled technique for including bone length constraints in motion capture occlusion filling},
  year      = {2010},
  address   = {Aire-la-Ville, Switzerland, Switzerland},
  pages     = {179--188},
  publisher = {Eurographics Association},
  series    = {SCA '10},
  acmid     = {1921454},
  file      = {:pubs/li2010bolero - BoLeRO_ a principled technique for including bone length constraints in motion capture occlusion filling.pdf:PDF;demo:pubs/BoLeRO-final-v1_xvid.avi:URL},
  location  = {Madrid, Spain},
  numpages  = {10},
  owner     = {leili},
  software  = {software/bolero-r349.zip},
  timestamp = {2011.03.05},
}

@Article{li2010efficient,
  author    = {Li, Lei and Fu, Bin and Faloutsos, Christos},
  journal   = {IEICE Transactions on Information and Systems},
  title     = {Efficient Parallel Learning of Hidden {Markov} chain Models on {SMP}s},
  year      = {2010},
  number    = {6},
  pages     = {1330--1342},
  volume    = {E93.D},
  abstract  = {Quad-core cpus have been a common desktop configuration for today’s office. The increasing number of processors on a single chip opens new opportunity for parallel computing. Our goal is to make use of the multi-core as well as multi-processor architectures to speed up large-scale data mining algorithms. In this paper, we present a general par- allel learning framework, Cut-And-Stitch, for training hidden Markov chain models. Particularly, we propose two model-specific variants, CAS-LDS for learning linear dynamical systems (LDS) and CAS-HMM for learning hidden Markov models (HMM). Our main contribution is a novel method to handle the data dependencies due to the chain structure of hidden variables, so as to parallelize the EM-based parameter learning algorithm. We imple- ment CAS-LDS and CAS-HMM using OpenMP on two supercomputers and a quad-core commercial desktop. The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the traditional serial version.},
  comment   = {This one is applying the idea from Cut-And-Stitch paper from linear dynamical system to hidden markov models. The extended version can be found in my thesis Chapter 6 and 7.},
  file      = {:pubs/li2010efficient - Efficient Parallel Learning of Hidden Markov Chain Models on SMPs.pdf:PDF},
  owner     = {leili},
  timestamp = {2011.03.05},
}

@Article{li2010parsimonious,
  author     = {Li, Lei and Prakash, B. Aditya and Faloutsos, Christos},
  title      = {Parsimonious linear fingerprinting for time series},
  journal    = {The Proceedings of the Very Large Data Bases Endowment (VLDB)},
  year       = {2010},
  volume     = {3},
  pages      = {385--396},
  month      = sep,
  issn       = {2150-8097},
  acmid      = {1920893},
  file       = {:pubs/li2010parsimonious - Parsimonious linear fingerprinting for time series.pdf:PDF},
  issue      = {1-2},
  issue_date = {September 2010},
  numpages   = {12},
  owner      = {leili},
  publisher  = {VLDB Endowment},
  slides     = {pubs/li-vldb10-plif-slides.pdf},
  software   = {software/plif-r345.zip},
  timestamp  = {2011.03.05},
}

@InProceedings{li2009dynammo,
  author    = {Li, Lei and McCann, James and Pollard, Nancy and Faloutsos, Christos},
  booktitle = {Proceeding of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  title     = {{DynaMMo}: Mining and Summarization of Coevolving Sequences with Missing Values},
  year      = {2009},
  address   = {New York, NY, USA},
  publisher = {ACM},
  file      = {:pubs/li2009dynammo - DynaMMo_ Mining and Summarization of Coevolving Sequences with Missing Values.pdf:PDF;Source repo:https\:/github.com/lileicc/dynammo:URL},
  isbn      = {978-1-60558-193-4},
  location  = {Paris, France},
  owner     = {leili},
  slides    = {pubs/li-kdd09-dynammo-slides.pdf},
  software  = {software/dynammo-r346.zip},
  timestamp = {2009.05.01},
}

@InProceedings{guo2009tailoring,
  author       = {Guo, Fan and Li, Lei and Faloutsos, Christos},
  booktitle    = {Proceedings of the 2009 workshop on Web Search Click Data},
  title        = {Tailoring click models to user goals},
  year         = {2009},
  address      = {New York, NY, USA},
  pages        = {88--92},
  publisher    = {ACM},
  series       = {WSCD '09},
  acmid        = {1507523},
  doi          = {http://doi.acm.org/10.1145/1507509.1507523},
  entrysubtype = {workshop},
  isbn         = {978-1-60558-434-8},
  keywords     = {click model, user behavior, web search},
  location     = {Barcelona, Spain},
  numpages     = {5},
}

@InProceedings{xu2008inferring,
  author       = {Xu, Wanhong and Zhou, Xi and Li, Lei},
  booktitle    = {IEEE 24th International Conference on Data Engineering workshops},
  title        = {Inferring privacy information via social relations},
  year         = {2008},
  pages        = {525--530},
  doi          = {10.1109/ICDEW.2008.4498373},
  entrysubtype = {workshop},
}

@InProceedings{li2008cut,
  author    = {Li, Lei and Fu, Wenjie and Guo, Fan and Mowry, Todd C. and Faloutsos, Christos},
  booktitle = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  title     = {{Cut-and-Stitch}: efficient parallel learning of linear dynamical systems on smps},
  year      = {2008},
  address   = {New York, NY, USA},
  pages     = {471--479},
  publisher = {ACM},
  abstract  = {Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing. Our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms. Specifically, we present a parallel algorithm for approximate learning of Linear Dynamical Systems (LDS), also known as Kalman Filters (KF). LDSs are widely used in time series analysis such as motion capture modeling and visual tracking etc. We propose Cut-And-Stitch (CAS), a novel method to handle the data dependencies due to the chain structure of hidden variables in LDS, so as to parallelize the EM- based parameter learning algorithm. We implement the algorithm using OpenMP on both a supercomputer and a quad-core commercial desktop. The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version. In addition, Cut-And-Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models (HMM) and Switching Kalman Filters (SKF).},
  file      = {:pubs/li2008cut - Cut-and-stitch_ efficient parallel learning of linear dynamical systems on smps.pdf:PDF},
  isbn      = {978-1-60558-193-4},
  location  = {Las Vegas, Nevada, USA},
  owner     = {leili},
  slides    = {pub/li-2008-cut-slides.pdf},
  software  = {software/paralearn.0.1.zip},
  timestamp = {2009.03.04},
}

@InProceedings{sakurai2008efficient,
  author    = {Sakurai, Yasushi and Chong, Rosalynn and Li, Lei and Faloutsos, Christos},
  booktitle = {SIAM International Conference on Data Mining (SDM)},
  title     = {Efficient Distribution Mining and Classification},
  year      = {2008},
  pages     = {632--643},
  ee        = {http://www.siam.org/proceedings/datamining/2008/dm08_58_sakurai.pdf},
  file      = {:http\:/www.siam.org/proceedings/datamining/2008/dm08_58_sakurai.pdf:URL},
}

@InProceedings{li2008laziness,
  author    = {Li, Lei and McCann, James and Faloutsos, Christos and Pollard, Nancy},
  booktitle = {The 29th Annual Conference of the European Association for Computer Graphics (EG), Short Paper Proceedings},
  title     = {Laziness is a virtue: Motion stitching using effort minimization},
  year      = {2008},
  file      = {:pubs/li2008laziness - Laziness is a virtue_ Motion stitching using effort minimization.pdf:PDF},
  owner     = {leili},
  timestamp = {2009.03.04},
}

@Article{guo2008c,
  author    = {Guo, Fan and Li, Lei and Faloutsos, Christos and Xing, Eric P.},
  journal   = {The Proceedings of the Very Large Data Bases Endowment (VLDB)},
  title     = {{C-DEM}: a multi-modal query system for Drosophila Embryo databases},
  year      = {2008},
  issn      = {2150-8097},
  month     = aug,
  pages     = {1508--1511},
  volume    = {1},
  acmid     = {1454214},
  file      = {:pubs/guo2008c - C-DEM_ a multi-modal query system for Drosophila Embryo databases.pdf:PDF},
  issue     = {2},
  numpages  = {4},
  owner     = {leili},
  publisher = {VLDB Endowment},
  timestamp = {2009.11.20},
}

@InProceedings{li2006providing,
  author    = {Li, Lei and Liu, Qiaoling and Tao, Yunfeng and Zhang, Lei and Zhou, Jian and Yu, Yong},
  title     = {Providing an Uncertainty Reasoning Service for Semantic Web Application},
  booktitle = {Asia-Pacific Web Conference},
  year      = {2006},
  pages     = {628--639},
  file      = {:pubs/li2006providing - Providing an Uncertainty Reasoning Service for Semantic Web Application.pdf:PDF},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:./;}

@Comment{jabref-meta: saveOrderConfig:specified;year;true;month;true;booktitle;false;}
