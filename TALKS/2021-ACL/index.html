<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Pre-training Methods for Neural Machine Translation</title>
    <!--Generated by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->
    <!--Document created on .-->
    <link rel="stylesheet" href="index.css" type="text/css">
    <meta name="viewport" content="width=device-width, initial-scale=1,
      shrink-to-fit=no">
  </head>
  <body>
    <div class="ltx_page_main">
      <div class="ltx_page_content">
        <article class="ltx_document ltx_authors_1line">
          <h1 class="ltx_title ltx_title_document"><a
href="https://2021.aclweb.org/tutorials/program/#t4-pre-training-methods-for-neural-machine-translation">ACL
              2021 Tutorial</a><br class="ltx_break">
            Pre-training Methods for Neural Machine Translation</h1>
          <div class="ltx_authors">
            <table style="width: 100%" border="0">
              <tbody>
                <tr>
                  <td><span class="ltx_creator ltx_role_author"><span
                        class="ltx_personname"><a
                          href="https://scholar.google.com/citations?user=hOQ6G6EAAAAJ&amp;hl">Mingxuan

                          Wang</a> <br class="ltx_break">
                        ByteDance AI Lab </span></span></td>
                  <td><span class="ltx_creator ltx_role_author"><span
                        class="ltx_personname"><a
                          href="https://www.cs.ucsb.edu/~lilei">Lei Li</a>
                        <br class="ltx_break">
                        University of California Santa Barbara </span></span>
                  </td>
                </tr>
              </tbody>
            </table>
            <span class="ltx_creator ltx_role_author"> <span
                class="ltx_personname">August 1, 2021<br>
                [<a moz-do-not-send="true"
                  href="pre-training_nmt_ACL_tutorial_2021.pdf">Slides</a>]<br>
              </span></span> </div>
          <section id="S1" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"><span class="ltx_tag
                ltx_tag_section">1 </span>Tutorial Introduction</h2>
            <div id="S1.p1" class="ltx_para">
              <p class="ltx_p">Pre-training is a dominant paradigm in
                Nature Language Processing (NLP)&nbsp;<cite
                  class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib76" title="Language models are
                    unsupervised multitask learners" class="ltx_ref">28</a>,
                  <a href="#bib.bib24" title="BERT: pre-training of deep
                    bidirectional transformers for language
                    understanding" class="ltx_ref">8</a>, <a
                    href="#bib.bib127" title="RoBERTa: a robustly
                    optimized bert pretraining approach" class="ltx_ref">20</a>]</cite>,
                Computer Vision&nbsp;(CV)&nbsp;<cite class="ltx_cite
                  ltx_citemacro_cite">[<a href="#bib.bib121"
                    title="Rethinking imagenet pre-training"
                    class="ltx_ref">12</a>, <a href="#bib.bib136"
                    title="Self-training with noisy student improves
                    imagenet classification" class="ltx_ref">34</a>]</cite>
                and Auto Speech Recognition (ASR)&nbsp;<cite
                  class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib6" title="Pre-training on
                    high-resource speech recognition improves
                    low-resource speech-to-text translation"
                    class="ltx_ref">3</a>, <a href="#bib.bib119"
                    title="SpeechBERT: an audio-and-text jointly learned
                    language model for end-to-end spoken question
                    answering" class="ltx_ref">6</a>, <a
                    href="#bib.bib68" title="Specaugment: a simple data
                    augmentation method for automatic speech
                    recognition" class="ltx_ref">24</a>]</cite>.
                Typically, the models are first pre-trained on large
                amount of unlabeled data to capture rich representations
                of the input, and then applied to the downstream tasks
                by either providing context-aware representation of the
                input, or initializing the parameters of the downstream
                model for fine-tuning. Recently, the trend of
                self-supervised pre-training and task-specific
                fine-tuning finally fully hits neural machine
                translation (NMT)&nbsp;<cite class="ltx_cite
                  ltx_citemacro_cite">[<a href="#bib.bib138"
                    title="Incorporating BERT into neural machine
                    translation" class="ltx_ref">37</a>, <a
                    href="#bib.bib137" title="Towards making the most of
                    BERT in neural machine translation" class="ltx_ref">35</a>,
                  <a href="#bib.bib118" title="Distilling knowledge
                    learned in BERT for text generation" class="ltx_ref">5</a>]</cite>.</p>
            </div>
            <div id="S1.p2" class="ltx_para">
              <p class="ltx_p">Despite its success, introducing a
                universal pre-trained model to NMT is non-trivial and
                not necessarily yields promising results, especially for
                the resource-rich setup. Unique challenges remain in
                several aspects. First, the objective of most
                pretraining methods are different from the downstream
                NMT tasks. For example, BERT&nbsp;<cite class="ltx_cite
                  ltx_citemacro_cite">[<a href="#bib.bib24" title="BERT:
                    pre-training of deep bidirectional transformers for
                    language understanding" class="ltx_ref">8</a>]</cite>,
                a popular pre-trained model, is designed for language
                understanding with only a transformer encoder, while an
                NMT model usually consists of an encoder and a decoder
                to perform cross-lingual generation. This gap makes it
                not feasible enough to apply pre-training for NMT&nbsp;<cite
                  class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib134" title="MASS: masked sequence to
                    sequence pre-training for language generation"
                    class="ltx_ref">30</a>]</cite>. Besides, machine
                translation is naturally a multi-lingual problem, but
                general pre-training methods for NLP mainly focus on
                English corpus, such as BERT and GPT. Given the success
                of transfer learning in multi-lingual machine
                translation, it is very appealing to introduce
                multi-lingual pre-training for NMT&nbsp;<cite
                  class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib51" title="Cross-lingual language
                    model pretraining" class="ltx_ref">7</a>]</cite>.
                Finally, speech translation has attracted much attention
                recently, while most pre-training methods are focused on
                text representation. How to leverage the pre-training
                methods to improve the speech translation becomes a new
                challenge. </p>
            </div>
            <div id="S1.p3" class="ltx_para">
              <p class="ltx_p">This tutorial provides a comprehensive
                guide to make the most of pre-training for neural
                machine translation. Firstly, we will briefly introduce
                the background of NMT, pre-training methodology, and
                point out the main challenges when applying pre-training
                for NMT. Then we will focus on analysing the role of
                pre-training in enhancing the performance of NMT, how to
                design a better pre-training model for executing
                specific NMT tasks and how to better integrate the
                pre-trained model into NMT system. In each part, we will
                provide examples, discuss training techniques and
                analyse what is transferred when applying pre-training.</p>
            </div>
            <div id="S1.p4" class="ltx_para">
              <p class="ltx_p">The first topic is the <em
                  class="ltx_emph ltx_font_italic">monolingual
                  pre-training for NMT</em>, which is one of the most
                well-studied field. Monolingual text representations
                like ELMo, GPT, MASS and BERT have superiorities, which
                significantly boost the performances of various natural
                language processing tasks&nbsp;<cite class="ltx_cite
                  ltx_citemacro_cite">[<a href="#bib.bib71" title="Deep
                    contextualized word representations" class="ltx_ref">25</a>,
                  <a href="#bib.bib24" title="BERT: pre-training of deep
                    bidirectional transformers for language
                    understanding" class="ltx_ref">8</a>, <a
                    href="#bib.bib76" title="Language models are
                    unsupervised multitask learners" class="ltx_ref">28</a>,
                  <a href="#bib.bib134" title="MASS: masked sequence to
                    sequence pre-training for language generation"
                    class="ltx_ref">30</a>]</cite>. However, NMT has
                several distinct characteristics, such as the
                availability of large training data (10 million or
                larger) and the high capacity of baseline NMT models,
                which requires carefully design of pre-training. In this
                part, we will introduce different pre-training methods
                and analyse the best practice when applying them to
                different machine translation scenarios, such as
                unsupervised NMT, low-resource NMT and rich-source
                NMT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib138" title="Incorporating BERT into
                    neural machine translation" class="ltx_ref">37</a>,
                  <a href="#bib.bib137" title="Towards making the most
                    of BERT in neural machine translation"
                    class="ltx_ref">35</a>]</cite>. We will cover
                techniques to finetune the pre-trained models with
                various strategies, such as knowledge distillation and
                adapter&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib181" title="Simple, scalable
                    adaptation for neural machine translation"
                    class="ltx_ref">4</a>, <a href="#bib.bib153"
                    title="Finding sparse structure for domain specific
                    neural machine translation" class="ltx_ref">16</a>]</cite>.</p>
            </div>
            <div id="S1.p5" class="ltx_para">
              <p class="ltx_p">The next topic is <em class="ltx_emph
                  ltx_font_italic">multi-lingual pre-training for NMT</em>.
                In this context, we aims at mitigating the
                English-centric bias and suggest that it is possible to
                build universal representation for different language to
                improve massive multi-lingual NMT. In this part, we will
                discuss the general representation of different
                languages and analyse how knowledge transfers across
                languages. These will allow a better design for
                multi-lingual pre-training, in particular for zero-shot
                transfer to non-English language pairs&nbsp;<cite
                  class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib125" title="Google’s multilingual
                    neural machine translation system: enabling
                    zero-shot translation" class="ltx_ref">15</a>, <a
                    href="#bib.bib74" title="When and why are
                    pre-trained word embeddings useful for neural
                    machine translation?" class="ltx_ref">27</a>, <a
                    href="#bib.bib51" title="Cross-lingual language
                    model pretraining" class="ltx_ref">7</a>, <a
                    href="#bib.bib131" title="How multilingual is
                    multilingual BERT?" class="ltx_ref">26</a>, <a
                    href="#bib.bib123" title="Unicoder: a universal
                    language encoder by pre-training with multiple
                    cross-lingual tasks" class="ltx_ref">13</a>, <a
                    href="#bib.bib126" title="Pre-training multilingual
                    neural machine translation by leveraging alignment
                    information" class="ltx_ref">17</a>, <a
                    href="#bib.bib128" title="Multilingual denoising
                    pre-training for neural machine translation"
                    class="ltx_ref">19</a>, <a href="#bib.bib140"
                    title="Contrastive learning for many-to-many
                    multilingual neural machine translation"
                    class="ltx_ref">23</a>, <a href="#bib.bib139"
                    title="Learning language specific sub-network for
                    multilingual machine translation" class="ltx_ref">18</a>]</cite>.</p>
            </div>
            <div id="S1.p6" class="ltx_para">
              <p class="ltx_p">The last technical part of this tutorial
                deals with the <em class="ltx_emph ltx_font_italic">Pre-training
                  for speech NMT</em>. In particular, we focus on
                leverage weakly supervised or unsupervised training data
                to improve speech translation. In this part, we will
                discuss the possibilities of building a general
                representations across speech and text. And shows how
                text or audio pre-training can guild the text generation
                of NMT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a
                    href="#bib.bib135" title="VaTeX: A large-scale,
                    high-quality multilingual dataset for
                    video-and-language research" class="ltx_ref">33</a>,
                  <a href="#bib.bib58" title="End-to-end speech
                    translation with knowledge distillation"
                    class="ltx_ref">21</a>, <a href="#bib.bib6"
                    title="Pre-training on high-resource speech
                    recognition improves low-resource speech-to-text
                    translation" class="ltx_ref">3</a>, <a
                    href="#bib.bib105" title="Curriculum pre-training
                    for end-to-end speech translation" class="ltx_ref">32</a>,
                  <a href="#bib.bib214" title="Vq-wav2vec:
                    self-supervised learning of discrete speech
                    representations" class="ltx_ref">1</a>, <a
                    href="#bib.bib3" title="Wav2vec 2.0: A framework for
                    self-supervised learning of speech representations"
                    class="ltx_ref">2</a>, <a href="#bib.bib124"
                    title="M3P: learning universal representations via
                    multitask multilingual multimodal pre-training"
                    class="ltx_ref">14</a>, <a href="#bib.bib132"
                    title="Generative imagination elevates machine
                    translation" class="ltx_ref">22</a>, <a
                    href="#bib.bib32" title="Listen, understand and
                    translate: triple supervision decouples end-to-end
                    speech-to-text translation" class="ltx_ref">10</a>,
                  <a href="#bib.bib31" title="Consecutive decoding for
                    speech-to-text translation" class="ltx_ref">9</a>, <a
                    href="#bib.bib145" title="Learning shared semantic
                    space for speech-to-text translation"
                    class="ltx_ref">11</a>, <a href="#bib.bib318"
                    title="End-to-end speech translation via cross-modal
                    progressive training" class="ltx_ref">36</a>]</cite>.</p>
            </div>
            <div id="S1.p7" class="ltx_para">
              <p class="ltx_p">We conclude the tutorial by pointing out
                the best practice when applying pre-training for NMT.
                The topics cover various of pre-training methods for
                different NMT scenarios. After this tutorial, the
                audience will understand why pre-training for NMT is
                different from other tasks and how to make the most of
                pre-training for NMT. Importantly, we will give deep
                analyze about how and why pre-training works in NMT,
                which will inspire future work on designing pre-training
                paradigm specific for NMT.</p>
            </div>
          </section>
          <section id="S2" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"> <span
                class="ltx_tag ltx_tag_section">2 </span>Tutorial
              Outline</h2>
            <div id="S2.p1" class="ltx_para ltx_noindent">
              <p class="ltx_p"><span class="ltx_text ltx_font_bold">PART
                  I: Introduction</span> (15 min) <a
                  href="pre-training_nmt_ACL_tutorial_2021.pdf">[slides]</a></p>
              <ul id="S2.I1" class="ltx_itemize">
                <li id="S2.I1.i1" class="ltx_item">
                  <div id="S2.I1.i1.p1" class="ltx_para">
                    <p class="ltx_p">Background of NMT</p>
                  </div>
                </li>
                <li id="S2.I1.i2" class="ltx_item">
                  <div id="S2.I1.i2.p1" class="ltx_para">
                    <p class="ltx_p">General pre-training paradigm</p>
                  </div>
                </li>
                <li id="S2.I1.i3" class="ltx_item">
                  <div id="S2.I1.i3.p1" class="ltx_para">
                    <p class="ltx_p">Unique Challenges</p>
                    <ul id="S2.I1.I1" class="ltx_itemize">
                      <li id="S2.I1.i3.i1" class="ltx_item">
                        <div id="S2.I1.i3.i1.p1" class="ltx_para">
                          <p class="ltx_p">Objective difference</p>
                        </div>
                      </li>
                      <li id="S2.I1.i3.i2" class="ltx_item">
                        <div id="S2.I1.i3.i2.p1" class="ltx_para">
                          <p class="ltx_p">Multi-lingual generation</p>
                        </div>
                      </li>
                      <li id="S2.I1.i3.i3" class="ltx_item">
                        <div id="S2.I1.i3.i3.p1" class="ltx_para">
                          <p class="ltx_p">Modality disparity</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
              </ul>
            </div>
            <div id="S2.p2" class="ltx_para ltx_noindent">
              <p class="ltx_p"><span class="ltx_text ltx_font_bold">PART
                  II: Monolingual Pre-training for NMT</span> (60 min) <a
                  href="pre-training_nmt_ACL_tutorial_2021.pdf">[slides]</a></p>
              <ul id="S2.I2" class="ltx_itemize">
                <li id="S2.I2.i1" class="ltx_item">
                  <div id="S2.I2.i1.p1" class="ltx_para">
                    <p class="ltx_p">The early stage</p>
                    <ul id="S2.I2.I1" class="ltx_itemize">
                      <li id="S2.I2.i1.i1" class="ltx_item">
                        <div id="S2.I2.i1.i1.p1" class="ltx_para">
                          <p class="ltx_p">NMT initialized with word2vec</p>
                        </div>
                      </li>
                      <li id="S2.I2.i1.i2" class="ltx_item">
                        <div id="S2.I2.i1.i2.p1" class="ltx_para">
                          <p class="ltx_p">NMT initialized with language
                            model</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
                <li id="S2.I2.i2" class="ltx_item">
                  <div id="S2.I2.i2.p1" class="ltx_para">
                    <p class="ltx_p">BERT fusion in NMT</p>
                    <ul id="S2.I2.I2" class="ltx_itemize">
                      <li id="S2.I2.i2.i1" class="ltx_item">
                        <div id="S2.I2.i2.i1.p1" class="ltx_para">
                          <p class="ltx_p">BERT Incorporating methods</p>
                        </div>
                      </li>
                      <li id="S2.I2.i2.i2" class="ltx_item">
                        <div id="S2.I2.i2.i2.p1" class="ltx_para">
                          <p class="ltx_p">BERT Tuning methods</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
                <li id="S2.I2.i3" class="ltx_item">
                  <div id="S2.I2.i3.p1" class="ltx_para">
                    <p class="ltx_p">Unified sequence-to-sequence
                      pre-training</p>
                    <ul id="S2.I2.I3" class="ltx_itemize">
                      <li id="S2.I2.i3.i1" class="ltx_item">
                        <div id="S2.I2.i3.i1.p1" class="ltx_para">
                          <p class="ltx_p">MASS, Bart, etc.</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
              </ul>
              <p class="ltx_p"><span class="ltx_text ltx_font_bold">PART
                  III: Multi-lingual Pre-training for NMT</span> (45
                min) <a href="pre-training_nmt_ACL_tutorial_2021.pdf">[slides]</a></p>
              <ul id="S2.I3" class="ltx_itemize">
                <li id="S2.I3.i1" class="ltx_item">
                  <div id="S2.I3.i1.p1" class="ltx_para">
                    <p class="ltx_p">Multilingual fused pre-training</p>
                    <ul id="S2.I3.I1" class="ltx_itemize">
                      <li id="S2.I3.i1.i1" class="ltx_item">
                        <div id="S2.I3.i1.i1.p1" class="ltx_para">
                          <p class="ltx_p">Cross-lingual Language Model
                            Pre-training</p>
                        </div>
                      </li>
                      <li id="S2.I3.i1.i2" class="ltx_item">
                        <div id="S2.I3.i1.i2.p1" class="ltx_para">
                          <p class="ltx_p">Alternating Language Modeling
                            Pre-training</p>
                        </div>
                      </li>
                      <li id="S2.I3.i1.i3" class="ltx_item">
                        <div id="S2.I3.i1.i3.p1" class="ltx_para">
                          <p class="ltx_p">XLM-T: Cross-lingual
                            Transformer Encoders</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
                <li id="S2.I3.i2" class="ltx_item">
                  <div id="S2.I3.i2.p1" class="ltx_para">
                    <p class="ltx_p">Multilingual sequence to sequence
                      pre-training</p>
                    <ul id="S2.I3.I2" class="ltx_itemize">
                      <li id="S2.I3.i2.i1" class="ltx_item">
                        <div id="S2.I3.i2.i1.p1" class="ltx_para">
                          <p class="ltx_p">mBART</p>
                        </div>
                      </li>
                      <li id="S2.I3.i2.i2" class="ltx_item">
                        <div id="S2.I3.i2.i2.p1" class="ltx_para">
                          <p class="ltx_p">CSP</p>
                        </div>
                      </li>
                      <li id="S2.I3.i2.i3" class="ltx_item">
                        <div id="S2.I3.i2.i3.p1" class="ltx_para">
                          <p class="ltx_p">mRASP</p>
                        </div>
                      </li>
                    </ul>
                  </div>
                </li>
              </ul>
            </div>
            <div id="S2.p3" class="ltx_para ltx_noindent">
              <p class="ltx_p"><span class="ltx_text ltx_font_bold">PART
                  IV: Pre-training for Speech Translation</span> (45
                min) <a href="pre-training_nmt_ACL_tutorial_2021.pdf">[slides]</a></p>
              <ul id="S2.I4" class="ltx_itemize">
                <li id="S2.I4.i1" class="ltx_item">
                  <div id="S2.I4.i1.p1" class="ltx_para">
                    <p class="ltx_p">MT pre-training</p>
                  </div>
                </li>
                <li id="S2.I4.i2" class="ltx_item">
                  <div id="S2.I4.i2.p1" class="ltx_para">
                    <p class="ltx_p">ASR pre-training</p>
                  </div>
                </li>
                <li id="S2.I4.i3" class="ltx_item">
                  <div id="S2.I4.i3.p1" class="ltx_para">
                    <p class="ltx_p">Audio pre-training</p>
                  </div>
                </li>
                <li id="S2.I4.i4" class="ltx_item">
                  <div id="S2.I4.i4.p1" class="ltx_para">
                    <p class="ltx_p">Raw text pre-training </p>
                  </div>
                </li>
                <li id="S2.I4.i5" class="ltx_item">
                  <div id="S2.I4.i5.p1" class="ltx_para">
                    <p class="ltx_p">Bi-modal pre-training</p>
                  </div>
                </li>
              </ul>
              <p class="ltx_p"><span class="ltx_text ltx_font_bold">PART
                  V: Conclusion and Future Directions</span> (15 min)</p>
            </div>
          </section>
          <section id="S6" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"> <span
                class="ltx_tag ltx_tag_section">3 </span>Prerequisites</h2>
            <div id="S6.p1" class="ltx_para">
              <p class="ltx_p">The tutorial is self-contained. We will
                address the background, the technical details and the
                examples. Basic knowledge about neural networks are
                required, including word embeddings, attention, and
                encoder-decoder models. Prior NLP courses and familarity
                with the machine translation task are preferred.</p>
            </div>
            <div id="S6.p2" class="ltx_para">
              <p class="ltx_p">It is recommended (and optional) that
                audience to read the following papers before the
                tutorial: </p>
              <ol id="S6.I1" class="ltx_enumerate">
                <li id="S6.I1.i1" class="ltx_item">
                  <div id="S6.I1.i1.p1" class="ltx_para">
                    <p class="ltx_p">Basic MT model: Attention is all
                      you need&nbsp;<cite class="ltx_cite
                        ltx_citemacro_cite">[<a href="#bib.bib99"
                          title="Attention is all you need"
                          class="ltx_ref">31</a>]</cite>.</p>
                  </div>
                </li>
                <li id="S6.I1.i2" class="ltx_item">
                  <div id="S6.I1.i2.p1" class="ltx_para">
                    <p class="ltx_p">Google’s multilingual neural
                      machine translation system&nbsp;<cite
                        class="ltx_cite ltx_citemacro_cite">[<a
                          href="#bib.bib125" title="Google’s
                          multilingual neural machine translation
                          system: enabling zero-shot translation"
                          class="ltx_ref">15</a>]</cite>.</p>
                  </div>
                </li>
                <li id="S6.I1.i3" class="ltx_item">
                  <div id="S6.I1.i3.p1" class="ltx_para">
                    <p class="ltx_p">Text pre-training with BERT&nbsp;<cite
                        class="ltx_cite ltx_citemacro_cite">[<a
                          href="#bib.bib24" title="BERT: pre-training of
                          deep bidirectional transformers for language
                          understanding" class="ltx_ref">8</a>]</cite>
                      and GPT&nbsp;<cite class="ltx_cite
                        ltx_citemacro_cite">[<a href="#bib.bib76"
                          title="Language models are unsupervised
                          multitask learners" class="ltx_ref">28</a>]</cite>.</p>
                  </div>
                </li>
                <li id="S6.I1.i4" class="ltx_item">
                  <div id="S6.I1.i4.p1" class="ltx_para">
                    <p class="ltx_p">Audio pre-training with Wav2vec and
                      Wav2vec2.0&nbsp;<cite class="ltx_cite
                        ltx_citemacro_cite">[<a href="#bib.bib222"
                          title="Wav2vec: unsupervised pre-training for
                          speech recognition." class="ltx_ref">29</a>, <a
                          href="#bib.bib3" title="Wav2vec 2.0: A
                          framework for self-supervised learning of
                          speech representations" class="ltx_ref">2</a>]</cite>.</p>
                  </div>
                </li>
                <li id="S6.I1.i5" class="ltx_item">
                  <div id="S6.I1.i5.p1" class="ltx_para">
                    <p class="ltx_p">Pre-training multilingual NMT&nbsp;<cite
                        class="ltx_cite ltx_citemacro_cite">[<a
                          href="#bib.bib126" title="Pre-training
                          multilingual neural machine translation by
                          leveraging alignment information"
                          class="ltx_ref">17</a>, <a href="#bib.bib128"
                          title="Multilingual denoising pre-training for
                          neural machine translation" class="ltx_ref">19</a>]</cite>.</p>
                  </div>
                </li>
              </ol>
            </div>
          </section>
          <section id="S7" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"> <span
                class="ltx_tag ltx_tag_section">4 </span>Target
              Audience</h2>
            <div id="S7.p1" class="ltx_para">
              <p class="ltx_p">This tutorial will be suitable for
                researchers and practitioners interested in pre-training
                applications and multilingual NLP, especially for
                machine translation.</p>
            </div>
            <div id="S7.p2" class="ltx_para">
              <p class="ltx_p">To the best of our knowledge, this is the
                first tutorial that focuses on the pre-training methods
                and practice for NMT.</p>
            </div>
          </section>
          <section id="S10" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"> <span
                class="ltx_tag ltx_tag_section">5</span>Tutorial
              Presenters</h2>
            <section id="S10.SS0.SSS0.Px1" class="ltx_paragraph">
              <h3 class="ltx_title ltx_title_paragraph">Mingxuan Wang</h3>
              <div id="S10.SS0.SSS0.Px1.p1" class="ltx_para">
                <p class="ltx_p">(ByteDance AI Lab)</p>
              </div>
              <div id="S10.SS0.SSS0.Px1.p3" class="ltx_para">
                <p class="ltx_p">Dr. Mingxuan Wang is a senior
                  researcher at ByteDance AI Lab. He received his PhD
                  degree from the Chinese Academy of Sciences Institute
                  of Computing Technology in 2017. His research focuses
                  on natural language processing and machine
                  translation. He has published over 20 papers in
                  leading NLP/AI journals and conferences such as ACL,
                  AAAI and EMNLP. He has served in the Program Committee
                  for ACL/EMNLP 2016-2020, AAAI/IJCAI 2018/2019, NeurIPS
                  2020. He achieved outstanding results in various
                  machine translation evaluation competitions, including
                  the first place of Chinese-to-English translation at
                  at the WMT 2018, the third place of Chinese-to-English
                  translation at NIST 2015, etc. Together with Dr. Lei
                  Li, he is leading a team developing the VolcTrans
                  machine translation system.</p>
              </div>
              <div id="S10.SS0.SSS0.Px1.p4" class="ltx_para">
                <p class="ltx_p">He has given a tutorial about Machine
                  Translation at CCMT 2017 and was an guest lecturer for
                  2016 Machine Translation for University of Chinese
                  Academy of Sciences (UCAS).</p>
              </div>
            </section>
            <section id="S10.SS0.SSS0.Px2" class="ltx_paragraph">
              <h3 class="ltx_title ltx_title_paragraph">Lei Li</h3>
              <div id="S10.SS0.SSS0.Px2.p1" class="ltx_para">
                <p class="ltx_p">(UCSB)</p>
              </div>
              <div id="S10.SS0.SSS0.Px2.p3" class="ltx_para">
                <p class="ltx_p">Dr. Lei Li is an assistant professor in
                  Computer Science Department at University of
                  California Santa Barbara. His research interest lies
                  in natural language processing, machine translation,
                  and AI-powered drug discovery. He received his B.S.
                  from Shanghai Jiao Tong University and Ph.D. from
                  Carnegie Mellon University. His dissertation work on
                  fast algorithms for mining co-evolving time series was
                  awarded ACM KDD best dissertation (runner up). His
                  recent work on AI writer Xiaomingbot received
                  2nd-class award of Wu Wen-tsün AI prize in 2017. He is
                  a recipient of ACL 2021 best paper award, CCF Young
                  Elite award in 2019, and CCF distinguished speaker in
                  2017. His team won first places for five language
                  translation directions in WMT 2020 and the best in
                  corpus filtering challenge. Previously, he worked at
                  EECS department of UC Berkeley, Baidu’s Institute of
                  Deep Learning in Silicon Valley, and at ByteDance as
                  the founding director of AI Lab. He has served
                  organizers and area chair/senior PC for multiple
                  conferences including KDD, ACL, EMNLP, NeurIPS, AAAI,
                  IJCAI, and CIKM. He has published over 100 technical
                  papers in ML, NLP and data mining and holds more than
                  10 patents. He has started ByteDance’s machine
                  translation system, VolcTrans and many of his
                  algorithms have been deployed in production.</p>
              </div>
              <div id="S10.SS0.SSS0.Px2.p4" class="ltx_para">
                <p class="ltx_p">He has delivered four tutorials at
                  EMNLP 2019, NLPCC 2019, NLPCC 2016, and KDD 2010. He
                  was an lecturer for 2014 Probabilistic Programming for
                  Advancing Machine Learning summer school at Portland,
                  USA.</p>
              </div>
            </section>
          </section>
          <section id="S11" class="ltx_section">
            <h2 class="ltx_title ltx_title_section"> <span
                class="ltx_tag ltx_tag_section">6 </span>Related
              Tutorials</h2>
            <section id="S11.SS0.SSS0.Px1" class="ltx_paragraph">
              <div id="S11.SS0.SSS0.Px1.p1" class="ltx_para">
                <p class="ltx_p">Neural Machine Translation, presented
                  by Thang Luong, Kyunghyun Cho, and Christopher Manning
                  at ACL 2016. This tutorial is related but different
                  from ACL 2016 NMT tutorial. It focuses on pre-training
                  methods for both bilingual, multi-lingual, and
                  multi-modal neural machine translation.</p>
              </div>
              <div id="S11.SS0.SSS0.Px1.p2" class="ltx_para">
                <p class="ltx_p">Unsupervised Cross-Lingual
                  Representation Learning, presented by Sebastian Ruder,
                  Anders Søgaard, and Ivan Vulić at ACL 2019. This
                  tutorial is related in concerning multi-lingual NLP.
                  However, their tutorial was on representation
                  learning, while our tutorial is on neural machine
                  translation. </p>
              </div>
            </section>
          </section>
          <section id="bib" class="ltx_bibliography">
            <h2 class="ltx_title ltx_title_bibliography">References</h2>
            <ul id="bib.L1" class="ltx_biblist">
              <li id="bib.bib214" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Baevski, S. Schneider, and M.
                    Auli</span><span class="ltx_text ltx_bib_year">
                    (2020)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">Vq-wav2vec:
                    self-supervised learning of discrete speech
                    representations</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ICLR</span>, </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib3" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Baevski, Y. Zhou, A. Mohamed, and
                    M. Auli</span><span class="ltx_text ltx_bib_year">
                    (2020)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">Wav2vec 2.0: A
                    framework for self-supervised learning of speech
                    representations</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NeurIPS</span>, <span
                    class="ltx_text ltx_bib_editor">H. Larochelle, M.
                    Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.)</span>,
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i4.p1" title="item 4 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;4</span></a>. </span> </li>
              <li id="bib.bib6" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">S. Bansal, H. Kamper, K. Livescu, A.
                    Lopez, and S. Goldwater</span><span class="ltx_text
                    ltx_bib_year"> (2019)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Pre-training on high-resource speech
                    recognition improves low-resource speech-to-text
                    translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NAACL-HLT</span>, </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;58–68</span>. </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib181" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Bapna and O. Firat</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Simple, scalable adaptation for
                    neural machine translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of EMNLP</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;1538–1548</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib118" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Chen, Z. Gan, Y. Cheng, J. Liu,
                    and J. Liu</span><span class="ltx_text ltx_bib_year">
                    (2020)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">Distilling knowledge
                    learned in BERT for text generation</span>. </span>
                <span class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ACL</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;7893–7905</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib119" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Chuang, C. Liu, and H. Lee</span><span
                    class="ltx_text ltx_bib_year"> (2020)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">SpeechBERT: an audio-and-text jointly
                    learned language model for end-to-end spoken
                    question answering</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of INTERSPEECH</span>, </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib51" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Conneau and G. Lample</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Cross-lingual language model
                    pretraining</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NeurIPS</span>, <span
                    class="ltx_text ltx_bib_editor">H. M. Wallach, H.
                    Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B.
                    Fox, and R. Garnett (Eds.)</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;7057–7067</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p2" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib24" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">J. Devlin, M. Chang, K. Lee, and K.
                    Toutanova</span><span class="ltx_text ltx_bib_year">
                    (2019)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">BERT: pre-training of
                    deep bidirectional transformers for language
                    understanding</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NAACL-HLT</span>, </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;4171–4186</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p2" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i3.p1" title="item 3 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;3</span></a>. </span> </li>
              <li id="bib.bib31" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Q. Dong, M. Wang, H. Zhou, S. Xu, B.
                    Xu, and L. Li</span><span class="ltx_text
                    ltx_bib_year"> (2021)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Consecutive decoding for
                    speech-to-text translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of AAAI</span>, </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib32" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Q. Dong, R. Ye, M. Wang, H. Zhou, S.
                    Xu, B. Xu, and L. Li</span><span class="ltx_text
                    ltx_bib_year"> (2021)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Listen, understand and translate:
                    triple supervision decouples end-to-end
                    speech-to-text translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of AAAI</span>, </span> <span
                  class="ltx_bibblock">Vol. <span class="ltx_text
                    ltx_bib_volume">35</span>, <span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;12749–12759</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib145" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">C. Han, M. Wang, H. Ji, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2021-08)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Learning shared semantic space for
                    speech-to-text translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ACL - Findings</span>, </span>
                <span class="ltx_bibblock">External Links: <span
                    class="ltx_text ltx_bib_links"><span class="ltx_text
                      ltx_bib_external"><span><a
                          href="https://arxiv.org/abs/2105.03095">https://arxiv.org/abs/2105.03095</a></span></span></span>
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib121" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">K. He, R. B. Girshick, and P. Dollár</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Rethinking imagenet pre-training</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of ICCV</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;4917–4926</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib123" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">H. Huang, Y. Liang, N. Duan, M.
                    Gong, L. Shou, D. Jiang, and M. Zhou</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Unicoder: a universal language
                    encoder by pre-training with multiple cross-lingual
                    tasks</span>. </span> <span class="ltx_bibblock">In

                  <span class="ltx_text ltx_bib_inbook">Proc. of EMNLP</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;2485–2494</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib124" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">H. Huang, L. Su, D. Qi, N. Duan, E.
                    Cui, T. Bharti, L. Zhang, L. Wang, J. Gao, B. Liu, <span
                      class="ltx_text ltx_bib_etal">et al.</span></span><span
                    class="ltx_text ltx_bib_year"> (2021)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">M3P: learning universal
                    representations via multitask multilingual
                    multimodal pre-training</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of CVPR</span>, </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib125" class="ltx_bibitem ltx_bib_article"> <span
                  class="ltx_tag ltx_bib_key ltx_role_refnum
                  ltx_tag_bibitem">[15]</span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">M. Johnson, M. Schuster, Q. V. Le,
                    M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Viégas, M.
                    Wattenberg, G. Corrado, M. Hughes, and J. Dean</span><span
                    class="ltx_text ltx_bib_year"> (2017)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Google’s multilingual neural machine
                    translation system: enabling zero-shot translation</span>.
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_journal">TACL</span> <span
                    class="ltx_text ltx_bib_volume">5</span>, <span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;339–351</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i2.p1" title="item 2 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;2</span></a>. </span> </li>
              <li id="bib.bib153" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">J. Liang, C. Zhao, M. Wang, X. Qiu,
                    and L. Li</span><span class="ltx_text ltx_bib_year">
                    (2021-02)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Finding sparse structure for domain
                    specific neural machine translation</span>. </span>
                <span class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of AAAI</span>, </span> <span
                  class="ltx_bibblock">External Links: <span
                    class="ltx_text ltx_bib_links"><span class="ltx_text
                      ltx_bib_external"><span><a
                          href="https://arxiv.org/abs/2012.10586">https://arxiv.org/abs/2012.10586</a></span></span>,
                    <a
                      href="https://ohlionel.github.io/project/Prune-Tune/"
                      title="" class="ltx_ref ltx_bib_external">Link</a></span>
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib126" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Z. Lin, X. Pan, M. Wang, X. Qiu, J.
                    Feng, H. Zhou, and L. Li</span><span class="ltx_text
                    ltx_bib_year"> (2020)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Pre-training multilingual neural
                    machine translation by leveraging alignment
                    information</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of EMNLP</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;2649–2663</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i5.p1" title="item 5 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;5</span></a>. </span> </li>
              <li id="bib.bib139" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Z. Lin, L. Wu, M. Wang, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2021-08)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Learning language specific
                    sub-network for multilingual machine translation</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of ACL</span>,
                </span> <span class="ltx_bibblock">External Links: <span
                    class="ltx_text ltx_bib_links"><span class="ltx_text
                      ltx_bib_external"><span><a
                          href="https://arxiv.org/abs/2105.09259">https://arxiv.org/abs/2105.09259</a></span></span></span>
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib128" class="ltx_bibitem ltx_bib_article"> <span
                  class="ltx_tag ltx_bib_key ltx_role_refnum
                  ltx_tag_bibitem">[19]</span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Liu, J. Gu, N. Goyal, X. Li, S.
                    Edunov, M. Ghazvininejad, M. Lewis, and L.
                    Zettlemoyer</span><span class="ltx_text
                    ltx_bib_year"> (2020)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Multilingual denoising pre-training
                    for neural machine translation</span>. </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_journal">TACL</span> <span class="ltx_text
                    ltx_bib_volume">8</span>, <span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;726–742</span>. </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i5.p1" title="item 5 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;5</span></a>. </span> </li>
              <li id="bib.bib127" class="ltx_bibitem ltx_bib_article"> <span
                  class="ltx_tag ltx_bib_key ltx_role_refnum
                  ltx_tag_bibitem">[20]</span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Liu, M. Ott, N. Goyal, J. Du, M.
                    Joshi, D. Chen, O. Levy, M. Lewis, L. S.
                    Zettlemoyer, and V. Stoyanov</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">RoBERTa: a robustly optimized bert
                    pretraining approach</span>. </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_journal">ArXiv</span> <span class="ltx_text
                    ltx_bib_volume">abs/1907.11692</span>. </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib58" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Liu, H. Xiong, J. Zhang, Z. He,
                    H. Wu, H. Wang, and C. Zong</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">End-to-end speech translation with
                    knowledge distillation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of INTERSPEECH</span>, <span
                    class="ltx_text ltx_bib_editor">G. Kubin and Z.
                    Kacic (Eds.)</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;1128–1132</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib132" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Q. Long, M. Wang, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2021)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Generative imagination elevates
                    machine translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NAACL-HLT</span>, </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;5738–5748</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib140" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">X. Pan, L. Wu, M. Wang, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2021-08)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Contrastive learning for many-to-many
                    multilingual neural machine translation</span>. </span>
                <span class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ACL</span>, </span> <span
                  class="ltx_bibblock">External Links: <span
                    class="ltx_text ltx_bib_links"><span class="ltx_text
                      ltx_bib_external"><span><a
                          href="https://arxiv.org/abs/2105.09501">https://arxiv.org/abs/2105.09501</a></span></span></span>
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib68" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">D. S. Park, W. Chan, Y. Zhang, C.
                    Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Specaugment: a simple data
                    augmentation method for automatic speech recognition</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of INTERSPEECH</span>,
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib71" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">M. Peters, M. Neumann, M. Iyyer, M.
                    Gardner, C. Clark, K. Lee, and L. Zettlemoyer</span><span
                    class="ltx_text ltx_bib_year"> (2018)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Deep contextualized word
                    representations</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NAACL-HLT</span>, </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;2227–2237</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib131" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">T. Pires, E. Schlinger, and D.
                    Garrette</span><span class="ltx_text ltx_bib_year">
                    (2019)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">How multilingual is
                    multilingual BERT?</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ACL</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;4996–5001</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib74" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Y. Qi, D. Sachan, M. Felix, S.
                    Padmanabhan, and G. Neubig</span><span
                    class="ltx_text ltx_bib_year"> (2018)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">When and why are pre-trained word
                    embeddings useful for neural machine translation?</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of NAACL-HLT</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;529–535</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p5" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib76" class="ltx_bibitem ltx_bib_article"> <span
                  class="ltx_tag ltx_bib_key ltx_role_refnum
                  ltx_tag_bibitem">[28]</span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Radford, J. Wu, R. Child, D.
                    Luan, D. Amodei, and I. Sutskever</span><span
                    class="ltx_text ltx_bib_year"> (2019)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Language models are unsupervised
                    multitask learners</span>. </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_journal">OpenAI Blog</span> <span
                    class="ltx_text ltx_bib_volume">1</span> (<span
                    class="ltx_text ltx_bib_number">8</span>). </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S6.I1.i3.p1" title="item 3 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;3</span></a>. </span> </li>
              <li id="bib.bib222" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">S. Schneider, A. Baevski, R.
                    Collobert, and M. Auli</span><span class="ltx_text
                    ltx_bib_year"> (2019)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Wav2vec: unsupervised pre-training
                    for speech recognition.</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of INTERSPEECH</span>, </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S6.I1.i4.p1" title="item 4 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;4</span></a>. </span> </li>
              <li id="bib.bib134" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">K. Song, X. Tan, T. Qin, J. Lu, and
                    T. Liu</span><span class="ltx_text ltx_bib_year">
                    (2019)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">MASS: masked sequence
                    to sequence pre-training for language generation</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of ICML</span>,
                  <span class="ltx_text ltx_bib_editor">K. Chaudhuri and
                    R. Salakhutdinov (Eds.)</span>, </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_series">Proceedings of Machine Learning
                    Research</span>, Vol. <span class="ltx_text
                    ltx_bib_volume">97</span>, <span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;5926–5936</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p2" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib99" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">A. Vaswani, N. Shazeer, N. Parmar,
                    J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
                    I. Polosukhin</span><span class="ltx_text
                    ltx_bib_year"> (2017)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Attention is all you need</span>. </span>
                <span class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of NeurIPS</span>, <span
                    class="ltx_text ltx_bib_editor">I. Guyon, U. von
                    Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V.
                    N. Vishwanathan, and R. Garnett (Eds.)</span>, </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_pages"> pp.&nbsp;5998–6008</span>. </span>
                <span class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S6.I1.i1.p1" title="item 1 ‣ 6 Prerequisites
                    ‣ Pre-training Methods for Neural Machine
                    Translation" class="ltx_ref"><span class="ltx_text
                      ltx_ref_tag">item&nbsp;1</span></a>. </span> </li>
              <li id="bib.bib105" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">C. Wang, Y. Wu, S. Liu, M. Zhou, and
                    Z. Yang</span><span class="ltx_text ltx_bib_year">
                    (2020)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">Curriculum
                    pre-training for end-to-end speech translation</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of ACL</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;3728–3738</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib135" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">X. Wang, J. Wu, J. Chen, L. Li, Y.
                    Wang, and W. Y. Wang</span><span class="ltx_text
                    ltx_bib_year"> (2019)</span> </span> <span
                  class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">VaTeX: A large-scale, high-quality
                    multilingual dataset for video-and-language research</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of ICCV</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;4580–4590</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib136" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">Q. Xie, M. Luong, E. H. Hovy, and Q.
                    V. Le</span><span class="ltx_text ltx_bib_year">
                    (2020)</span> </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_title">Self-training with
                    noisy student improves imagenet classification</span>.
                </span> <span class="ltx_bibblock">In <span
                    class="ltx_text ltx_bib_inbook">Proc. of CVPR</span>,
                </span> <span class="ltx_bibblock"><span
                    class="ltx_text ltx_bib_pages"> pp.&nbsp;10684–10695</span>.
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib137" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">J. Yang, M. Wang, H. Zhou, C. Zhao,
                    W. Zhang, Y. Yu, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2020)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Towards making the most of BERT in
                    neural machine translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of AAAI</span>, </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib318" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">R. Ye, M. Wang, and L. Li</span><span
                    class="ltx_text ltx_bib_year"> (2021-08)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">End-to-end speech translation via
                    cross-modal progressive training</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of INTERSPEECH</span>, </span>
                <span class="ltx_bibblock">External Links: <span
                    class="ltx_text ltx_bib_links"><span class="ltx_text
                      ltx_bib_external"><span><a
                          href="https://arxiv.org/abs/2104.10380">https://arxiv.org/abs/2104.10380</a></span></span></span>
                </span> <span class="ltx_bibblock ltx_bib_cited">Cited
                  by: <a href="#S1.p6" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
              <li id="bib.bib138" class="ltx_bibitem
                ltx_bib_inproceedings"> <span class="ltx_tag
                  ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_author">J. Zhu, Y. Xia, L. Wu, D. He, T.
                    Qin, W. Zhou, H. Li, and T. Liu</span><span
                    class="ltx_text ltx_bib_year"> (2020)</span> </span>
                <span class="ltx_bibblock"><span class="ltx_text
                    ltx_bib_title">Incorporating BERT into neural
                    machine translation</span>. </span> <span
                  class="ltx_bibblock">In <span class="ltx_text
                    ltx_bib_inbook">Proc. of ICLR</span>, </span> <span
                  class="ltx_bibblock ltx_bib_cited">Cited by: <a
                    href="#S1.p1" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>,
                  <a href="#S1.p4" title="1 Tutorial Introduction ‣
                    Pre-training Methods for Neural Machine Translation"
                    class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
                </span> </li>
            </ul>
          </section>
        </article>
      </div>
      <footer class="ltx_page_footer">
        <div class="ltx_page_logo">Generated by <a
            href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img
src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="
              alt="[LOGO]"></a> </div>
      </footer>
    </div>
    <script src="index.js" type="text/javascript"></script>
  </body>
</html>
