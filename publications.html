<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en">
<head>
<title>Lei Li's publications</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 2.0
//
// Copyright (c) 2006-2008, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/

// Some features:
// + optionally searches Abstracts and Reviews
// + allows RegExp searches
//   e.g. to search for entries between 1980 and 1989, type:  198[0-9]
//   e.g. for any entry ending with 'symmetry', type:  symmetry$
//   e.g. for all reftypes that are books: ^book$, or ^article$
//   e.g. for entries by either John or Doe, type john|doe
// + easy toggling of Abstract/Review/BibTeX

// Search settings
var searchAbstract = true;
var searchReview = true;

// Speed optimisation introduced some esoteric problems with certain RegExp searches
// e.g. if the previous search is 200[-7] and the next search is 200[4-7] then the search doesn't work properly until the next 'keyup'
// hence the searchOpt can be turned off for RegExp adepts
var searchOpt = true;

if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// basic object detection
	if(!document.getElementById || !document.getElementsByTagName) { return; }
	if (!document.getElementById('qstable')||!document.getElementById('qs')) { return; }

	// find QS table and appropriate rows
	searchTable = document.getElementById('qstable');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array();
	infoRows = new Array(); absRows = new Array(); revRows = new Array();

	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j++] = allRows[i];
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
			}
		}
	}

	//number of entries and rows
	numRows = allRows.length;
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;

	//find the query field
	qsfield = document.getElementById('qsfield');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);

	// creates the appropriate search settings
	createQSettingsDialog();

	// shows the searchfield
	document.getElementById('qs').style.display = 'block';
	document.getElementById('qsfield').onkeyup = testEvent;
}

function quickSearch(tInput){

	 if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		// only search for valid RegExp
		try {
			var searchText = new RegExp(tInput.value,"i")
			closeAllInfo();
			qsfield.className = '';
		}
		catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		// some further optimisation is possible: if the search string is getting shorter, and the row is already visible, skip it. Then be careful with hits!
		if(!searchOpt || cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			var inCells = cRow.getElementsByTagName('td');
			var numCols = inCells.length;
				
			for (var j=0; j<numCols; j++) {
				cCell = inCells[j];
				var t = cCell.innerText?cCell.innerText:getTextContent(cCell);
				if (t.search(searchText) != -1){ 
					found=true; 
					break;
				} 
			}

			// look for further hits in Abstract and Review
			if(!found) {
				var articleid = cRow.id;
				if(searchAbstract && (abs = document.getElementById('abs_'+articleid))) {
					if (getTextContent(abs).search(searchText) != -1){ found=true; } 
				}
				if(searchReview && (rev = document.getElementById('rev_'+articleid))) {
					if (getTextContent(rev).search(searchText) != -1){ found=true; } 
				}
			}
			
			if(found) {
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		if(abs.className.indexOf('abstract') != -1) {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract';
		}
	} else if (rev && info == 'review') {
		if(rev.className.indexOf('review') != -1) {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review';
		}
	} else if (bib && info == 'bibtex') {
		if(bib.className.indexOf('bibtex') != -1) {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
		}		
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow = false;
	var absshow = false;
	var bibshow = false;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className == 'bibtex')? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}		
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1) { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	// first close all abstracts, reviews, etc.
	closeAllInfo();

	for (var i = 0; i < numEntries; i++){
		entryRows[i].className = 'entry show'; 
	}
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function testEvent(e){
	if (!e) var e = window.event;
	quickSearch(this);
}

function clearQS() {
	qsfield.value = '';
	quickSearch(qsfield);
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

// Create Search Settings

function toggleQSettingsDialog() {

	var qssettings = document.getElementById('qssettings');
	
	if(qssettings.className.indexOf('active')==-1) {
		qssettings.className = 'active';

		if(absCheckBox && searchAbstract == true) { absCheckBox.checked = 'checked'; }
		if(revCheckBox && searchReview == true) { revCheckBox.checked = 'checked'; }

	} else {
		qssettings.className= '';
	}
}

function createQSettingsDialog(){
	var qssettingslist = document.getElementById('qssettings').getElementsByTagName('ul')[0];
	
	if(numAbs!=0) {
		var x = document.createElement('input');
		x.id = "searchAbs";
		x.type = "checkbox";
		x.onclick = toggleQSetting;
		var y = qssettingslist.appendChild(document.createElement('li')).appendChild(document.createElement('label'));
		y.appendChild(x);
		y.appendChild(document.createTextNode('search abstracts'));		
	}
	if(numRev!=0) {
		var x = document.createElement('input');
		x.id = "searchRev";
		x.type = "checkbox";		
		x.onclick = toggleQSetting;
		var y = qssettingslist.appendChild(document.createElement('li')).appendChild(document.createElement('label'));		
		y.appendChild(x);		
		y.appendChild(document.createTextNode('search reviews'));
	}
		
	// global variables
	absCheckBox = document.getElementById('searchAbs');
	revCheckBox = document.getElementById('searchRev');
	
	// show search settings
	if(absCheckBox||revCheckBox) {
		document.getElementById('qssettings').style.display = 'block';
	}
}

function toggleQSetting() {
	if(this.id=='searchAbs') { searchAbstract = !searchAbstract; }
	if(this.id=='searchRev') { searchReview = !searchReview; }
	redoQS()
} 
-->
</script>	
<style type="text/css">
body { background-color: white; font-family: "Trebuchet MS", Arial, sans-serif; font-size: 14px; line-height: 1.2; padding: 1em; color: #2E2E2E; width: 50em; margin: auto auto; }

#qs { width: auto; border-style: solid; border-color: gray; border-width: 1px 1px 1px 1px; padding: 0.5em 0.5em; display:none; position:relative; }
#qs form { padding: 0px; margin: 0px; }
#qs form p { padding: 0px; margin: 0px; }

.invalidsearch { background-color: red; }

table { border: 1px gray none; width: 100%; empty-cells: show; border-spacing: 0em 0.3em; margin-top: 1em; }
th, td { border-width: 0px 0px; border-color: gray; border-style: solid; padding: 0.5em; vertical-align: top;  }
td { text-align: justify; vertical-align: top; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}

tr.highlight td { background-color: #F1F1F1; border-top: 2px black solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #F1F1F1; border-bottom: 2px black solid; text-align: justify; }
tr.nextshow td { border-bottom: none; }

tr.bibtex pre { width: 100%; overflow: auto;}

p.infolinks { margin: 0.5em 0em 0em 0em; padding: 0px; }

#qssettings { padding: 0.5em; position: absolute; top: 0.2em; right: 0.2em; border: 1px gray solid; background-color: white; display: none; }
#qssettings p { font-weight: bold; cursor: pointer; }
#qssettings ul { display: none; list-style-type: none; padding-left: 0; margin: 0; }
#qssettings.active ul { display: block; }

@media print {
	p.infolinks, #qssettings, #qs { display: none !important; }
	tr { page-break-inside: avoid; }
}

.note {color: green; font-size: 67%; text-indent:2em; margin-top:1px; margin-bottom:-3px;}
</style>
</head>
<body>

<div id="qs">
	<form action="">
	<p>QuickSearch: <input type="text" name="qsfield" id="qsfield" autocomplete="off" title="Allows plain text as well as RegExp searches" /><input type="button" onclick="clearQS()" value="clear" />&nbsp; Number of matching entries: <span id="stat">0</span>.</p>
	<div id="qssettings">
		<p onclick="toggleQSettingsDialog()">Search Settings</p>
		<ul>
		</ul>
	</div>
	</form>
</div>
<table id="qstable" border="1">
<tbody>
<tr id="chu2021icm" class="entry">
	<td>Ruihang Chu, Yukang Chen, Tao Kong, Lu Qi and Lei Li, <a href="pubs/chu2021icm.pdf"><i>"ICM-3D: Instantiated Category Modeling for 3D Instance Segmentation"</i></a>, IEEE Robotics and Automation Letters (RA-L), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chu2021icm','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chu2021icm" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{chu2021icm,
  author = {Ruihang Chu and Yukang Chen and Tao Kong and Lu Qi and Lei Li},
  title = {ICM-3D: Instantiated Category Modeling for 3D Instance Segmentation},
  journal = {IEEE Robotics and Automation Letters (RA-L)},
  year = {2021}
}
</pre></td>
</tr>
<tr id="wang2021solo" class="entry">
	<td>Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong and Lei Li, <a href="pubs/wang2021solo.pdf"><i>"SOLO: A Simple Framework for Instance Segmentation"</i></a>, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021solo','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_wang2021solo" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{wang2021solo,
  author = {Xinlong Wang and Rufeng Zhang and Chunhua Shen and Tao Kong and Lei Li},
  title = {SOLO: A Simple Framework for Instance Segmentation},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year = {2021}
}
</pre></td>
</tr>
<tr id="jiang2021learning" class="entry">
	<td>Qingnan Jiang, Mingxuan Wang, Jun Cao, Shanbo Cheng, Shujian Huang and Lei Li, <a href="pubs/jiang2021learning.pdf"><i>"Learning Kernel-Smoothed Machine Translation with Retrieved Examples"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('jiang2021learning','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('jiang2021learning','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_jiang2021learning" class="abstract noshow">
	<td><b>Abstract</b>: How to effectively adapt neural machine translation  (NMT)  models  according  to  emergingcases  without  retraining?    Despite  the  greatsuccess  of  neural  machine  translation,  updating the deployed models online remains a challenge. Existing  non-parametric  approachesthat retrieve similar examples from a databaseto guide the translation process are promisingbut are prone to overfit the retrieved examples. However,  non-parametric  methods  are  proneto overfit the retrieved examples. In this work,we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online.  Experiments on domainadaptation and multi-domain machine translation  datasets  show  that  even  without  expensive retraining, KSTER is able to achieve im-provement  of  1.1  to  1.5  BLEU  scores  overthe  best  existing  online  adaptation  methods. The  code  and  trained  models  are  released  at https://github.com/jiangqn/KSTER.</td>
</tr>

<tr id="bib_jiang2021learning" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{jiang2021learning,
  author = {Qingnan Jiang and Mingxuan Wang and Jun Cao and Shanbo Cheng and Shujian Huang and Lei Li},
  title = {Learning Kernel-Smoothed Machine Translation with Retrieved Examples},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2021}
}
</pre></td>
</tr><tr id="ru2021learning" class="entry">
	<td>Dongyu Ru, Changzhi Sun, Jiangtao Feng, Lin Qiu, Hao Zhou, Weinan Zhang, Yong Yu and Lei Li, <a href="pubs/ru2021learning.pdf"><i>"Learning Logic Rules for Document-level Relation Extraction"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('ru2021learning','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('ru2021learning','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_ru2021learning" class="abstract noshow">
	<td><b>Abstract</b>: Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation--maximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that LogiRE significantly outperforms several strong baselines in terms of relation performance (∼1.8 F1 score) and logical consistency (over 3.3 logic score). Our code is available at https://github. com/rudongyu/LogiRE.</td>
</tr>

<tr id="bib_ru2021learning" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{ru2021learning,
  author = {Dongyu Ru and Changzhi Sun and Jiangtao Feng and Lin Qiu and Hao Zhou and Weinan Zhang and Yong Yu and Lei Li},
  title = {Learning Logic Rules for Document-level Relation Extraction},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2021}
}
</pre></td>
</tr><tr id="sun2021multilingual" class="entry">
	<td>Zewei Sun, Mingxuan Wang and Lei Li, <a href="pubs/sun2021multilingual.pdf"><i>"Multilingual Translation via Grafting Pre-trained Language Models"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sun2021multilingual','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_sun2021multilingual" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sun2021multilingual,
  author = {Zewei Sun and Mingxuan Wang and Lei Li},
  title = {Multilingual Translation via Grafting Pre-trained Language Models},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="wang2021secoco" class="entry">
	<td>Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, Hang Li and Deyi Xiong, <a href="pubs/wang2021secoco.pdf"><i>"Secoco: Self-Correcting Encoding for Neural Machine Translation"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2021secoco','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2021secoco','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2021secoco" class="abstract noshow">
	<td><b>Abstract</b>: Different from previous robust approaches, Secoco enables NMT to explicitly correct noisy inputs and delete specific errors simultaneously with the translation decoding process. Secoco is able to achieve significant improvements of 1.6 BLEU points over strong baselines on two real-world test sets and a benchmark WMT dataset  with good interpretability.<br>The code and dataset are publicly available at https://github.com/rgwt123/Secoco.</td>
</tr>

<tr id="bib_wang2021secoco" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021secoco,
  author = {Tao Wang and Chengqi Zhao and Mingxuan Wang and Lei Li and Hang Li and Deyi Xiong},
  title = {Secoco: Self-Correcting Encoding for Neural Machine Translation},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="zeng2021gradient" class="entry">
	<td>Zhiyuan Zeng, Jiaze Chen, Weiran Xu and Lei Li, <a href="pubs/zeng2021gradient.pdf"><i>"Gradient-based Adversarial Factual Consistency Evaluation for Abstractive Summarization"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zeng2021gradient','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_zeng2021gradient" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zeng2021gradient,
  author = {Zhiyuan Zeng and Jiaze Chen and Weiran Xu and Lei Li},
  title = {Gradient-based Adversarial Factual Consistency Evaluation for Abstractive Summarization},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2021}
}
</pre></td>
</tr><tr id="zhu2021counter" class="entry">
	<td>Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang and Lei Li, <a href="pubs/zhu2021counter.pdf"><i>"Counter-Interference Adapter for Multilingual Machine Translation"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zhu2021counter','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zhu2021counter','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_zhu2021counter" class="abstract noshow">
	<td><b>Abstract</b>: Developing  a  unified  multilingual  model  haslong  been  a  pursuit  for  machine  translation. However, existing approaches suffer from performance degradation — a single multilingualmodel  is  inferior  to  separately  trained  bilingual ones on rich-resource languages. We conjecture  that such  a phenomenon  is due  to interference  caused  by  joint  training  with  multiple  languages.   To  accommodate  the  issue,we  propose  CIAT,  an  adapted  Transformermodel  with  a  small  parameter  overhead  formultilingual  machine  translation.   We  evaluate CIAT on multiple benchmark datasets, including  IWSLT,  OPUS-100,  and  WMT.  Experiments  show  that  CIAT  consistently  outperforms strong multilingual baselines on 64 of  total  66  language  directions,  42  of  whichsee  above  0.5  BLEU  improvement. Our code  is  available  at https://github.com/Yaoming95/CIAT.</td>
</tr>

<tr id="bib_zhu2021counter" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhu2021counter,
  author = {Yaoming Zhu and Jiangtao Feng and Chengqi Zhao and Mingxuan Wang and Lei Li},
  title = {Counter-Interference Adapter for Multilingual Machine Translation},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="wang2021cnewsum" class="entry">
	<td>Danqing Wang, Jiaze Chen, Xianze Wu, Hao Zhou and Lei Li, <a href="pubs/wang2021cnewsum.pdf"><i>"CNewSum: A Large-scale Chinese News Summarization Dataset with Human-annotated Adequacy and Deducibility Level"</i></a>, In The 10th CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC), Qingdao, China, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021cnewsum','bibtex')">[BibTeX]</a>
  
  
   <a href="https://dqwang122.github.io/projects/CNewSum/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wang2021cnewsum" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021cnewsum,
  author = {Danqing Wang and Jiaze Chen and Xianze Wu and Hao Zhou and Lei Li},
  title = {CNewSum: A Large-scale Chinese News Summarization Dataset with Human-annotated Adequacy and Deducibility Level},
  booktitle = {The 10th CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC)},
  year = {2021},
  url = {https://dqwang122.github.io/projects/CNewSum/}
}
</pre></td>
</tr><tr id="li2021simultaneous" class="entry">
	<td>Yiming Li, Tao Kong, Ruihang Chu, Yifeng Li, Peng Wang and Lei Li, <a href="pubs/li2021simultaneous.pdf"><i>"Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation"</i></a>, In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2021simultaneous','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_li2021simultaneous" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2021simultaneous,
  author = {Yiming Li and Tao Kong and Ruihang Chu and Yifeng Li and Peng Wang and Lei Li},
  title = {Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2021}
}
</pre></td>
</tr><tr id="li2021learning" class="entry">
	<td>Yunfei Li, Tao Kong, Lei Li, Yifeng Li and Yi Wu, <a href="pubs/li2021learning.pdf"><i>"Learning to Design and Construct Bridge without Blueprint"</i></a>, In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2021learning','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_li2021learning" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2021learning,
  author = {Yunfei Li and Tao Kong and Lei Li and Yifeng Li and Yi Wu},
  title = {Learning to Design and Construct Bridge without Blueprint},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2021}
}
</pre></td>
</tr><tr id="shi2021follow" class="entry">
	<td>Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li and Lei Li, <a href="pubs/shi2021follow.pdf"><i>"Follow Your Path: a Progressive Method for Knowledge Distillation"</i></a>, In Proc. of ECML-PKDD, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('shi2021follow','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_shi2021follow" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{shi2021follow,
  author = {Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},
  title = {Follow Your Path: a Progressive Method for Knowledge Distillation},
  booktitle = {Proc. of ECML-PKDD},
  year = {2021}
}
</pre></td>
</tr><tr id="han2021learning" class="entry">
	<td>Chi Han, Mingxuan Wang, Heng Ji and Lei Li, <a href="pubs/han2021learning.pdf"><i>"Learning Shared Semantic Space for Speech-to-Text Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('han2021learning','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_han2021learning" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{han2021learning,
  author = {Chi Han and Mingxuan Wang and Heng Ji and Lei Li},
  title = {Learning Shared Semantic Space for Speech-to-Text Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="lin2021learning" class="entry">
	<td>Zehui Lin, Liwei Wu, Mingxuan Wang and Lei Li, <a href="pubs/lin2021learning.pdf"><i>"Learning Language Specific Sub-network for Multilingual Machine Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('lin2021learning','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('lin2021learning','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_lin2021learning" class="abstract noshow">
	<td><b>Abstract</b>: Multilingual neural machine translation aimsat learning a single translation model for muliple  languages.  These  jointly  trained  mod-els often suffer from performance degradationon rich-resource language pairs. We attributethis degeneration to parameter interference. Inthis paper, we propose LaSS to jointly train asingle  unified  multilingual  MT  model.  LaSS learns Language Secific Sub-network (LaSS)for  each  language  pair  to  counter  parameterinterference.  Comprehensive  experiments  on IWSLT and WMT datasets with various Transformer  architectures  show  that  LaSS  obtainsgains on 36 language pairs by up to 1.2 BLEU.Besides, LaSS shows its strong generalization performance  at  easy  adaptation  to  new  lan-guage  pairs  and  zero-shot  translation.  LaSS boosts  zero-shot  translation  with  an  averageof  8.3  BLEU  on  30  language  pairs.</td>
</tr>

<tr id="bib_lin2021learning" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{lin2021learning,
  author = {Zehui Lin and Liwei Wu and Mingxuan Wang and Lei Li},
  title = {Learning Language Specific Sub-network for Multilingual Machine Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021}
}
</pre></td>
</tr><tr id="pan2021contrastive" class="entry">
	<td>Xiao Pan, Liwei Wu, Mingxuan Wang and Lei Li, <a href="pubs/pan2021contrastive.pdf"><i>"Contrastive Learning for Many-to-many Multilingual Neural Machine Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('pan2021contrastive','bibtex')">[BibTeX]</a>
  
  
   <a href="https://medium.com/@panxiao1994/mrasp2-multilingual-nmt-advances-via-contrastive-learning-ac8c4c35d63">[URL]</a>
	
	 <a href="pubs/mRASP2_ACL2021.pdf">[PPT]</a>
	
	</p>
	</td>
</tr>


<tr id="bib_pan2021contrastive" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{pan2021contrastive,
  author = {Xiao Pan and Liwei Wu and Mingxuan Wang and Lei Li},
  title = {Contrastive Learning for Many-to-many Multilingual Neural Machine Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021},
  url = {https://medium.com/@panxiao1994/mrasp2-multilingual-nmt-advances-via-contrastive-learning-ac8c4c35d63}
}
</pre></td>
</tr><tr id="qian2021glancing" class="entry">
	<td>Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu and Lei Li, <a href="pubs/qian2021glancing.pdf"><i>"Glancing Transformer for Non-Autoregressive Neural Machine Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('qian2021glancing','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_qian2021glancing" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{qian2021glancing,
  author = {Lihua Qian and Hao Zhou and Yu Bao and Mingxuan Wang and Lin Qiu and Weinan Zhang and Yong Yu and Lei Li},
  title = {Glancing Transformer for Non-Autoregressive Neural Machine Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021}
}
</pre></td>
</tr><tr id="sun2021probabilistic" class="entry">
	<td>Changzhi Sun, Xinbo Zhang, Jiangjie Chen, Chun Gan, Yuanbin Wu, Jiaze Chen, Hao Zhou and Lei Li, <a href="pubs/sun2021probabilistic.pdf"><i>"Probabilistic Graph Reasoning for Natural Proof Generation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sun2021probabilistic','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_sun2021probabilistic" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sun2021probabilistic,
  author = {Changzhi Sun and Xinbo Zhang and Jiangjie Chen and Chun Gan and Yuanbin Wu and Jiaze Chen and Hao Zhou and Lei Li},
  title = {Probabilistic Graph Reasoning for Natural Proof Generation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="wang2021contrastive" class="entry">
	<td>Danqing Wang, Jiaze Chen, Hao Zhou, Xipeng Qiu and Lei Li, <a href="pubs/wang2021contrastive.pdf"><i>"Contrastive Aligned Joint Learning for Multilingual Summarization"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021contrastive','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wang2021contrastive" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021contrastive,
  author = {Danqing Wang and Jiaze Chen and Hao Zhou and Xipeng Qiu and Lei Li},
  title = {Contrastive Aligned Joint Learning for Multilingual Summarization},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="wang2021unire" class="entry">
	<td>Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li and Junchi Yan, <a href="pubs/wang2021unire.pdf"><i>"UniRE: A Unified Label Space for Entity Relation Extraction"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021unire','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wang2021unire" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021unire,
  author = {Yijun Wang and Changzhi Sun and Yuanbin Wu and Hao Zhou and Lei Li and Junchi Yan},
  title = {UniRE: A Unified Label Space for Entity Relation Extraction},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021}
}
</pre></td>
</tr><tr id="wu2021language" class="entry">
	<td>Liwei Wu, Shanbo Chen, Mingxuan Wang and Lei Li, <a href="pubs/wu2021language.pdf"><i>"Language Tags Matter for Zero-Shot Neural Machine Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wu2021language','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wu2021language" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2021language,
  author = {Liwei Wu and Shanbo Chen and Mingxuan Wang and Lei Li},
  title = {Language Tags Matter for Zero-Shot Neural Machine Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL) - Findings},
  year = {2021}
}
</pre></td>
</tr><tr id="xu2021vocabulary" class="entry">
	<td>Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng and Lei Li, <a href="pubs/xu2021vocabulary.pdf"><i>"Vocabulary Learning via Optimal Transport for Neural Machine Translation"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('xu2021vocabulary','bibtex')">[BibTeX]</a>
  
  
   <a href="https://jingjing-nlp.github.io/volt-blog/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_xu2021vocabulary" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xu2021vocabulary,
  author = {Jingjing Xu and Hao Zhou and Chun Gan and Zaixiang Zheng and Lei Li},
  title = {Vocabulary Learning via Optimal Transport for Neural Machine Translation},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021},
  url = {https://jingjing-nlp.github.io/volt-blog/}
}
</pre></td>
</tr><tr id="xu2021document" class="entry">
	<td>Runxin Xu, Tianyu Liu, Lei Li and Baobao Chang, <a href="pubs/xu2021document.pdf"><i>"Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('xu2021document','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_xu2021document" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xu2021document,
  author = {Runxin Xu and Tianyu Liu and Lei Li and Baobao Chang},
  title = {Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2021}
}
</pre></td>
</tr><tr id="ye2021end" class="entry">
	<td>Rong Ye, Mingxuan Wang and Lei Li, <a href="pubs/ye2021end.pdf"><i>"End-to-end Speech Translation via Cross-modal Progressive Training"</i></a>, In Proc. of INTERSPEECH, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('ye2021end','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_ye2021end" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{ye2021end,
  author = {Rong Ye and Mingxuan Wang and Lei Li},
  title = {End-to-end Speech Translation via Cross-modal Progressive Training},
  booktitle = {Proc. of INTERSPEECH},
  year = {2021}
}
</pre></td>
</tr><tr id="zhao2021neurst" class="entry">
	<td>Chengqi Zhao, Mingxuan Wang, Qianqian Dong, Rong Ye and Lei Li, <a href="pubs/zhao2021neurst.pdf"><i>"NeurST: Neural Speech Translation Toolkit"</i></a>, In the 59th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations, 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhao2021neurst','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_zhao2021neurst" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhao2021neurst,
  author = {Chengqi Zhao and Mingxuan Wang and Qianqian Dong and Rong Ye and Lei Li},
  title = {NeurST: Neural Speech Translation Toolkit},
  booktitle = {the 59th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations},
  year = {2021}
}
</pre></td>
</tr><tr id="jing2021adversarial" class="entry">
	<td>Mingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao Kong, Chuang Gan and Lei Li, <a href="pubs/jing2021adversarial.pdf"><i>"Adversarial Option-Aware Hierarchical Imitation Learning"</i></a>, In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('jing2021adversarial','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_jing2021adversarial" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{jing2021adversarial,
  author = {Mingxuan Jing and Wenbing Huang and Fuchun Sun and Xiaojian Ma and Tao Kong and Chuang Gan and Lei Li},
  title = {Adversarial Option-Aware Hierarchical Imitation Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year = {2021}
}
</pre></td>
</tr><tr id="chen2021scale" class="entry">
	<td>Yukang Chen, Yanwei Li, Tao Kong, Lu Qi, Ruihang Chu, Lei Li and Jiaya Jia, <a href="pubs/chen2021scale.pdf"><i>"Scale-aware Automatic Augmentation for Object Detection"</i></a>, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2021scale','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_chen2021scale" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{chen2021scale,
  author = {Yukang Chen and Yanwei Li and Tao Kong and Lu Qi and Ruihang Chu and Lei Li and Jiaya Jia},
  title = {Scale-aware Automatic Augmentation for Object Detection},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2021}
}
</pre></td>
</tr><tr id="jing2021locate" class="entry">
	<td>Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li and Tieniu Tan, <a href="pubs/jing2021locate.pdf"><i>"Locate then Segment: A Strong Pipeline for Referring Image Segmentation"</i></a>, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('jing2021locate','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_jing2021locate" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{jing2021locate,
  author = {Ya Jing and Tao Kong and Wei Wang and Liang Wang and Lei Li and Tieniu Tan},
  title = {Locate then Segment: A Strong Pipeline for Referring Image Segmentation},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2021}
}
</pre></td>
</tr><tr id="long2021generative" class="entry">
	<td>Quanyu Long, Mingxuan Wang and Lei Li, <a href="pubs/long2021generative.pdf"><i>"Generative Imagination Elevates Machine Translation"</i></a>, In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Online, pp. 5738-5748. Association for Computational Linguistics, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('long2021generative','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('long2021generative','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_long2021generative" class="abstract noshow">
	<td><b>Abstract</b>: There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the ``imagined representation'' to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.</td>
</tr>

<tr id="bib_long2021generative" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{long2021generative,
  author = {Long, Quanyu and Wang, Mingxuan and Li, Lei},
  title = {Generative Imagination Elevates Machine Translation},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  publisher = {Association for Computational Linguistics},
  year = {2021},
  pages = {5738--5748}
}
</pre></td>
</tr><tr id="sun2021sparse" class="entry">
	<td>Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang and Ping Luo, <a href="pubs/sun2021sparse.pdf"><i>"Sparse R-CNN: End-to-End Object Detection with Learnable Proposals"</i></a>, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sun2021sparse','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_sun2021sparse" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sun2021sparse,
  author = {Peize Sun and Rufeng Zhang and Yi Jiang and Tao Kong and Chenfeng Xu and Wei Zhan and Masayoshi Tomizuka and Lei Li and Zehuan Yuan and Changhu Wang and Ping Luo},
  title = {Sparse R-CNN: End-to-End Object Detection with Learnable Proposals},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2021}
}
</pre></td>
</tr><tr id="wang2021cross" class="entry">
	<td>Mingxuan Wang, Hongxiao Bai, Hai Zhao and Lei Li, <a href="pubs/wang2021cross.pdf"><i>"Cross-lingual Supervision Improves Unsupervised Neural Machine Translation"</i></a>, In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT), Online, pp. 89-96. Association for Computational Linguistics, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2021cross','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2021cross','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2021cross" class="abstract noshow">
	<td><b>Abstract</b>: We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. % is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT'14 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT'16 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline.</td>
</tr>

<tr id="bib_wang2021cross" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021cross,
  author = {Wang, Mingxuan and Bai, Hongxiao and Zhao, Hai and Li, Lei},
  title = {Cross-lingual Supervision Improves Unsupervised Neural Machine Translation},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)},
  publisher = {Association for Computational Linguistics},
  year = {2021},
  pages = {89--96}
}
</pre></td>
</tr><tr id="wang2021autocorrect" class="entry">
	<td>Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li and Deyi Xiong, <a href="pubs/wang2021autocorrect.pdf"><i>"Autocorrect in the Process of Translation --- Multi-task Learning Improves Dialogue Machine Translation"</i></a>, In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT), Online, pp. 105-112. Association for Computational Linguistics, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2021autocorrect','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2021autocorrect','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2021autocorrect" class="abstract noshow">
	<td><b>Abstract</b>: Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%.</td>
</tr>

<tr id="bib_wang2021autocorrect" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021autocorrect,
  author = {Wang, Tao and Zhao, Chengqi and Wang, Mingxuan and Li, Lei and Xiong, Deyi},
  title = {Autocorrect in the Process of Translation --- Multi-task Learning Improves Dialogue Machine Translation},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)},
  publisher = {Association for Computational Linguistics},
  year = {2021},
  pages = {105--112}
}
</pre></td>
</tr><tr id="wang2021lightseq" class="entry">
	<td>Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang and Lei Li, <a href="pubs/wang2021lightseq.pdf"><i>"LightSeq: A High Performance Inference Library for Transformers"</i></a>, In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT), Online, pp. 113-120. Association for Computational Linguistics, 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2021lightseq','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2021lightseq','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2021lightseq" class="abstract noshow">
	<td><b>Abstract</b>: Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.</td>
</tr>

<tr id="bib_wang2021lightseq" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021lightseq,
  author = {Wang, Xiaohui and Xiong, Ying and Wei, Yang and Wang, Mingxuan and Li, Lei},
  title = {LightSeq: A High Performance Inference Library for Transformers},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)},
  publisher = {Association for Computational Linguistics},
  year = {2021},
  pages = {113--120}
}
</pre></td>
</tr><tr id="wang2021dense" class="entry">
	<td>Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong and Lei Li, <a href="pubs/wang2021dense.pdf"><i>"Dense Contrastive Learning for Self-Supervised Visual Pre-Training"</i></a>, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021dense','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wang2021dense" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021dense,
  author = {Xinlong Wang and Rufeng Zhang and Chunhua Shen and Tao Kong and Lei Li},
  title = {Dense Contrastive Learning for Self-Supervised Visual Pre-Training},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2021}
}
</pre></td>
</tr><tr id="li2021target" class="entry">
	<td>Gen Li, Shikun Xu, Yandong Zhu, Lei Li and Changhu Wang, <a href="pubs/li2021target.pdf"><i>"Target Object Image Detection Method and Device"</i></a>(201811010095.X), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2021target','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_li2021target" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{li2021target,
  author = {Li, Gen and Xu, Shikun and Zhu, Yandong and Li, Lei and Wang, Changhu},
  title = {Target Object Image Detection Method and Device},
  year = {2021},
  number = {201811010095.X}
}
</pre></td>
</tr>
<tr id="xie2021mars" class="entry">
	<td>Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu and Lei Li, <a href="pubs/xie2021mars.pdf"><i>"MARS: Markov Molecular Sampling for Multi-objective Drug Discovery"</i></a>, In International Conference on Learning Representations (ICLR), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('xie2021mars','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('xie2021mars','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_xie2021mars" class="abstract noshow">
	<td><b>Abstract</b>: Searching for novel molecules with desired chemical properties is crucial in drug discovery.  Existing work focuses on developing deep generative models to generate either sequences or chemical molecular graphs. However, it remains a great challenge to find novel and diverse compounds satisfying many properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iterative editing fragments of molecular graphs. To search for the best candidates, it employs an annealing scheme together with Markov chain Monte Carlo sampling (MCMC) on molecules. To further improve sample efficiency, MARS is equipped with a graph neural network (GNN) as the proposal for candidate edits on molecules, while the GNN is trained on-the-fly utilizing the sample paths in MCMC. Our experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are simultaneously considered.  In the most challenging setting where four objectives – bio-activities to two different targets, drug-likeness and synthesizability – are simultaneously considered, our method outperforms the state-of-the-art significantly in a comprehensive evaluation.</td>
</tr>

<tr id="bib_xie2021mars" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xie2021mars,
  author = {Yutong Xie and Chence Shi and Hao Zhou and Yuwei Yang and Weinan Zhang and Yong Yu and Lei Li},
  title = {MARS: Markov Molecular Sampling for Multi-objective Drug Discovery},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2021}
}
</pre></td>
</tr><tr id="chen2021methoda" class="entry">
	<td>Yangyu Chen, Yi He and Lei Li, <a href="pubs/chen2021methoda.pdf"><i>"Method and Device for determining geometric transformation relation for images"</i></a>(16/981,240), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2021methoda','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2021methoda" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2021methoda,
  author = {Chen, Yangyu and He, Yi and Li, Lei},
  title = {Method and Device for determining geometric transformation relation for images},
  year = {2021},
  number = {16/981,240}
}
</pre></td>
</tr>
<tr id="he2021image" class="entry">
	<td>Yi He, Gen Li and Lei Li, <a href="pubs/he2021image.pdf"><i>"Image processing method and device"</i></a>(201910498629.6), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('he2021image','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_he2021image" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{he2021image,
  author = {He, Yi and Li, Gen and Li, Lei},
  title = {Image processing method and device},
  year = {2021},
  number = {201910498629.6}
}
</pre></td>
</tr>
<tr id="huang2021target" class="entry">
	<td>Xunpeng Huang, Zhengyang Liu and Lei Li, <a href="pubs/huang2021target.pdf"><i>"Target Object Classification method and device"</i></a>(202010057296.6), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('huang2021target','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_huang2021target" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{huang2021target,
  author = {Huang, Xunpeng and Liu, Zhengyang and Li, Lei},
  title = {Target Object Classification method and device},
  year = {2021},
  number = {202010057296.6}
}
</pre></td>
</tr>
<tr id="li2021method" class="entry">
	<td>Lei Li, Jiaze Chen, Jiamin Chen, Weiying Ma and Lifeng Hua, <a href="pubs/li2021method.pdf"><i>"Method and Device for generating information"</i></a>(201811455645.9), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2021method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_li2021method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{li2021method,
  author = {Li, Lei and Chen, Jiaze and Chen, Jiamin and Ma, Weiying and Hua, Lifeng},
  title = {Method and Device for generating information},
  year = {2021},
  number = {201811455645.9}
}
</pre></td>
</tr>
<tr id="wang2021enpar" class="entry">
	<td>Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li and Junchi Yan, <a href="pubs/wang2021enpar.pdf"><i>"ENPAR: Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction"</i></a>, In Proceedings of European Chapter of the Association for Computational Linguistics (EACL), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wang2021enpar','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wang2021enpar" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2021enpar,
  author = {Yijun Wang and Changzhi Sun and Yuanbin Wu and Hao Zhou and Lei Li and Junchi Yan},
  title = {ENPAR: Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction},
  booktitle = {Proceedings of European Chapter of the Association for Computational Linguistics (EACL)},
  year = {2021}
}
</pre></td>
</tr><tr id="she2021method" class="entry">
	<td>Heng She, Yang Wang, Yinuo Guo, Huiru Zhang, Yitan Li, Lei Li and Hang Li, <a href="pubs/she2021method.pdf"><i>"Method and Device for Push-Notifying Information"</i></a>(201811562666.0), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('she2021method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_she2021method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{she2021method,
  author = {She, Heng and Wang, Yang and Guo, Yinuo and Zhang, Huiru and Li, Yitan and Li, Lei and Li, Hang},
  title = {Method and Device for Push-Notifying Information},
  year = {2021},
  number = {201811562666.0}
}
</pre></td>
</tr>
<tr id="song2021triangular" class="entry">
	<td>Zhenqiao Song, Jiaze Chen, Hao Zhou and Lei Li, <a href="pubs/song2021triangular.pdf"><i>"Triangular Bidword Generation for Sponsored Search Auction"</i></a>, In Proceedings of the 14th International Conference on Web Search and Data Mining (WSDM), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('song2021triangular','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('song2021triangular','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_song2021triangular" class="abstract noshow">
	<td><b>Abstract</b>: Sponsored search auction is a crucial component of modern search engines. It requires a set of candidate bidwords that advertisers can place bids on. Existing methods generate bidwords from search queries or advertisement content. However, they suffer from the data noise in <query, bidword> and <advertisement, bidword> pairs. In this paper, we propose a triangular bidword generation model (TRIDENT), which takes the high-quality data of paired <query, advertisement> as a supervision signal to indirectly guide the bidword generation process. Our proposed model is simple yet effective: by using bidword as the bridge between search query and advertisement, the generation of search query, advertisement and bidword can be jointly learned in the triangular training framework. This alleviates the problem that the training data of bidword may be noisy. Experimental results, including automatic and human evaluations, show that our proposed TRIDENT can generate relevant and diverse bidwords for both search queries and advertisements. Our evaluation on online real data validates the effectiveness of the TRIDENT’s generated bidwords for product search.</td>
</tr>

<tr id="bib_song2021triangular" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{song2021triangular,
  author = {Zhenqiao Song and Jiaze Chen and Hao Zhou and Lei Li},
  title = {Triangular Bidword Generation for Sponsored Search Auction},
  booktitle = {Proceedings of the 14th International Conference on Web Search and Data Mining (WSDM)},
  year = {2021}
}
</pre></td>
</tr><tr id="dong2021consecutive" class="entry">
	<td>Qianqian Dong, Mingxuan Wang, Hao Zhou, Shuang Xu, Bo Xu and Lei Li, <a href="pubs/dong2021consecutive.pdf"><i>"Consecutive Decoding for Speech-to-text Translation"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('dong2021consecutive','bibtex')">[BibTeX]</a>
  
  
   <a href="https://dqqcasia.github.io/projects/COSTT/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_dong2021consecutive" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{dong2021consecutive,
  author = {Qianqian Dong and Mingxuan Wang and Hao Zhou and Shuang Xu and Bo Xu and Lei Li},
  title = {Consecutive Decoding for Speech-to-text Translation},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021},
  url = {https://dqqcasia.github.io/projects/COSTT/}
}
</pre></td>
</tr><tr id="dong2021listen" class="entry">
	<td>Qianqian Dong, Rong Ye, Mingxuan Wang, Hao Zhou, Shuang Xu, Bo Xu and Lei Li, <a href="pubs/dong2021listen.pdf"><i>"Listen, Understand and Translate: Triple Supervision Decouples End-to-end Speech-to-text Translation"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('dong2021listen','bibtex')">[BibTeX]</a>
  
  
   <a href="https://dqqcasia.github.io/projects/LUT/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_dong2021listen" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{dong2021listen,
  author = {Qianqian Dong and Rong Ye and Mingxuan Wang and Hao Zhou and Shuang Xu and Bo Xu and Lei Li},
  title = {Listen, Understand and Translate: Triple Supervision Decouples End-to-end Speech-to-text Translation},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021},
  url = {https://dqqcasia.github.io/projects/LUT/}
}
</pre></td>
</tr><tr id="huang2021acmo" class="entry">
	<td>Xunpeng Huang, Runxin Xu, Hao Zhou, Zhe Wang, Zhengyang Liu and Lei Li, <a href="pubs/huang2021acmo.pdf"><i>"ACMo: Angle-Calibrated Moment Methods for Stochastic Optimization"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"><a href="javascript:toggleInfo('huang2021acmo','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('huang2021acmo','bibtex')">[BibTeX]</a>
  
  
   <a href="https://xunpeng746.github.io/projects/ACMo/ACMo.html">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_huang2021acmo" class="abstract noshow">
	<td><b>Abstract</b>: Stochastic gradient descent (SGD) is a widely used method for its outstanding generalization ability and simplicity.  daptive gradient methods have been proposed to further accelerate the optimization process.  n this paper, we revisit existing adaptive gradient optimization methods with a new interpretation. <br>Such new perspective leads to a refreshed understanding of the roles of second moments in stochastic optimization.  Based on this, we propose Angle-Calibration Moment method (ACMo), a novel stochastic optimization method. It enjoys the benefits of second moments with only first moment updates. Theoretical analysis shows that ACMo is able to achieve the same convergence rate as mainstream adaptive methods. Experiments on a variety of CV and NLP tasks demonstrate that ACMo has a comparable convergence to state-of-the-art Adam-type optimizers, and even a better generalization performance in most cases. The code is available at https://github.com/Xunpeng746/ACMo.</td>
</tr>

<tr id="bib_huang2021acmo" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{huang2021acmo,
  author = {Xunpeng Huang and Runxin Xu and Hao Zhou and Zhe Wang and Zhengyang Liu and Lei Li},
  title = {ACMo: Angle-Calibrated Moment Methods for Stochastic Optimization},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021},
  url = {https://xunpeng746.github.io/projects/ACMo/ACMo.html}
}
</pre></td>
</tr><tr id="liang2021finding" class="entry">
	<td>Jianze Liang, Chengqi Zhao, Mingxuan Wang, Xipeng Qiu and Lei Li, <a href="pubs/liang2021finding.pdf"><i>"Finding Sparse Structure for Domain Specific Neural Machine Translation"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('liang2021finding','bibtex')">[BibTeX]</a>
  
  
   <a href="https://ohlionel.github.io/project/Prune-Tune/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_liang2021finding" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{liang2021finding,
  author = {Jianze Liang and Chengqi Zhao and Mingxuan Wang and Xipeng Qiu and Lei Li},
  title = {Finding Sparse Structure for Domain Specific Neural Machine Translation},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021},
  url = {https://ohlionel.github.io/project/Prune-Tune/}
}
</pre></td>
</tr><tr id="wu2021textgail" class="entry">
	<td>Qingyang Wu, Lei Li and Zhou Yu, <a href="pubs/wu2021textgail.pdf"><i>"TextGAIL: Generative Adversarial Imitation Learning for Text Generation"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wu2021textgail','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wu2021textgail" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2021textgail,
  author = {Qingyang Wu and Lei Li and Zhou Yu},
  title = {TextGAIL: Generative Adversarial Imitation Learning for Text Generation},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021}
}
</pre></td>
</tr><tr id="zhang2021taxonomy" class="entry">
	<td>Jieyu Zhang, Xiangchen Song, Ying Zeng, Jiaze Chen, Jiaming Shen, Yuning Mao and Lei Li, <a href="pubs/zhang2021taxonomy.pdf"><i>"Taxonomy Completion via Triplet Matching Network"</i></a>, In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhang2021taxonomy','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_zhang2021taxonomy" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhang2021taxonomy,
  author = {Jieyu Zhang and Xiangchen Song and Ying Zeng and Jiaze Chen and Jiaming Shen and Yuning Mao and Lei Li},
  title = {Taxonomy Completion via Triplet Matching Network},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2021}
}
</pre></td>
</tr><tr id="chen2021method" class="entry">
	<td>Jiaze Chen, Lei Li, Ying Zeng and Weiying Ma, <a href="pubs/chen2021method.pdf"><i>"Method and Device for generation product description information"</i></a>(201811457980.2), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2021method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2021method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2021method,
  author = {Chen, Jiaze and Li, Lei and Zeng, Ying and Ma, Weiying},
  title = {Method and Device for generation product description information},
  year = {2021},
  number = {201811457980.2}
}
</pre></td>
</tr>
<tr id="deng2021method" class="entry">
	<td>Jiangdong Deng, Lei Li and Weiying Ma, <a href="pubs/deng2021method.pdf"><i>"Method and device for stock selection"</i></a>(201810910344.4), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('deng2021method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_deng2021method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{deng2021method,
  author = {Deng, Jiangdong and Li, Lei and Ma, Weiying},
  title = {Method and device for stock selection},
  year = {2021},
  number = {201810910344.4}
}
</pre></td>
</tr>
<tr id="he2021duplicate" class="entry">
	<td>Yi He, Cheng Yang, Gen Li, Yitan Li and Lei Li, <a href="pubs/he2021duplicate.pdf"><i>"Duplicate video detection method and device"</i></a>(201810273706.3), 2021.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('he2021duplicate','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_he2021duplicate" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{he2021duplicate,
  author = {He, Yi and Yang, Cheng and Li, Gen and Li, Yitan and Li, Lei},
  title = {Duplicate video detection method and device},
  year = {2021},
  number = {201810273706.3}
}
</pre></td>
</tr>
<tr id="wang2020solov2" class="entry">
	<td>Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li and Chunhua Shen, <a href="pubs/wang2020solov2.pdf"><i>"SOLOv2: Dynamic and Fast Instance Segmentation"</i></a>, In the 34th Conference on Neural Information Processing Systems (NeurIPS), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2020solov2','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2020solov2','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2020solov2" class="abstract noshow">
	<td><b>Abstract</b>: In this work, we design a simple, direct, and fast framework for instance segmentation with strong performance. To this end, we propose a novel and effective approach, termed SOLOv2, following the principle of the SOLO method. First, our new framework is empowered by an efficient and holistic instance mask representation scheme, which dynamically segments each instance in the image, without resorting to bounding box detection. Specifically, the object mask generation is decoupled into a mask kernel prediction and mask feature learning, which are responsible for generating convolution kernels and the feature maps to be convolved with, respectively. Second, SOLOv2 significantly reduces inference overhead with our novel matrix non-maximum suppression (NMS) technique. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results. We demonstrate that the proposed SOLOv2 achieves the state-of-the- art performance with high efficiency, making it suitable for both mobile and cloud applications. A light-weight version of SOLOv2 executes at 31.3 FPS and yields 37.1% AP on COCO test-dev. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential of SOLOv2 to serve as a new strong baseline for many instance-level recognition tasks.</td>
</tr>

<tr id="bib_wang2020solov2" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2020solov2,
  author = {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},
  title = {SOLOv2: Dynamic and Fast Instance Segmentation},
  booktitle = {the 34th Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2020}
}
</pre></td>
</tr><tr id="li2020sentence" class="entry">
	<td>Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang and Lei Li, <a href="pubs/li2020sentence.pdf"><i>"On the Sentence Embeddings from Pre-trained Language Models"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2020sentence','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2020sentence','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_li2020sentence" class="abstract noshow">
	<td><b>Abstract</b>: Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.</td>
</tr>

<tr id="bib_li2020sentence" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2020sentence,
  author = {Bohan Li and Hao Zhou and Junxian He and Mingxuan Wang and Yiming Yang and Lei Li},
  title = {On the Sentence Embeddings from Pre-trained Language Models},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2020}
}
</pre></td>
</tr><tr id="lin2020pre" class="entry">
	<td>Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao Zhou and Lei Li, <a href="pubs/lin2020pre.pdf"><i>"Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('lin2020pre','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('lin2020pre','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_lin2020pre" class="abstract noshow">
	<td><b>Abstract</b>: We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with simlar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple low-esource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pre- training corpus.</td>
</tr>

<tr id="bib_lin2020pre" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{lin2020pre,
  author = {Zehui Lin and Xiao Pan and Mingxuan Wang and Xipeng Qiu and Jiangtao Feng and Hao Zhou and Lei Li},
  title = {Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2020}
}
</pre></td>
</tr><tr id="ru2020active" class="entry">
	<td>Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mngxuan Wang, Weinan Zhang, Yong Yu and Lei Li, <a href="pubs/ru2020active.pdf"><i>"Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings, 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('ru2020active','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('ru2020active','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_ru2020active" class="abstract noshow">
	<td><b>Abstract</b>: Active learning for sentence understanding aims at discovering informative unlabeled data for annotation and therefore reducing the demand for labeled data. We argue that the typical uncertainty sampling method for active learning is time-consuming and can hardly work in real-time, which may lead to ineffective sample selection. We propose adversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popuar pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness.</td>
</tr>

<tr id="bib_ru2020active" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{ru2020active,
  author = {Dongyu Ru and Jiangtao Feng and Lin Qiu and Hao Zhou and Mngxuan Wang and Weinan Zhang and Yong Yu and Lei Li},
  title = {Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings},
  year = {2020}
}
</pre></td>
</tr><tr id="wu2020volctrans" class="entry">
	<td>Liwei Wu, Xiao Pan, Zehui Lin, Yaoming Zhu, Mingxuan Wang and Lei Li, <a href="pubs/wu2020volctrans.pdf"><i>"The Volctrans Machine Translation System for WMT20"</i></a>, In Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wu2020volctrans','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_wu2020volctrans" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2020volctrans,
  author = {Liwei Wu and Xiao Pan and Zehui Lin and Yaoming Zhu and Mingxuan Wang and Lei Li},
  title = {The Volctrans Machine Translation System for WMT20},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers)},
  year = {2020}
}
</pre></td>
</tr><tr id="xu2020volctrans" class="entry">
	<td>Runxin Xu, Zhuo Zhi, Jun Cao, Mingxuan Wang and Lei Li, <a href="pubs/xu2020volctrans.pdf"><i>"Volctrans Parallel Corpus Filtering System for WMT 2020"</i></a>, In Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('xu2020volctrans','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_xu2020volctrans" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xu2020volctrans,
  author = {Runxin Xu and Zhuo Zhi and Jun Cao and Mingxuan Wang and Lei Li},
  title = {Volctrans Parallel Corpus Filtering System for WMT 2020},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation (Volume 2: Shared Task Papers)},
  year = {2020}
}
</pre></td>
</tr><tr id="zeng2020double" class="entry">
	<td>Shuang Zeng, Runxin Xu, Baobao Chang and Lei Li, <a href="pubs/zeng2020double.pdf"><i>"Double Graph Based Reasoning for Document-level Relation Extraction"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zeng2020double','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zeng2020double','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_zeng2020double" class="abstract noshow">
	<td><b>Abstract</b>: Document-level relation extraction aims to extract relations among entities within a docuent. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention- level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art.</td>
</tr>

<tr id="bib_zeng2020double" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zeng2020double,
  author = {Shuang Zeng and Runxin Xu and Baobao Chang and Lei Li},
  title = {Double Graph Based Reasoning for Document-level Relation Extraction},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2020}
}
</pre></td>
</tr><tr id="zhang2020language" class="entry">
	<td>Maosen Zhang, Nan Jiang, Lei Li and Yexiang Xue, <a href="pubs/zhang2020language.pdf"><i>"Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced Monte-Carlo Approach"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings, 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zhang2020language','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zhang2020language','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_zhang2020language" class="abstract noshow">
	<td><b>Abstract</b>: Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMH1, an efficient method to generate high likelihood sentences with respect to a pre-trained language model while satisfying the constraints. Our approach is highly flexible, requires no task-specific training, and leverages efficient constraint satisfaction solving techniques. To better handle the combinatorial constraints, a tree search algorithm is embedded into the proposal process of the Markov chain Monte Carlo (MCMC) to explore candidates that satisfy more constraints. Compared to existing MCMC approaches, our sampling approach has a better mixing performance. Experiments show that TSMH achieves consistent and significant improvement on multiple language generation tasks.</td>
</tr>

<tr id="bib_zhang2020language" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhang2020language,
  author = {Maosen Zhang and Nan Jiang and Lei Li and Yexiang Xue},
  title = {Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced Monte-Carlo Approach},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings},
  year = {2020}
}
</pre></td>
</tr><tr id="deng2020sentiment" class="entry">
	<td>Jiangdong Deng, Lei Li and Weiying Ma, <a href="pubs/deng2020sentiment.pdf"><i>"Sentiment Prediction Method and Device"</i></a>(201810909879.X), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('deng2020sentiment','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_deng2020sentiment" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{deng2020sentiment,
  author = {Jiangdong Deng and Lei Li and Weiying Ma},
  title = {Sentiment Prediction Method and Device},
  year = {2020},
  number = {201810909879.X}
}
</pre></td>
</tr>
<tr id="song2020improving" class="entry">
	<td>Yuxuan Song, Ning Miao, Hao Zhou, Lantao Yu, Mingxuan Wang and Lei Li, <a href="pubs/song2020improving.pdf"><i>"Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation"</i></a>, In The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('song2020improving','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('song2020improving','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_song2020improving" class="abstract noshow">
	<td><b>Abstract</b>: Auto-regressive  sequence  generative  models trained by Maximum Likelihood Estimation suffer  the  exposure  bias  problem  in  practical finite sample scenarios.  The crux is that the number of training samples for Maximum Likelihood Estimation is usually limited and the input data distributions are different at training and inference stages.  Many method shave been proposed to solve the above problem (Yu et al., 2017; Lu et al., 2018), which relies  on  sampling  from  the  non-stationary model distribution and suffers from high variance  or  biased  estimations.   In  this  paper, we  proposeψ-MLE,  a  new  training  scheme for auto-regressive sequence generative models, which is effective and stable when operating at large sample space encountered in text generation.   We  derive  our  algorithm  from a  new  perspective  of  self-augmentation  and introduce  bias  correction  with  density  ratio estimation.   Extensive  experimental  results on  synthetic  data  and  real-world  text  generation  tasks  demonstrate  that  our  method stably outperforms Maximum Likelihood Estimation and other state-of-the-art sequence generative  models  in  terms  of  both  quality and diversity.</td>
</tr>

<tr id="bib_song2020improving" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{song2020improving,
  author = {Yuxuan Song and Ning Miao and Hao Zhou and Lantao Yu and Mingxuan Wang and Lei Li},
  title = {Improving Maximum Likelihood Training for Text Generation with Density Ratio Estimation},
  booktitle = {The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2020}
}
</pre></td>
</tr><tr id="wang2020solo" class="entry">
	<td>Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang and Lei Li, <a href="pubs/wang2020solo.pdf"><i>"SOLO: Segmenting Objects by Locations"</i></a>, In The European Conference on Computer Vision (ECCV), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2020solo','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2020solo','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2020solo" class="abstract noshow">
	<td><b>Abstract</b>: We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.</td>
</tr>

<tr id="bib_wang2020solo" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2020solo,
  author = {Xinlong Wang and Tao Kong and Chunhua Shen and Yuning Jiang and Lei Li},
  title = {SOLO: Segmenting Objects by Locations},
  booktitle = {The European Conference on Computer Vision (ECCV)},
  year = {2020}
}
</pre></td>
</tr><tr id="he2020method" class="entry">
	<td>Yi He, Lei Li, Cheng Yang, Gen Li and Yitan Li, <a href="pubs/he2020method.pdf"><i>"A Method and Device for Video Feature Extraction"</i></a>(201810271774.6), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('he2020method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_he2020method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{he2020method,
  author = {He, Yi and Li, Lei and Yang, Cheng and Li, Gen and Li, Yitan},
  title = {A Method and Device for Video Feature Extraction},
  year = {2020},
  number = {201810271774.6}
}
</pre></td>
</tr>
<tr id="miao2020do" class="entry">
	<td>Ning Miao, Yuxuan Song, Hao Zhou and Lei Li, <a href="pubs/miao2020do.pdf"><i>"Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods"</i></a>, In the 58th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers, 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('miao2020do','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('miao2020do','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_miao2020do" class="abstract noshow">
	<td><b>Abstract</b>: It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.  In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to  over- and/or under-estimation problem.  In this paper, we propose MC-Taylor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Taylor consistently and significantly outperforms the fine-tuning approach. Our code is available at https://github.com/NingMiao/MC-tailor.</td>
</tr>

<tr id="bib_miao2020do" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{miao2020do,
  author = {Ning Miao and Yuxuan Song and Hao Zhou and Lei Li},
  title = {Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods},
  booktitle = {the 58th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers},
  year = {2020}
}
</pre></td>
</tr><tr id="ru2020quachie" class="entry">
	<td>Dongyu Ru, Zhenghui Wang, Lin Qiu, Hao Zhou, Lei Li, Weinan Zhang and Yong Yu, <a href="pubs/ru2020quachie.pdf"><i>"QuAChIE: Question Answering based Chinese Information Extraction System"</i></a>, In the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) - System Demonstrations, 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('ru2020quachie','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_ru2020quachie" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{ru2020quachie,
  author = {Dongyu Ru and Zhenghui Wang and Lin Qiu and Hao Zhou and Lei Li and Weinan Zhang and Yong Yu},
  title = {QuAChIE: Question Answering based Chinese Information Extraction System},
  booktitle = {the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) - System Demonstrations},
  year = {2020}
}
</pre></td>
</tr><tr id="shi2020dispersed" class="entry">
	<td>Wenxian Shi, Hao Zhou, Ning Miao and Lei Li, <a href="pubs/shi2020dispersed.pdf"><i>"Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation"</i></a>, In Proceedings of the 37th International Conference on Machine Learning (ICML), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('shi2020dispersed','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('shi2020dispersed','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_shi2020dispersed" class="abstract noshow">
	<td><b>Abstract</b>: Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable. To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM-VAE), whose mixture components could be related to hidden semantic aspects of data. In this paper, we generalize the practice and introduce DEM-VAE, a class of models for text generation using VAEs with a mixture distribution of exponential family. Unfortunately, a standard variational training algorithm fails due to the mode-collapse problem. We theoretically identify the root cause of the problem and propose an effective algorithm to train DEM-VAE. Our method penalizes the training with an extra dispersion term to induce a well-structured latent space. Experimental results show that our approach does obtain a meaningful space, and it outperforms strong baselines in text generation benchmarks. The code is available at https://github.com/wenxianxian/demvae.</td>
</tr>

<tr id="bib_shi2020dispersed" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{shi2020dispersed,
  author = {Wenxian Shi and Hao Zhou and Ning Miao and Lei Li},
  title = {Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  year = {2020}
}
</pre></td>
</tr><tr id="xu2020xiaomingbot" class="entry">
	<td>Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang and Lei Li, <a href="pubs/xu2020xiaomingbot.pdf"><i>"Xiaomingbot: A Multilingual Robot News Reporter"</i></a>, In the 58th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations, 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('xu2020xiaomingbot','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('xu2020xiaomingbot','bibtex')">[BibTeX]</a>
  
  
   <a href="https://xiaomingbot.github.io">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_xu2020xiaomingbot" class="abstract noshow">
	<td><b>Abstract</b>: This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multi-modal software robot equipped with four integral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multilingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person’s voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.</td>
</tr>

<tr id="bib_xu2020xiaomingbot" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xu2020xiaomingbot,
  author = {Runxin Xu and Jun Cao and Mingxuan Wang and Jiaze Chen and Hao Zhou and Ying Zeng and Yuping Wang and Li Chen and Xiang Yin and Xijin Zhang and Songcheng Jiang and Yuxuan Wang and Lei Li},
  title = {Xiaomingbot: A Multilingual Robot News Reporter},
  booktitle = {the 58th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations},
  year = {2020},
  url = {https://xiaomingbot.github.io}
}
</pre></td>
</tr><tr id="zhou2020machine" class="entry">
	<td>Hao Zhou and Lei Li, <a href="pubs/zhou2020machine.pdf"><i>"Machine Translation Method and Device"</i></a>(201910105606.4), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhou2020machine','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_zhou2020machine" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{zhou2020machine,
  author = {Zhou, Hao and Li, Lei},
  title = {Machine Translation Method and Device},
  year = {2020},
  number = {201910105606.4}
}
</pre></td>
</tr>
<tr id="hua2020xref" class="entry">
	<td>Xinyu Hua, Lei Li, Lifeng Hua and Lu Wang, <a href="pubs/hua2020xref.pdf"><i>"XREF: Entity Linking for Chinese News Comments with Supplementary Article Reference"</i></a>, In Automated Knowledge Base Construction (AKBC), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('hua2020xref','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('hua2020xref','bibtex')">[BibTeX]</a>
  
  
   <a href="https://xinyuhua.github.io/Resources/akbc20/">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_hua2020xref" class="abstract noshow">
	<td><b>Abstract</b>: Automatic identification of mentioned entities in social media posts facilitates quick digestion of trending topics and popular opinions. Nonetheless, this remains a challenging task due to limited context and diverse name variations. In this paper, we study the problem of entity linking for Chinese news comments given mentions’ spans. We hypothesize that comments often refer to entities in the corresponding news article, as well as topics involving the entities. We therefore propose a novel model, XREF, that leverages attention mechanisms to (1) pinpoint relevant context within comments, and (2) detect supporting entities from the news article. To improve training, we make two contributions: (a) we propose a supervised attention loss in addition to the standard cross entropy, and (b) we develop a weakly supervised training scheme to utilize the large-scale unlabeled corpus. Two new datasets in entertainment and product domains are collected and annotated for experiments. Our proposed method outperforms previous methods on both datasets.</td>
</tr>

<tr id="bib_hua2020xref" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{hua2020xref,
  author = {Xinyu Hua and Lei Li and Lifeng Hua and Lu Wang},
  title = {XREF: Entity Linking for Chinese News Comments with Supplementary Article Reference},
  booktitle = {Automated Knowledge Base Construction (AKBC)},
  year = {2020},
  url = {https://xinyuhua.github.io/Resources/akbc20/}
}
</pre></td>
</tr><tr id="kong2020foveabox" class="entry">
	<td>Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li and Jianbo Shi, <a href="pubs/kong2020foveabox.pdf"><i>"FoveaBox: Beyound Anchor-based Object Detection"</i></a>, IEEE Transactions on Image Processing, Volume 29, pp. 7389-7398., 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('kong2020foveabox','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('kong2020foveabox','bibtex')">[BibTeX]</a>
  
   <a href="https://doi.org/10.1109/TIP.2020.3002345">[DOI]</a>
   <a href="http://www.taokong.org/projects/FoveaBox/">[URL]</a>
  
  
  
	</p>
	</td>
</tr>

<tr id="abs_kong2020foveabox" class="abstract noshow">
	<td><b>Abstract</b>: We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox.</td>
</tr>

<tr id="bib_kong2020foveabox" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{kong2020foveabox,
  author = {Tao Kong and Fuchun Sun and Huaping Liu and Yuning Jiang and Lei Li and Jianbo Shi},
  title = {FoveaBox: Beyound Anchor-based Object Detection},
  journal = {IEEE Transactions on Image Processing},
  year = {2020},
  volume = {29},
  pages = {7389-7398},
  url = {http://www.taokong.org/projects/FoveaBox/},
  doi = {https://doi.org/10.1109/TIP.2020.3002345}
}
</pre></td>
</tr>
<tr id="wu2020towards" class="entry">
	<td>Fei Wu, Cewu Lu, Mingjie Zhu, Hao Chen, Jun Zhu, Kai Yu, Lei Li, Ming Li, Qianfeng Chen, Xi Li, Xudong Cao, Zhongyuan Wang, Zhengjun Zha, Yueting Zhuang and Yunhe Pan, <a href="pubs/wu2020towards.pdf"><i>"Towards a new generation of artificial intelligence in China"</i></a>, Nature Machine Intelligence, Volume 2, pp. 312-316., 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('wu2020towards','bibtex')">[BibTeX]</a>
  
   <a href="https://doi.org/10.1038/s42256-020-0183-4">[DOI]</a>
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_wu2020towards" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{wu2020towards,
  author = {Wu, Fei and Lu, Cewu and Zhu, Mingjie and Chen, Hao and Zhu, Jun and Yu, Kai and Li, Lei and Li, Ming and Chen, Qianfeng and Li, Xi and Cao, Xudong and Wang, Zhongyuan and Zha, Zhengjun and Zhuang, Yueting and Pan, Yunhe},
  title = {Towards a new generation of artificial intelligence in China},
  journal = {Nature Machine Intelligence},
  year = {2020},
  volume = {2},
  pages = {312-316},
  doi = {https://doi.org/10.1038/s42256-020-0183-4}
}
</pre></td>
</tr>
<tr id="he2020video" class="entry">
	<td>Yi He, Lei Li, Cheng Yang, Gen Li and Yitan Li, <a href="pubs/he2020video.pdf"><i>"Video Feature Extraction Method and Device"</i></a>(201810271773.1), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('he2020video','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_he2020video" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{he2020video,
  author = {He, Yi and Li, Lei and Yang, Cheng and Li, Gen and Li, Yitan},
  title = {Video Feature Extraction Method and Device},
  year = {2020},
  number = {201810271773.1}
}
</pre></td>
</tr>
<tr id="zhou2020method" class="entry">
	<td>Hao Zhou, Lei Li and Ning Miao, <a href="pubs/zhou2020method.pdf"><i>"Method and Device for Generating Text"</i></a>(201910105002.X), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhou2020method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_zhou2020method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{zhou2020method,
  author = {Zhou, Hao and Li, Lei and Miao, Ning},
  title = {Method and Device for Generating Text},
  year = {2020},
  number = {201910105002.X}
}
</pre></td>
</tr>
<tr id="li2020target" class="entry">
	<td>Gen Li, Shikun Xu, Yandong Zhu, Lei Li and Changhu Wang, <a href="pubs/li2020target.pdf"><i>"Target Object Image Detection Method and Device"</i></a>(201811010092.6), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2020target','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_li2020target" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{li2020target,
  author = {Li, Gen and Xu, Shikun and Zhu, Yandong and Li, Lei and Wang, Changhu},
  title = {Target Object Image Detection Method and Device},
  year = {2020},
  number = {201811010092.6}
}
</pre></td>
</tr>
<tr id="ye2020variational" class="entry">
	<td>Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei and Lei Li, <a href="pubs/ye2020variational.pdf"><i>"Variational Template Machine for Data-to-Text Generation"</i></a>, In International Conference on Learning Representations (ICLR), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('ye2020variational','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('ye2020variational','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_ye2020variational" class="abstract noshow">
	<td><b>Abstract</b>: How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include:  a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able generate more diversely while keeping a good fluency and quality.</td>
</tr>

<tr id="bib_ye2020variational" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{ye2020variational,
  author = {Rong Ye and Wenxian Shi and Hao Zhou and Zhongyu Wei and Lei Li},
  title = {Variational Template Machine for Data-to-Text Generation},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2020}
}
</pre></td>
</tr><tr id="yu2020method" class="entry">
	<td>Linyun Yu, Lei Li, Haibin Yin, Wenjia Zhu and Dong Jiang, <a href="pubs/yu2020method.pdf"><i>"Method and Apparatus for generating image"</i></a>(201810668219.7), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('yu2020method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_yu2020method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{yu2020method,
  author = {Yu, Linyun and Li, Lei and Yin, Haibin and Zhu, Wenjia and Jiang, Dong},
  title = {Method and Apparatus for generating image},
  year = {2020},
  number = {201810668219.7}
}
</pre></td>
</tr>
<tr id="zheng2020mirror" class="entry">
	<td>Zaixiang Zheng, Hao Zhou, Shujian Huang, Lei Li, Xinyu Dai and Jiajun Chen, <a href="pubs/zheng2020mirror.pdf"><i>"Mirror Generative Models for Neural Machine Translation"</i></a>, In International Conference on Learning Representations (ICLR), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zheng2020mirror','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zheng2020mirror','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_zheng2020mirror" class="abstract noshow">
	<td><b>Abstract</b>: Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation models and language models can collaborate together during decoding. Our experiments show that the proposed MGNMT consistently outperforms existing approaches in all a variety of scenarios and language pairs, including resource-rich and low-resource languages.</td>
</tr>

<tr id="bib_zheng2020mirror" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zheng2020mirror,
  author = {Zaixiang Zheng and Hao Zhou and Shujian Huang and Lei Li and Xinyu Dai and Jiajun Chen},
  title = {Mirror Generative Models for Neural Machine Translation},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2020}
}
</pre></td>
</tr><tr id="deng2020method" class="entry">
	<td>Jiangdong Deng, Qu Peng, Lei Li and Weiying Ma, <a href="pubs/deng2020method.pdf"><i>"A method for outputing information"</i></a>(201811074033.5), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('deng2020method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_deng2020method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{deng2020method,
  author = {Deng, Jiangdong and Peng, Qu and Li, Lei and Ma, Weiying},
  title = {A method for outputing information},
  year = {2020},
  number = {201811074033.5}
}
</pre></td>
</tr>
<tr id="li2020systems" class="entry">
	<td>Lei Li, Zihang Dai and Wei Xu, <a href="pubs/li2020systems.pdf"><i>"Systems and methods for human inspired simple question answering (HISQA)"</i></a>(US10606846B2), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2020systems','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_li2020systems" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{li2020systems,
  author = {Li, Lei and Dai, Zihang and Xu, Wei},
  title = {Systems and methods for human inspired simple question answering (HISQA)},
  year = {2020},
  number = {US10606846B2}
}
</pre></td>
</tr>
<tr id="zhou2020methodb" class="entry">
	<td>Hao Zhou and Lei Li, <a href="pubs/zhou2020methodb.pdf"><i>"Method and Device for generating information"</i></a>(201910105235.X), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhou2020methodb','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_zhou2020methodb" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{zhou2020methodb,
  author = {Zhou, Hao and Li, Lei},
  title = {Method and Device for generating information},
  year = {2020},
  number = {201910105235.X}
}
</pre></td>
</tr>
<tr id="zhou2020methoda" class="entry">
	<td>Hao Zhou, Lei Li, Jiaze Chen and Haoyue Shi, <a href="pubs/zhou2020methoda.pdf"><i>"Method and Device for generating information"</i></a>(201910105241.5), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('zhou2020methoda','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_zhou2020methoda" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{zhou2020methoda,
  author = {Zhou, Hao and Li, Lei and Chen, Jiaze and Shi, Haoyue},
  title = {Method and Device for generating information},
  year = {2020},
  number = {201910105241.5}
}
</pre></td>
</tr>
<tr id="huang2020span" class="entry">
	<td>Xunpeng Huang, Xianfeng Liang, Zhengyang Liu, Yue Yu and Lei Li, <a href="pubs/huang2020span.pdf"><i>"SPAN: A Stochastic Projected Approximate Newton Method"</i></a>, In the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('huang2020span','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('huang2020span','bibtex')">[BibTeX]</a>
  
  
   <a href="https://xunpeng746.github.io/projects/SPAN/SPAN.html">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_huang2020span" class="abstract noshow">
	<td><b>Abstract</b>: Second-order optimization methods have desirable convergence properties. However, the exact Newton method requires expensive computation for the Hessian and its inverse. In this paper, we propose SPAN, a novel approximate and fast Newton method. SPAN computes the inverse of the Hessian matrix via low-rank approximation and stochastic Hessian-vector products. Our experiments on multiple benchmark datasets demonstrate that SPAN outperforms existing first-order and second-order optimization methods in terms of the convergence wall-clock time. Furthermore, we provide a theoretical analysis of the per-iteration complexity, the approximation error, and the convergence rate. Both the theoretical analysis and experimental results show that our proposed method achieves a better trade-off between the convergence rate and the per-iteration efficiency.</td>
</tr>

<tr id="bib_huang2020span" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{huang2020span,
  author = {Xunpeng Huang and Xianfeng Liang and Zhengyang Liu and Yue Yu and Lei Li},
  title = {SPAN: A Stochastic Projected Approximate Newton Method},
  booktitle = {the 34th AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2020},
  url = {https://xunpeng746.github.io/projects/SPAN/SPAN.html}
}
</pre></td>
</tr><tr id="wang2020task" class="entry">
	<td>Xinlong Wang, Wei Yin, Tao Kong, Yuning Jiang, Lei Li and Chunhua Shen, <a href="pubs/wang2020task.pdf"><i>"Task-Aware Monocular Depth Estimation for 3D Object Detection"</i></a>, In the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2020task','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2020task','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2020task" class="abstract noshow">
	<td><b>Abstract</b>: Monocular depth estimation enables 3D perception from a single 2D image, thus attracting much research attention for years. Almost all methods treat foreground and background regions (``things and stuff'') in an image equally. However, not all pixels are equal. Depth of foreground objects plays a crucial role in 3D object recognition and localization. To date how to boost the depth prediction accuracy of foreground objects is rarely discussed. In this paper, we first analyse the data distributions and interaction of foreground and background, then propose the foreground-background separated monocular depth estimation (ForeSeE) method, to estimate the foreground depth and background depth using separate optimization objectives and depth decoders. Our method significantly improves the depth estimation performance on foreground objects. Applying ForeSeE to 3D object detection, we achieve 7.5 AP gains and set new state-of-the-art results among other monocular methods.</td>
</tr>

<tr id="bib_wang2020task" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2020task,
  author = {Xinlong Wang and Wei Yin and Tao Kong and Yuning Jiang and Lei Li and Chunhua Shen},
  title = {Task-Aware Monocular Depth Estimation for 3D Object Detection},
  booktitle = {the 34th AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2020}
}
</pre></td>
</tr><tr id="wu2020importance" class="entry">
	<td>Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng and Zhou Yu, <a href="pubs/wu2020importance.pdf"><i>"Importance-Aware Learning for Neural Headline Editing"</i></a>, In the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wu2020importance','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wu2020importance','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wu2020importance" class="abstract noshow">
	<td><b>Abstract</b>: Many social media news writers are not professionally trained. Therefore, social media platforms have to hire professional editors to adjust amateur headlines to attract more readers. We propose to automate this headline editing process through neural network models to provide more immediate writing support for these social media news writers. To train such a neural headline editing model, we collected a dataset which contains articles with original headlines and professionally edited headlines. However, it is expensive to collect a large number of professionally edited headlines. To solve this low-resource problem, we design an encoder-decoder model which leverages large scale pre-trained language models. We further improve the pre-trained model's quality by introducing a headline generation task as an intermediate task before the headline editing task. Also, we propose Self Importance-Aware (SIA) loss to address the different levels of editing in the dataset by down-weighting the importance of easily classified tokens and sentences. With the help of Pre-training, Adaptation, and SIA, the model learns to generate headlines in the professional editor's style. Experimental results show that our method significantly improves the quality of headline editing comparing against previous methods.</td>
</tr>

<tr id="bib_wu2020importance" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2020importance,
  author = {Qingyang Wu and Lei Li and Hao Zhou and Ying Zeng and Zhou Yu},
  title = {Importance-Aware Learning for Neural Headline Editing},
  booktitle = {the 34th AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2020}
}
</pre></td>
</tr><tr id="yang2020towards" class="entry">
	<td>Jiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Weinan Zhang, Yong Yu and Lei Li, <a href="pubs/yang2020towards.pdf"><i>"Towards Making the Most of BERT in Neural Machine Translation"</i></a>, In the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.
	
	<p class="infolinks"><a href="javascript:toggleInfo('yang2020towards','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('yang2020towards','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_yang2020towards" class="abstract noshow">
	<td><b>Abstract</b>: GPT-2 and BERT demonstrate the effectiveness of using pretrained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks. In this work, we introduce a concerted training framework (CTNMT) that is the key to integrate the pre-trained LMs to neural machine translation (NMT). Our proposed CTNMT consists of three techniques: a) asymptotic distillation to ensure that the NMT model can retain the previous pre-trained knowledge; b) a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and c) a strategy to adjust the learning paces according to a scheduled policy. Our experiments in machine translation show CTNMT gains of up<br>to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pretraining aided NMT by 1.4 BLEU score. While for the large WMT14 English-French task with 40 millions of sentencepairs, our base model still significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score.</td>
</tr>

<tr id="bib_yang2020towards" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{yang2020towards,
  author = {Jiacheng Yang and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Weinan Zhang and Yong Yu and Lei Li},
  title = {Towards Making the Most of BERT in Neural Machine Translation},
  booktitle = {the 34th AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2020}
}
</pre></td>
</tr><tr id="he2020methoda" class="entry">
	<td>Yi He, Lei Li, Xianzi Zong, Hao Tang and Guangguo Zheng, <a href="pubs/he2020methoda.pdf"><i>"Method and Device for searching information"</i></a>(201811060981.3), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('he2020methoda','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_he2020methoda" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{he2020methoda,
  author = {He, Yi and Li, Lei and Zong, Xianzi and Tang, Hao and Zheng, Guangguo},
  title = {Method and Device for searching information},
  year = {2020},
  number = {201811060981.3}
}
</pre></td>
</tr>
<tr id="yu2020methoda" class="entry">
	<td>Linyun Yu, Lei Li, Haibin Yin and Dong Jiang, <a href="pubs/yu2020methoda.pdf"><i>"Method and apparatus for generating image"</i></a>(201810669838.8), 2020.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('yu2020methoda','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_yu2020methoda" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{yu2020methoda,
  author = {Yu, Linyun and Li, Lei and Yin, Haibin and Jiang, Dong},
  title = {Method and apparatus for generating image},
  year = {2020},
  number = {201810669838.8}
}
</pre></td>
</tr>
<tr id="miao2019kernelized" class="entry">
	<td>Ning Miao, Hao Zhou, Chengqi Zhao, Wenxian Shi and Lei Li, <a href="pubs/miao2019kernelized.pdf"><i>"Kernelized Bayesian Softmax for Text Generation"</i></a>, In the 33rd Conference on Neural Information Processing Systems (NeurIPS), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('miao2019kernelized','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('miao2019kernelized','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_miao2019kernelized" class="abstract noshow">
	<td><b>Abstract</b>: Neural models for text generation require a softmax layer with proper word embeddings during the decoding phase. Most existing approaches adopt single point embedding for each word. However, a word may have multiple senses according to different context, some of which might be distinct. In this paper, we propose KerBS, a novel approach for learning better embeddings for text generation. KerBS embodies two advantages: a) it employs a Bayesian composition of embeddings for words with multiple senses; b) it is adaptive to semantic variances of words and<br>robust to rare sentence context by imposing learned kernels to capture the closeness of words (senses) in the embedding space. Empirical studies show that KerBS significantly boosts the performance of several text generation tasks.</td>
</tr>

<tr id="bib_miao2019kernelized" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{miao2019kernelized,
  author = {Miao, Ning and Zhou, Hao and Zhao, Chengqi and Shi, Wenxian and Li, Lei},
  title = {Kernelized Bayesian Softmax for Text Generation},
  booktitle = {the 33rd Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2019}
}
</pre></td>
</tr><tr id="chen2019method" class="entry">
	<td>Yangyu Chen, Yi He and Lei Li, <a href="pubs/chen2019method.pdf"><i>"A method and apparatus for determining a geometric transformation relationship between images"</i></a>(201811060837.X), 2019.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2019method','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2019method" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2019method,
  author = {Chen, Yangyu and He, Yi and Li, Lei},
  title = {A method and apparatus for determining a geometric transformation relationship between images},
  year = {2019},
  number = {201811060837.X}
}
</pre></td>
</tr>
<tr id="wang2019towards" class="entry">
	<td>Mingxuan Wang, Jun Xie, Zhixing Tan, Jinsong Su, Deyi Xiong and Lei Li, <a href="pubs/wang2019towards.pdf"><i>"Towards Linear Time Neural Machine Translation with Capsule Networks"</i></a>, In the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2019towards','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2019towards','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2019towards" class="abstract noshow">
	<td><b>Abstract</b>: In this study, we first investigate a novel capsule network with dynamic routing for linear time Neural Machine Translation (NMT), referred as CAPSNMT. CAPSNMT uses an aggregation mechanism to map the source sentence into a matrix with pre-determined size, and then applys a deep LSTM network to decode the target sequence from the source representation. Unlike the previous work (Sutskever et al., 2014) to store the source sentence with a passive and bottom-up way, the dynamic routing policy encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CAPSNMT has two core properties: it runs in time that is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, CAPSNMT achieves comparable results with the Transformer system. We also devise new hybrid architectures intended to combine the strength of CAPSNMT and the RNMT model. Our hybrid models obtain state-of-the-arts results on both benchmark datasets. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems</td>
</tr>

<tr id="bib_wang2019towards" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2019towards,
  author = {Wang, Mingxuan and Xie, Jun and Tan, Zhixing and Su, Jinsong and Xiong, Deyi and Li, Lei},
  title = {Towards Linear Time Neural Machine Translation with Capsule Networks},
  booktitle = {the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2019}
}
</pre></td>
</tr><tr id="zhao2019what" class="entry">
	<td>Zhichen Zhao, Lei Li, Bowen Zhang, Meng Wang, Yuning Jiang, Li Xu, Fengkun Wang and Weiying Ma, <a href="pubs/zhao2019what.pdf"><i>"What You Look Matters: Offline Evaluation of Advertising Creatives for Cold Start Problem"</i></a>, In the 28th ACM International Conference on Information and Knowledge Management (CIKM), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zhao2019what','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zhao2019what','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_zhao2019what" class="abstract noshow">
	<td><b>Abstract</b>: Modern online-auction-based advertising systems utilize user and item features to automatically place ads. In order to train a model to rank the most profitable ads, new ad creatives have to be placed online for hours to receive sufficient user-click data. This corresponds to the cold-start stage. Random strategy lead to inefficiency and inferior selections of potential ads. In this paper, we analyze the effectiveness of content-based selection during the cold-start stage. Specifically, we propose Pre Evaluation of Ad Creative Model (PEAC), a novel method to evaluate and select ad creatives offline before being placed online. Our proposed PEAC utilizes the automatically extracted deep feature from ad content to predict and rank their potential online placement performance. It does not rely on any user-click data, which is scarce during the cold-starting phase. A large-scale system based on our method has been deployed in a real online advertising platform. The online A/B testing shows the ads system with PEAC pre-ranking obtains significant improvement in revenue gain compared to the prior system. Furthermore, we provide detailed analyses on what the model learned, which gives further suggestions to improve ad creative design.</td>
</tr>

<tr id="bib_zhao2019what" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhao2019what,
  author = {Zhao, Zhichen and Li, Lei and Zhang, Bowen and Wang, Meng and Jiang, Yuning and Xu, Li and Wang, Fengkun and Ma, Weiying},
  title = {What You Look Matters: Offline Evaluation of Advertising Creatives for Cold Start Problem},
  booktitle = {the 28th ACM International Conference on Information and Knowledge Management (CIKM)},
  year = {2019}
}
</pre></td>
</tr><tr id="fu2019rethinking" class="entry">
	<td>Yao Fu, Hao Zhou, Jiaze Chen and Lei Li, <a href="pubs/fu2019rethinking.pdf"><i>"Rethinking Text Attribute Transfer: A Lexical Analysis"</i></a>, In the 12th International Conference on Natural Language Generation (INLG), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('fu2019rethinking','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('fu2019rethinking','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_fu2019rethinking" class="abstract noshow">
	<td><b>Abstract</b>: Text attribute transfer is modifying certain linguistic attributes (e.g. sentiment, style, authorship, etc.) of a sentence and transforming them from one type to another. In this paper, we aim to analyze and interpret what is changed during the transfer process. We start from the observation that in many existing models and datasets, certain words within a sentence play important roles in determining the sentence attribute class. These words are referred to as the Pivot Words. Based on these pivot words, we propose a lexical analysis framework, the Pivot Analysis, to quantitatively analyze the effects of these words in text attribute classification and transfer. We apply this framework to existing datasets and models, and show that: (1) the pivot words are strong features for the classification of sentence attributes; (2) to change the attribute of a sentence, many datasets only requires to change certain pivot words; (3) consequently, many transfer models only perform the lexical-level modification, while leaving higher-level sentence structures unchanged. Our work provides an in-depth understanding of linguistic attribute transfer and further identifies the future requirements and challenges of this task.</td>
</tr>

<tr id="bib_fu2019rethinking" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{fu2019rethinking,
  author = {Fu, Yao and Zhou, Hao and Chen, Jiaze and Li, Lei},
  title = {Rethinking Text Attribute Transfer: A Lexical Analysis},
  booktitle = {the 12th International Conference on Natural Language Generation (INLG)},
  year = {2019}
}
</pre></td>
</tr><tr id="jiang2019svd" class="entry">
	<td>Qing-Yuan Jiang, Yi He, Gen Li, Jian Lin, Lei Li and Wu-Jun Li., <a href="pubs/jiang2019svd.pdf"><i>"SVD: A Large-Scale Short Video Dataset for Near Duplicate Video Retrieval."</i></a>, In International Conference on Computer Vision (ICCV), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('jiang2019svd','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('jiang2019svd','bibtex')">[BibTeX]</a>
  
  
   <a href="https://svdbase.github.io">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_jiang2019svd" class="abstract noshow">
	<td><b>Abstract</b>: With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difficult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io.</td>
</tr>

<tr id="bib_jiang2019svd" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{jiang2019svd,
  author = {Jiang, Qing-Yuan and He, Yi and Li, Gen and Lin, Jian and Li, Lei and Li., Wu-Jun},
  title = {SVD: A Large-Scale Short Video Dataset for Near Duplicate Video Retrieval.},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year = {2019},
  url = {https://svdbase.github.io}
}
</pre></td>
</tr><tr id="wang2019vatex" class="entry">
	<td>Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang and William Yang Wang, <a href="pubs/wang2019vatex.pdf"><i>"VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"</i></a>, In International Conference on Computer Vision (ICCV), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wang2019vatex','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wang2019vatex','bibtex')">[BibTeX]</a>
  
  
   <a href="https://vatex.org/main/index.html">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wang2019vatex" class="abstract noshow">
	<td><b>Abstract</b>: We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.</td>
</tr>

<tr id="bib_wang2019vatex" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wang2019vatex,
  author = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  title = {VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year = {2019},
  url = {https://vatex.org/main/index.html}
}
</pre></td>
</tr><tr id="lu2019uncovering" class="entry">
	<td>Yunfei Lu, Linyun Yu, Peng Cui, Chengxi Zang, Renzhe Xu, Yihao Liu, Lei Li and Wenwu Zhu, <a href="pubs/lu2019uncovering.pdf"><i>"Uncovering the Co-driven Mechanism of Social and Content Links in User Churn Phenomena"</i></a>, In the 25th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('lu2019uncovering','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('lu2019uncovering','bibtex')">[BibTeX]</a>
   <a href="pubs/lu-kdd2019-social-content-links.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_lu2019uncovering" class="abstract noshow">
	<td><b>Abstract</b>: Recent years witness the merge of social networks and user-generatedcontent (UGC) platforms. In these new platforms, users establishlinks to others not only driven by their social relationships in thephysical world but also driven by the contents published by others.During this merging process, social networks gradually integrateboth social and content links and become unprecedentedly complicated,with the motivation to exploit both the advantages of socialviscosity and content attractiveness to reach the best customerretention situation. However, due to the lack of fine-grained datarecording such merging phenomena, the co-driven mechanism ofsocial and content links in churn phenomena remains unexplored.How do social and content factors jointly influence customers’churn? What is the best ratio of social and content links for a user’sretention? Is there a model to capture this co-driven mechanism inusers’ churn phenomena?In this paper, we collect a real-world dataset with more than 5.77million users and 925 million links, with each link being tagged asa social one or a content one. We find that both social and contentlinks have a significant impact on users’ churn and theywork jointlyas a complicated mixture effect. As a result, we propose a novelsurvival model, which incorporates both social and content factors,to predict churn probability over time. Our model successfully fitsthe churn distribution in reality and accurately predicts the churnrate of different subpopulations in the future. By analyzing themodeling parameters, we try to strike a balance between socialdrivenand content-driven links in a user’s social network to reachthe lowest churn rate. Our model and findings may have potentialimplications for the design of future social media.</td>
</tr>

<tr id="bib_lu2019uncovering" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{lu2019uncovering,
  author = {Lu, Yunfei and Yu, Linyun and Cui, Peng and Zang, Chengxi and Xu, Renzhe and Liu, Yihao and Li, Lei and Zhu, Wenwu},
  title = {Uncovering the Co-driven Mechanism of Social and Content Links in User Churn Phenomena},
  booktitle = {the 25th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2019}
}
</pre></td>
</tr><tr id="sun2019graspsnooker" class="entry">
	<td>Zhaoyue Sun, Jiaze Chen, Hao Zhou, Deyu Zhou, Lei Li and Mingmin Jiang, <a href="pubs/sun2019graspsnooker.pdf"><i>"GraspSnooker: Automatic Chinese Commentary Generation for Snooker Videos"</i></a>, In the 28th International Joint Conference on Artificial Intelligence (IJCAI), pp. 6569-6571., 2019.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sun2019graspsnooker','bibtex')">[BibTeX]</a>
  
   <a href="https://doi.org/10.24963/ijcai.2019/959">[DOI]</a>
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_sun2019graspsnooker" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sun2019graspsnooker,
  author = {Sun, Zhaoyue and Chen, Jiaze and Zhou, Hao and Zhou, Deyu and Li, Lei and Jiang, Mingmin},
  title = {GraspSnooker: Automatic Chinese Commentary Generation for Snooker Videos},
  booktitle = {the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
  year = {2019},
  pages = {6569--6571},
  doi = {https://doi.org/10.24963/ijcai.2019/959}
}
</pre></td>
</tr><tr id="weng2019correct" class="entry">
	<td>Rongxiang Weng, Hao Zhou, Shujian Huang, Yifan Xia, Lei Li and Jiajun Chen, <a href="pubs/weng2019correct.pdf"><i>"Correct-and-Memorize: Learning to Translate from Interactive Revisions"</i></a>, In the 28th International Joint Conference on Artificial Intelligence (IJCAI), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('weng2019correct','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('weng2019correct','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_weng2019correct" class="abstract noshow">
	<td><b>Abstract</b>: State-of-the-art machine translation models are stillnot on a par with human translators. Previous worktakes human interactions into the neural machine translation process to obtain improved results in target languages. However, not all model-translation errors are equal some are critical while others are minor. In the mean while, same translation mistakes occur repeatedly in similar context. To solve bothissues, we propose CAMIT, a novel method for translating in an interactive environment. Our proposed method works with critical revision instructions,therefore allows human to correct arbitrary words in model-translated sentences. In addition,CAMIT learns from and softly memorizes revision actions based on the context, alleviating the issue of repeating mistakes. Experiments in both ideal and real interactive translation settings demonstrate that our proposed CAMIT enhances machine translation results significantly while requires fewer revision instructions from human compared to previous methods.</td>
</tr>

<tr id="bib_weng2019correct" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{weng2019correct,
  author = {Weng, Rongxiang and Zhou, Hao and Huang, Shujian and Xia, Yifan and Li, Lei and Chen, Jiajun},
  title = {Correct-and-Memorize: Learning to Translate from Interactive Revisions},
  booktitle = {the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
  year = {2019}
}
</pre></td>
</tr><tr id="bao2019generating" class="entry">
	<td>Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai and Jiajun Chen, <a href="pubs/bao2019generating.pdf"><i>"Generating Sentences from Disentangled Syntactic and Semantic Spaces"</i></a>, In the 57th Annual Meeting of the Association for Computational Linguistics (ACL), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('bao2019generating','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('bao2019generating','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_bao2019generating" class="abstract noshow">
	<td><b>Abstract</b>: Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE’s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax-transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.</td>
</tr>

<tr id="bib_bao2019generating" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{bao2019generating,
  author = {Bao, Yu and Zhou, Hao and Huang, Shujian and Li, Lei and Mou, Lili and Vechtomova, Olga and Dai, Xinyu and Chen, Jiajun},
  title = {Generating Sentences from Disentangled Syntactic and Semantic Spaces},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2019}
}
</pre></td>
</tr><tr id="qiu2019dynamically" class="entry">
	<td>Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang and Yong Yu, <a href="pubs/qiu2019dynamically.pdf"><i>"Dynamically Fused Graph Network for Multi-hop Reasoning"</i></a>, In the 57th Annual Meeting of the Association for Computational Linguistics (ACL), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('qiu2019dynamically','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('qiu2019dynamically','bibtex')">[BibTeX]</a>
   <a href="pubs/qiu-acl2019-dfgn.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_qiu2019dynamically" class="abstract noshow">
	<td><b>Abstract</b>: Text-based question answering (TBQA) has been studied extensively in recent years. Most existing approaches focus on finding the answer to a question within a single paragraph. However, many difficult questions require multiple supporting evidence from scattered text across two or more documents. In this paper, we propose the Dynamically Fused Graph Network (DFGN), a novel method to answer those questions requiring multiple scattered evidence and reasoning over them. Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents. We evaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning. DFGN achieves competitive results on the public board. Furthermore, our analy- sis shows DFGN could produce interpretable reasoning chains.</td>
</tr>

<tr id="bib_qiu2019dynamically" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{qiu2019dynamically,
  author = {Qiu, Lin and Xiao, Yunxuan and Qu, Yanru and Zhou, Hao and Li, Lei and Zhang, Weinan and Yu, Yong},
  title = {Dynamically Fused Graph Network for Multi-hop Reasoning},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2019}
}
</pre></td>
</tr><tr id="zhang2019generating" class="entry">
	<td>Huangzhao Zhang, Ning Miao, Hao Zhou and Lei Li, <a href="pubs/zhang2019generating.pdf"><i>"Generating Fluent Adversarial Examples for Natural Languages"</i></a>, In the 57th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers, 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('zhang2019generating','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('zhang2019generating','bibtex')">[BibTeX]</a>
   <a href="pubs/zhang-acl2019generation-cgmh-adversarial.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_zhang2019generating" class="abstract noshow">
	<td><b>Abstract</b>: Efficiently building an adversarial attacker fornatural language processing (NLP) tasks is areal challenge. Firstly, as the sentence spaceis discrete, it is difficult to make small perturbations along the direction of gradients. Secondly,the fluency of the generated examples can not be guaranteed. In this paper, we propose MHA, which addresses both problemsby performing Metropolis-Hastings sampling,whose proposal is designed with the guidanceof gradients. Experiments on IMDB and SNLIshow that our proposed MHA outperforms thebaseline model on attacking capability. Adversarial training with MHA also leads to better robustness and performance.</td>
</tr>

<tr id="bib_zhang2019generating" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{zhang2019generating,
  author = {Zhang, Huangzhao and Miao, Ning and Zhou, Hao and Li, Lei},
  title = {Generating Fluent Adversarial Examples for Natural Languages},
  booktitle = {the 57th Annual Meeting of the Association for Computational Linguistics (ACL) - short papers},
  year = {2019}
}
</pre></td>
</tr><tr id="wu2019unified" class="entry">
	<td>Hao Wu, Jiayuan Mao, Yufeng Zhang, Weiwei Sun, Yuning Jiang, Lei Li and Wei-Ying Ma, <a href="pubs/wu2019unified.pdf"><i>"Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations"</i></a>, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wu2019unified','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wu2019unified','bibtex')">[BibTeX]</a>
   <a href="pubs/wu-cvpr2019-vse.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_wu2019unified" class="abstract noshow">
	<td><b>Abstract</b>: We propose Unified Visual-Semantic Embeddings (VSE)<br>for learning a joint space for scene representation and textual<br>semantics. It unifies the embeddings of concepts at different<br>levels: objects, attributes, relations and full scenes. We<br>view the sentential semantics as a combination of different<br>semantic components such as object or relational descriptors,<br>and align their embeddings with different regions of a<br>scene. A contrastive learning approach is proposed for the<br>effective learning of such fine-grained alignment from only<br>image-caption pairs. We also present a simple yet effective<br>approach that enforces the coverage of caption embeddings<br>on the semantic components that appear in the sentence. We<br>demonstrate that the Unified VSE outperforms other baselines<br>on cross-modal retrieval tasks and the enforcement<br>of the semantic coverage improves models’ robustness in<br>defending text-domain adversarial attacks. Moreover, such<br>robustness empowers the use of visual cues to accurately<br>resolve word dependencies in novel sentences.</td>
</tr>

<tr id="bib_wu2019unified" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2019unified,
  author = {Wu, Hao and Mao, Jiayuan and Zhang, Yufeng and Sun, Weiwei and Jiang, Yuning and Li, Lei and Ma, Wei-Ying},
  title = {Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2019}
}
</pre></td>
</tr><tr id="jiang2019construction" class="entry">
	<td>Dong Jiang, Yanbin Zhao, Shuang Hou, Xuhong Xia, Lei Li and Dingkun Hong, <a href="pubs/jiang2019construction.pdf"><i>"Construction method and device of voice classification model."</i></a>(201710388497.2), 2019.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('jiang2019construction','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_jiang2019construction" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{jiang2019construction,
  author = {Jiang, Dong and Zhao, Yanbin and Hou, Shuang and Xia, Xuhong and Li, Lei and Hong, Dingkun},
  title = {Construction method and device of voice classification model.},
  year = {2019},
  number = {201710388497.2}
}
</pre></td>
</tr>
<tr id="miao2019cgmh" class="entry">
	<td>Ning Miao, Hao Zhou, Lili Mou, Rui Yan and Lei Li, <a href="pubs/miao2019cgmh.pdf"><i>"CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling"</i></a>, In the 33rd AAAI Conference on Artificial Intelligence (AAAI), 2019.
	
	<p class="infolinks"><a href="javascript:toggleInfo('miao2019cgmh','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('miao2019cgmh','bibtex')">[BibTeX]</a>
  
  
  
	
	 <a href="pubs/miao2019cgmh-ppt.pdf">[PPT]</a>
	
	</p>
	</td>
</tr>

<tr id="abs_miao2019cgmh" class="abstract noshow">
	<td><b>Abstract</b>: In real-world applications of natural language generation,<br>there are often constraints on the target sentences in addition<br>to fluency and naturalness requirements. Existing language<br>generation techniques are usually based on recurrent<br>neural networks (RNNs). However, it is non-trivial to impose<br>constraints on RNNs while maintaining generation quality,<br>since RNNs generate sentences sequentially (or with beam<br>search) from the first word to the last. In this paper, we propose<br>CGMH, a novel approach using Metropolis-Hastings<br>sampling for constrained sentence generation. CGMH allows<br>complicated constraints such as the occurrence of multiple<br>keywords in the target sentences, which cannot be handled in<br>traditional RNN-based approaches. Moreover, CGMH works<br>in the inference stage, and does not require parallel corpora<br>for training.We evaluate our method on a variety of tasks, including<br>keywords-to-sentence generation, unsupervised sentence<br>paraphrasing, and unsupervised sentence error correction.<br>CGMH achieves high performance compared with previous<br>supervised methods for sentence generation. Our code<br>is released at https://github.com/NingMiao/CGMH</td>
</tr>

<tr id="bib_miao2019cgmh" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{miao2019cgmh,
  author = {Miao, Ning and Zhou, Hao and Mou, Lili and Yan, Rui and Li, Lei},
  title = {CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling},
  booktitle = {the 33rd AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2019}
}
</pre></td>
</tr><tr id="cao2018brits" class="entry">
	<td>Wei Cao, Dong Wang, Jian Li, Hao Zhou, Yitan Li and Lei Li, <a href="pubs/cao2018brits.pdf"><i>"BRITS: Bidirectional Recurrent Imputation for Time Series"</i></a>, In the 32nd Conference on Neural Information Processing Systems (NeurIPS), 2018.
	
	<p class="infolinks"><a href="javascript:toggleInfo('cao2018brits','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('cao2018brits','bibtex')">[BibTeX]</a>
  
  
   <a href="https://arxiv.org/abs/1805.10572">[URL]</a>
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_cao2018brits" class="abstract noshow">
	<td><b>Abstract</b>: Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation.BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.</td>
</tr>

<tr id="bib_cao2018brits" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{cao2018brits,
  author = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Yitan and Li, Lei},
  title = {BRITS: Bidirectional Recurrent Imputation for Time Series},
  booktitle = {the 32nd Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2018},
  url = {https://arxiv.org/abs/1805.10572}
}
</pre></td>
</tr><tr id="li2018overview" class="entry">
	<td>Lei Li and Xiaojun Wan, <a href="pubs/li2018overview.pdf"><i>"Overview of the NLPCC 2018 shared task: Single document summarization"</i></a>, In Proc. of NLPCC, 2018.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2018overview','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_li2018overview" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2018overview,
  author = {Lei Li and Xiaojun Wan},
  title = {Overview of the NLPCC 2018 shared task: Single document summarization},
  booktitle = {Proc. of NLPCC},
  year = {2018}
}
</pre></td>
</tr><tr id="shi2018tree" class="entry">
	<td>Haoyue Shi, Hao Zhou, Jiaze Chen and Lei Li, <a href="pubs/shi2018tree.pdf"><i>"On Tree-Based Neural Sentence Modeling"</i></a>, In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.
	
	<p class="infolinks"><a href="javascript:toggleInfo('shi2018tree','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('shi2018tree','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_shi2018tree" class="abstract noshow">
	<td><b>Abstract</b>: Neural networks with tree-based sentence encoders have shown better results on many downstream tasks. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (i.e., binary balanced tree, left-branching tree and right-branching tree) in the encoders. Though trivial trees contain no syntactic information, those encoders get competitive or even better results on all of the ten downstream tasks we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that tree modeling gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder.</td>
</tr>

<tr id="bib_shi2018tree" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{shi2018tree,
  author = {Shi, Haoyue and Zhou, Hao and Chen, Jiaze and Li, Lei},
  title = {On Tree-Based Neural Sentence Modeling},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2018}
}
</pre></td>
</tr><tr id="li2018jersey" class="entry">
	<td>Gen Li, Shikun Xu, Xiang Liu, Lei Li and Changhu Wang, <a href="pubs/li2018jersey.pdf"><i>"Jersey Number Recognition with Semi-Supervised Spatial Transformer Network"</i></a>, In IEEE Conference on Computer Vision and Pattern Recognition workshops, Computer Vision in Sports, pp. 1864 -1871., 2018.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2018jersey','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2018jersey','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_li2018jersey" class="abstract noshow">
	<td><b>Abstract</b>: It is still a challenging task to recognize the jersey number <br>of players on the court in soccer match videos, as the<br>jersey numbers are very small in the object detection task<br>and annotated data are not easy to collect. Based on the<br>object detection results of all the players on the court, a<br>CNN model is first introduced to classify these numbers on<br>the deteced players’ images. To localize the jersey number<br>more precisely without involving another digit detector and<br>extra consumption, we then improve the former network to<br>an end-to-end framework by fusing with the spatial transformer<br>network (STN). To further improve the accuracy, we<br>bring extra supervision to STN and upgrade the model to<br>a semi-supervised multi-task learning system, by labeling a<br>small portion of the number areas in the dataset by quadrangle.<br>Extensive experiments illustrate the effectiveness of<br>the proposed framework.</td>
</tr>

<tr id="bib_li2018jersey" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2018jersey,
  author = {Li, Gen and Xu, Shikun and Liu, Xiang and Li, Lei and Wang, Changhu},
  title = {Jersey Number Recognition with Semi-Supervised Spatial Transformer Network},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition workshops, Computer Vision in Sports},
  year = {2018},
  pages = {1864 --1871}
}
</pre></td>
</tr><tr id="wu2018reinforced" class="entry">
	<td>Jiawei Wu, Lei Li and William Yang Wang, <a href="pubs/wu2018reinforced.pdf"><i>"Reinforced Co-Training"</i></a>, In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), New Orleans, Louisiana, pp. 1252-1262. Association for Computational Linguistics, 2018.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wu2018reinforced','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wu2018reinforced','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wu2018reinforced" class="abstract noshow">
	<td><b>Abstract</b>: Co-training is a popular semi-supervised learning framework to utilize a<br>large amount of unlabeled data in addition to a small labeled set. Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training. However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the sampling bias between the unlabeled and the labeled subsets, and fails to explore the data space. In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on. More specifically, our approach uses Q-learning to learn a data selection policy with a small labeled dataset, and then exploits this policy to train the co-training classifiers automatically. Experimental results on<br>clickbait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate text classification results.</td>
</tr>

<tr id="bib_wu2018reinforced" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2018reinforced,
  author = {Wu, Jiawei and Li, Lei and Wang, William Yang},
  title = {Reinforced Co-Training},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  publisher = {Association for Computational Linguistics},
  year = {2018},
  pages = {1252--1262}
}
</pre></td>
</tr><tr id="hua2017overview" class="entry">
	<td>Lifeng Hua, Xiaojun Wan and Lei Li, <a href="pubs/hua2017overview.pdf"><i>"Overview of the NLPCC 2017 shared task: Single document summarization"</i></a>, In Proc. of NLPCC, 2017.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('hua2017overview','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_hua2017overview" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{hua2017overview,
  author = {Lifeng Hua and Xiaojun Wan and Lei Li},
  title = {Overview of the NLPCC 2017 shared task: Single document summarization},
  booktitle = {Proc. of NLPCC},
  year = {2017}
}
</pre></td>
</tr><tr id="erol2017nearly" class="entry">
	<td>Yusuf Erol, Yi Wu, Lei Li and Stuart Russell, <a href="pubs/erol2017nearly.pdf"><i>"A Nearly-Black-Box Online Algorithm for Joint Parameter and State Estimation in Temporal Models"</i></a>, In the 31st AAAI Conference on Artificial Intelligence (AAAI), 2017.
	
	<p class="infolinks"><a href="javascript:toggleInfo('erol2017nearly','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('erol2017nearly','bibtex')">[BibTeX]</a>
   <a href="pubs/erol-aaai2017-apf-appendix.pdf">&#91;Appendix_PDF&#93;</a>
  
   <a href="pubs/erol-aaai2017-apf-appendix.pdf">[URL]</a>
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_erol2017nearly" class="abstract noshow">
	<td><b>Abstract</b>: Online joint parameter and state estimation is a core problem for temporal models. Most existing methods are either restricted to a particular class of models (e.g., the Storvik filter) or computationally expensive (e.g., particle MCMC). We propose a novel nearly-black-box algorithm, the Assumed Parameter Filter (APF), a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has the following advantages: (a) it is online and computationally efficient; (b) it is applicable to both discrete and continuous parameter spaces with arbitrary transition dynamics. On a variety of toy and real models, APF generates more accurate results within a fixed computation budget compared to several standard algorithms from the literature.</td>
</tr>

<tr id="bib_erol2017nearly" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{erol2017nearly,
  author = {Erol, Yusuf and Wu, Yi and Li, Lei and Russell, Stuart},
  title = {A Nearly-Black-Box Online Algorithm for Joint Parameter and State Estimation in Temporal Models},
  booktitle = {the 31st AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2017},
  url = {pubs/erol-aaai2017-apf-appendix.pdf}
}
</pre></td>
</tr><tr id="matsubara2017non" class="entry">
	<td>Yasuko Matsubara, Yasushi Sakurai, B. Aditya Prakash, Lei Li and Christos Faloutsos, <a href="pubs/matsubara2017non.pdf"><i>"Non-linear Dynamics of Information Diffusion in Social Networks"</i></a>, ACM Transactions on the Web, Volume 11(1), 2017.
	
	<p class="infolinks"><a href="javascript:toggleInfo('matsubara2017non','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('matsubara2017non','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>

<tr id="abs_matsubara2017non" class="abstract noshow">
	<td><b>Abstract</b>: The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this paper, we propose SPIKEM, a concise yet flexible analytical model of the rise and fall patterns of information diffusion. Our model has the following advantages: (a) unification power: it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models. We provide the threshold of the take-off vs. die-out conditions for SPIKEM, and discuss the generality of our model, by applying it to an arbitrary graph topology; (b) practicality: it matches the observed behavior of diverse sets of real data; (c) parsimony: it requires only a handful of parameters; and (d) usefulness: it makes it possible to perform analytic tasks such as forecasting, spotting anomalies, and interpretation by reverse engineering the system parameters of interest (e.g. quality of news, number of interested bloggers, etc.). We also introduce an efficient and effective algorithm for the real-time monitoring of information diffusion, namely, SPIKESTREAM, which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets demonstrate that SPIKEM accurately and succinctly describes all the patterns of the rise-and-fall spikes in social networks.</td>
</tr>

<tr id="bib_matsubara2017non" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{matsubara2017non,
  author = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
  title = {Non-linear Dynamics of Information Diffusion in Social Networks},
  journal = {ACM Transactions on the Web},
  year = {2017},
  volume = {11},
  number = {1}
}
</pre></td>
</tr>
<tr id="dai2016cfo" class="entry">
	<td>Zihang Dai, Lei Li and Wei Xu, <a href="pubs/dai2016cfo.pdf"><i>"CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases"</i></a>, In the 54th Annual Meeting of the Association for Computational Linguistics (ACL), 2016.
	
	<p class="infolinks"><a href="javascript:toggleInfo('dai2016cfo','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('dai2016cfo','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_dai2016cfo" class="abstract noshow">
	<td><b>Abstract</b>: How can we enable computers to automatically answer questions like ``Who created the character Harry Potter''? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions --- ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neural-network-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions - the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%.</td>
</tr>

<tr id="bib_dai2016cfo" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{dai2016cfo,
  author = {Dai, Zihang and Li, Lei and Xu, Wei},
  title = {CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases},
  booktitle = {the 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year = {2016}
}
</pre></td>
</tr><tr id="wu2016swift" class="entry">
	<td>Yi Wu, Lei Li, Stuart Russell and Rastislav Bodik, <a href="pubs/wu2016swift.pdf"><i>"Swift: Compiled Inference for Probabilistic Programming Languages"</i></a>, In 25th International Joint Conference on Artificial Intelligence (IJCAI), 2016.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wu2016swift','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wu2016swift','bibtex')">[BibTeX]</a>
  
  
  
	
	
	
	</p>
	</td>
</tr>

<tr id="abs_wu2016swift" class="abstract noshow">
	<td><b>Abstract</b>: A probabilistic program defines a probability measure over its semantic structures. One common goal of probabilistic programming languages (PPLs) is to compute posterior probabilities for arbitrary models and queries, given observed evidence, using a generic inference engine. Most PPL inference engines—even the compiled ones—incur significant runtime interpretation overhead, especially for contingent and open-universe models. This paper describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates optimizations that eliminate interpretation overhead, maintain dynamic dependencies efficiently, and handle memory management for possible worlds of varying sizes. Experiments comparing Swift with other PPL engines on a variety of inference problems demonstrate speedups ranging from 12x to 326x.</td>
</tr>

<tr id="bib_wu2016swift" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2016swift,
  author = {Wu, Yi and Li, Lei and Russell, Stuart and Bodik, Rastislav},
  title = {Swift: Compiled Inference for Probabilistic Programming Languages},
  booktitle = {25th International Joint Conference on Artificial Intelligence (IJCAI)},
  year = {2016}
}
</pre></td>
</tr><tr id="lu2015twisted" class="entry">
	<td>Zefu Lu, Lei Li and Wei Xu, <a href="pubs/lu2015twisted.pdf"><i>"Twisted Recurrent Network for Named Entity Recognition"</i></a>
  , 2015.
	<p class="infolinks"> <a href="javascript:toggleInfo('lu2015twisted','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
  </p>
	</td>
</tr>
<tr id="bib_lu2015twisted" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@misc{lu2015twisted,
  author = {Lu, Zefu and Li, Lei and Xu, Wei},
  title = {Twisted Recurrent Network for Named Entity Recognition},
  booktitle = {Bay Area Machine Learning Symposium},
  year = {2015}
}
</pre></td>
</tr>
<tr id="pham2015optimization" class="entry">
	<td>Hieu Pham, Zihang Dai and Lei Li, <a href="pubs/pham2015optimization.pdf"><i>"On Optimization Algorithms for Recurrent Networks with Long Short-Term Memory"</i></a>
  , 2015.
	<p class="infolinks"> <a href="javascript:toggleInfo('pham2015optimization','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
  </p>
	</td>
</tr>
<tr id="bib_pham2015optimization" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@misc{pham2015optimization,
  author = {Pham, Hieu and Dai, Zihang and Li, Lei},
  title = {On Optimization Algorithms for Recurrent Networks with Long Short-Term Memory},
  booktitle = {Bay Area Machine Learning Symposium},
  year = {2015}
}
</pre></td>
</tr>
<tr id="du2014maxios" class="entry">
	<td>Simon Shaolei Du, Yilin Liu, Boyi Chen and Lei Li, <a href="pubs/du2014maxios.pdf"><i>"Maxios: Large Scale Nonnegative Matrix Factorization for Collaborative Filtering"</i></a>, In Neural Information Processing Systems, workshop on Distributed Machine Learning and Matrix Computations, 2014.
	
	<p class="infolinks"><a href="javascript:toggleInfo('du2014maxios','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('du2014maxios','bibtex')">[BibTeX]</a>
   <a href="pubs/du2014maxios - Maxios_ Large Scale Nonnegative Matrix Factorization for Collaborative Filtering.pdf">&#91;PDF&#93;</a>
  
  
	
	 <a href="pubs/du-2014-maxios-poster.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>

<tr id="abs_du2014maxios" class="abstract noshow">
	<td><b>Abstract</b>: Nonnegative matrix factorization proved useful in many applications, including collaborative filtering – from existing ratings data one would like to predict new product ratings by users. However, factorizing a user-product score matrix is computation and memory intensive. We propose Maxios, a novel approach to fill missing values for large scale and highly sparse matrices efficiently and ac- curately. We formulate the matrix-completion problem as weighted nonnegative matrix factorization. In addition, we develop distributed update rules using alter- nating direction method of multipliers. We have implemented the Maxios system on top of Spark, a distributed in-memory computation framework. Experiments on commercial clusters show that Maxios is competitive in terms of scalability and accuracy against the existing solutions on a variety of datasets.</td>
</tr>

<tr id="bib_du2014maxios" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{du2014maxios,
  author = {Du, Simon Shaolei and Liu, Yilin and Chen, Boyi and Li, Lei},
  title = {Maxios: Large Scale Nonnegative Matrix Factorization for Collaborative Filtering},
  booktitle = {Neural Information Processing Systems, workshop on Distributed Machine Learning and Matrix Computations},
  year = {2014}
}
</pre></td>
</tr><tr id="juan2014poisson" class="entry">
	<td>Da-Cheng Juan, Lei Li, Huan-Kai Peng, Diana Marculescu and Christos Faloutsos, <a href="pubs/juan2014poisson.pdf"><i>"Beyond Poisson: Modeling Inter-Arrival Times of Requests in a Datacenter"</i></a>, In The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), 2014.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('juan2014poisson','bibtex')">[BibTeX]</a>
   <a href="pubs/juan2014poisson - Beyond Poisson_ Modeling Inter-Arrival Times of Requests in a Datacenter.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_juan2014poisson" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{juan2014poisson,
  author = {Juan, Da-Cheng and Li, Lei and Peng, Huan-Kai and Marculescu, Diana and Faloutsos, Christos},
  title = {Beyond Poisson: Modeling Inter-Arrival Times of Requests in a Datacenter},
  booktitle = {The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)},
  year = {2014}
}
</pre></td>
</tr><tr id="wu2014bfit" class="entry">
	<td>Yi Wu, Lei Li and Stuart J. Russell, <a href="pubs/wu2014bfit.pdf"><i>"BFiT: From Possible-World Semantics to Random-Evaluation Semantics in Open Universe"</i></a>, In Neural Information Processing Systems, Probabilistic Programming workshop, 2014.
	
	<p class="infolinks"><a href="javascript:toggleInfo('wu2014bfit','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('wu2014bfit','bibtex')">[BibTeX]</a>
   <a href="pubs/wu2014bfit - BFiT_ From Possible-World Semantics to Random-Evaluation Semantics in Open Universe.pdf">&#91;PDF&#93;</a>
  
  
	
	 <a href="pubs/wu-2014-bfit-poster.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>

<tr id="abs_wu2014bfit" class="abstract noshow">
	<td><b>Abstract</b>: In recent years, several probabilistic programming languages (PPLs) have emerged, such as Bayesian Logic (BLOG), Church, and Figaro. These languages can be classified into two categories: PPLs interpreted using possible-world se- mantics and ones using random-evaluation semantics. In this paper, we explic- itly analyze the equivalence between these two semantics in the context of open- universe probability models (OUPMs). We propose a novel dynamic memoization technique to construct OUPMs using procedural instructions in random-evaluation based PPLs. We implemented a translator named BFiT, which converts code in BLOG (possible-world based) to Figaro (random-evaluation based). The trans- lated program in Figaro exhibits a merely constant blowup factor in program size while yielding the same inference results as the original model in BLOG.</td>
</tr>

<tr id="bib_wu2014bfit" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{wu2014bfit,
  author = {Wu, Yi and Li, Lei and Russell, Stuart J.},
  title = {BFiT: From Possible-World Semantics to Random-Evaluation Semantics in Open Universe},
  booktitle = {Neural Information Processing Systems, Probabilistic Programming workshop},
  year = {2014}
}
</pre></td>
</tr><tr id="erol2013extended" class="entry">
	<td>Yusuf Erol, Lei Li, Bharath Ramsundar and Stuart J. Russell, <a href="pubs/erol2013extended.pdf"><i>"The Extended Parameter Filter"</i></a>, In Proceedings of the 30th International Conference on Machine learning (ICML), 2013.
	
	<p class="infolinks"><a href="javascript:toggleInfo('erol2013extended','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('erol2013extended','bibtex')">[BibTeX]</a>
   <a href="pubs/erol2013extended - The Extended Parameter Filter.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_erol2013extended" class="abstract noshow">
	<td><b>Abstract</b>: The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.</td>
</tr>

<tr id="bib_erol2013extended" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{erol2013extended,
  author = {Erol, Yusuf and Li, Lei and Ramsundar, Bharath and Russell, Stuart J.},
  title = {The Extended Parameter Filter},
  booktitle = {Proceedings of the 30th International Conference on Machine learning (ICML)},
  year = {2013}
}
</pre></td>
</tr><tr id="fu2013why" class="entry">
	<td>Bin Fu, Jialiu Lin, Lei Li, Christos Faloutsos, Jason Hong and Norman Sadeh, <a href="pubs/fu2013why.pdf"><i>"Why People Hate Your App - Making Sense of User Feedback in a Mobile App Store"</i></a>, In the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2013.
	
	<p class="infolinks"><a href="javascript:toggleInfo('fu2013why','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('fu2013why','bibtex')">[BibTeX]</a>
   <a href="pubs/fu2013why - Why People Hate Your App - Making Sense of User Feedback in a Mobile App Store.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_fu2013why" class="abstract noshow">
	<td><b>Abstract</b>: User review is a crucial component of open mobile app mar- kets such as the Google Play Store. How do we automatically summarize millions of user reviews and make sense out of them? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose WisCom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users’ reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying users’ major concerns and preferences of different types of apps. Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users.</td>
</tr>

<tr id="bib_fu2013why" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{fu2013why,
  author = {Fu, Bin and Lin, Jialiu and Li, Lei and Faloutsos, Christos and Hong, Jason and Sadeh, Norman},
  title = {Why People Hate Your App - Making Sense of User Feedback in a Mobile App Store},
  booktitle = {the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2013}
}
</pre></td>
</tr><tr id="li2013dynamic" class="entry">
	<td>Lei Li, Bharath Ramsundar and Stuart Russell, <a href="pubs/li2013dynamic.pdf"><i>"Dynamic Scaled Sampling for Deterministic Constraints"</i></a>, In 16th International Conference on Artificial Intelligence and Statistics (AISTATS), 2013.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2013dynamic','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2013dynamic','bibtex')">[BibTeX]</a>
   <a href="pubs/li2013dynamic - Dynamic Scaled Sampling for Deterministic Constraints.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_li2013dynamic" class="abstract noshow">
	<td><b>Abstract</b>: Deterministic and near-deterministic relationships among subsets of random variables in multivariate systems are known to cause serious problems for Monte Carlo algorithms. We examine the case in which the relationship Z = f(X1,...,Xk) holds, where each Xi has a continuous prior pdf and we wish to obtain samples from the conditional distribution P(X1,...,Xk | Z = s). When f is addition, the problem is NP-hard even when the Xi are independent. In more restricted cases—for example, i.i.d. Boolean or categorical Xi—efficient exact samplers have been obtained previously. For the general continuous case, we propose a dynamic scaling algorithm (DYSC), and prove that it has O(k) expected running time and finite variance. We discuss generalizations of DYSC to functions f described by binary operation trees. We evaluate the algorithm on several examples.</td>
</tr>

<tr id="bib_li2013dynamic" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2013dynamic,
  author = {Li, Lei and Ramsundar, Bharath and Russell, Stuart},
  title = {Dynamic Scaled Sampling for Deterministic Constraints},
  booktitle = {16th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2013}
}
</pre></td>
</tr><tr id="liu2013hibernating" class="entry">
	<td>Siyuan Liu, Lei Li and Ramayya Krishnan, <a href="pubs/liu2013hibernating.pdf"><i>"Hibernating Process: Modelling Mobile Calls at Multiple Scales"</i></a>, In IEEE International Conference on Data Mining (ICDM), 2013.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('liu2013hibernating','bibtex')">[BibTeX]</a>
   <a href="pubs/liu2013hibernating - Hibernating Process_ Modelling Mobile Calls at Multiple Scales.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_liu2013hibernating" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{liu2013hibernating,
  author = {Liu, Siyuan and Li, Lei and Krishnan, Ramayya},
  title = {Hibernating Process: Modelling Mobile Calls at Multiple Scales},
  booktitle = {IEEE International Conference on Data Mining (ICDM)},
  year = {2013}
}
</pre></td>
</tr><tr id="matsubara2013f" class="entry">
	<td>Yasuko Matsubara, Lei Li, Evangelos E. Papalexakis, David Lo, Yasushi Sakurai and Christos Faloutsos, <a href="pubs/matsubara2013f.pdf"><i>"F-Trail: Finding Patterns in Taxi Trajectories"</i></a>, In The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), pp. 86-98., 2013.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('matsubara2013f','bibtex')">[BibTeX]</a>
   <a href="pubs/matsubara-The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)13-ftrail.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_matsubara2013f" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{matsubara2013f,
  author = {Matsubara, Yasuko and Li, Lei and Papalexakis, Evangelos E. and Lo, David and Sakurai, Yasushi and Faloutsos, Christos},
  title = {F-Trail: Finding Patterns in Taxi Trajectories},
  booktitle = {The Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)},
  year = {2013},
  pages = {86--98}
}
</pre></td>
</tr><tr id="rogers2013multilinear" class="entry">
	<td>Mark Rogers, Lei Li and Stuart J. Russell, <a href="pubs/rogers2013multilinear.pdf"><i>"Multilinear Dynamical Systems for Tensor Time Series"</i></a>, In the 27th Conference on Neural Information Processing Systems(NeurIPS), 2013.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('rogers2013multilinear','bibtex')">[BibTeX]</a>
   <a href="pubs/rogers2013multilinear - Multilinear Dynamical Systems for Tensor Time Series.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/mlds-r662.zip">[Software]</a>
	
	 
	</p>
	</td>
</tr>


<tr id="bib_rogers2013multilinear" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{rogers2013multilinear,
  author = {Rogers, Mark and Li, Lei and Russell, Stuart J.},
  title = {Multilinear Dynamical Systems for Tensor Time Series},
  booktitle = {the 27th Conference on Neural Information Processing Systems(NeurIPS)},
  year = {2013}
}
</pre></td>
</tr><tr id="vikram2013handwriting" class="entry">
	<td>Sharad Vikram, Lei Li and Stuart Russell, <a href="pubs/vikram2013handwriting.pdf"><i>"Handwriting and Gestures in the Air, Recognizing on the Fly"</i></a>, In ACM Conference on Human Factors in Computing Systems (CHI) Extended Abstracts, 2013.
	
	<p class="infolinks"><a href="javascript:toggleInfo('vikram2013handwriting','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('vikram2013handwriting','bibtex')">[BibTeX]</a>
   <a href="pubs/vikram2013handwriting - Handwriting and Gestures in the Air, Recognizing on the Fly.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>

<tr id="abs_vikram2013handwriting" class="abstract noshow">
	<td><b>Abstract</b>: Recent technologies in vision sensors are capable of capturing 3D finger positions and movements. We propose a novel way to control and interact with computers by moving fingers in the air. The positions of fingers are precisely captured by a computer vision device. By tracking the moving patterns of fingers, we can then recognize users’ intended control commands or input information. We demonstrate this human input approach through an example application of handwriting recognition. By treating the input as a time series of 3D positions, we propose a fast algorithm using dynamic time warping to recognize characters in online fashion. We employ various optimization techniques to recognize in real time as one writes. Experiments show promising recognition performance and speed.</td>
</tr>

<tr id="bib_vikram2013handwriting" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{vikram2013handwriting,
  author = {Vikram, Sharad and Li, Lei and Russell, Stuart},
  title = {Handwriting and Gestures in the Air, Recognizing on the Fly},
  booktitle = {ACM Conference on Human Factors in Computing Systems (CHI) Extended Abstracts},
  year = {2013}
}
</pre></td>
</tr><tr id="li2013blog" class="entry">
	<td>Lei Li and Stuart J. Russell, <a href="pubs/li2013blog.pdf"><i>"The BLOG Language Reference"</i></a>. EECS Department, University of California, BerkeleyEECS Department, University of California, Berkeley, Technical Report UCB/EECS-2013-51, May, 2013.
	<p class="infolinks"><a href="javascript:toggleInfo('li2013blog','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2013blog','bibtex')">[BibTeX]</a>
   <a href="pubs/li2013blog - The BLOG Language Reference.pdf">&#91;PDF&#93;</a>
  
  
  
  
    <a href="http:/www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-51.html">&#91;Tech report&#93;</a> 
	</td>
</tr>
<tr id="abs_li2013blog" class="abstract noshow">
	<td><b>Abstract</b>: This document introduces the syntax of BLOG, a probabilistic programming language, for describing random variables and their probabilistic dependencies. BLOG defines probabilistic generative models over first-order structures. For example, all Bayesian networks can be easily described by BLOG. BLOG has the following features: (a) it employs open-universe semantics; (b) it can describe relational uncertainty; (c) it can handle identity uncertainty; and (d) it is empowered by first-order logic. The syntax as described in this document corresponds to BLOG version 0.6. The current version represents a significant redesign and extension to previous versions of BLOG, based on the principles of usability and implementation efficiency.</td>
</tr>
<tr id="bib_li2013blog" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@techreport{li2013blog,
  author = {Li, Lei and Russell, Stuart J.},
  title = {The BLOG Language Reference},
  school = {EECS Department, University of California, Berkeley},
  year = {2013},
  number = {UCB/EECS-2013-51}
}
</pre></td>
</tr>
<tr id="henderson2012rolx" class="entry">
	<td>Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos and Lei Li, <a href="pubs/henderson2012rolx.pdf"><i>"RolX: Structural Role Extraction and Mining in Large Graphs"</i></a>, In Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2012.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('henderson2012rolx','bibtex')">[BibTeX]</a>
   <a href="pubs/henderson2012rolx - RolX_ Structural Role Extraction and Mining in Large Graphs.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_henderson2012rolx" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{henderson2012rolx,
  author = {Henderson, Keith and Gallagher, Brian and Eliassi-Rad, Tina and Tong, Hanghang and Basu, Sugato and Akoglu, Leman and Koutra, Danai and Faloutsos, Christos and Li, Lei},
  title = {RolX: Structural Role Extraction and Mining in Large Graphs},
  booktitle = {Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2012}
}
</pre></td>
</tr><tr id="matsubara2012rise" class="entry">
	<td>Yasuko Matsubara, Yasushi Sakurai, B. Aditya Prakash, Lei Li and Christos Faloutsos, <a href="pubs/matsubara2012rise.pdf"><i>"Rise and Fall Patterns of Information Diffusion: Model and Implications"</i></a>, In Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2012.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('matsubara2012rise','bibtex')">[BibTeX]</a>
   <a href="pubs/matsubara2012rise - Rise and Fall Patterns of Information Diffusion_ Model and Implications.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_matsubara2012rise" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{matsubara2012rise,
  author = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
  title = {Rise and Fall Patterns of Information Diffusion: Model and Implications},
  booktitle = {Proceeding of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2012}
}
</pre></td>
</tr><tr id="henderson2011its" class="entry">
	<td>Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong and Christos Faloutsos, <a href="pubs/henderson2011its.pdf"><i>"It's Who You Know: Graph Mining Using Recursive Structural Features"</i></a>, In Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2011.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('henderson2011its','bibtex')">[BibTeX]</a>
   <a href="pubs/henderson2011its - It's Who You Know_ Graph Mining Using Recursive Structural Features.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_henderson2011its" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{henderson2011its,
  author = {Henderson, Keith and Gallagher, Brian and Li, Lei and Akoglu, Leman and Eliassi-Rad, Tina and Tong, Hanghang and Faloutsos, Christos},
  title = {It's Who You Know: Graph Mining Using Recursive Structural Features},
  booktitle = {Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2011}
}
</pre></td>
</tr><tr id="li2011fast" class="entry">
	<td>Lei Li, <a href="pubs/li2011fast.pdf"><i>"Fast algorithms for mining co-evolving time series"</i></a>
	. <i>Ph.D. Dissertation</i>, Carnegie Mellon University.
	, Available as technical report CMU-CS-11-127. 
  , 2011.
	<p class="infolinks"> <a href="javascript:toggleInfo('li2011fast','bibtex')">[BibTeX]</a>
  
  
  
  
   <a href="pubs/leili-talk2011-defense.pdf">[PPT]</a>
  
	</p>
	</td>
</tr>
<tr id="bib_li2011fast" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@phdthesis{li2011fast,
  author = {Li, Lei},
  title = {Fast algorithms for mining co-evolving time series},
  school = {Carnegie Mellon University},
  year = {2011}
}
</pre></td>
</tr>
<tr id="li2011thermocast" class="entry">
	<td>Lei Li, Chieh-Jan Mike Liang, Jie Liu, Suman Nath, Andreas Terzis and Christos Faloutsos, <a href="pubs/li2011thermocast.pdf"><i>"ThermoCast: A Cyber-Physical Forecasting Model for Data Centers"</i></a>, In Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), New York, NY, USA ACM, 2011.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2011thermocast','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2011thermocast','bibtex')">[BibTeX]</a>
   <a href="pubs/li2011thermocast - ThermoCast_ A Cyber-Physical Forecasting Model for Data Centers.pdf">&#91;PDF&#93;</a>
  
  
	
	 <a href="pubs/li2011thermocast-poster.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>

<tr id="abs_li2011thermocast" class="abstract noshow">
	<td><b>Abstract</b>: Efficient thermal management is important in modern data centers as cooling consumes up to 50% of the total energy. Unlike previous work, we consider proactive thermal management, whereby servers can predict potential overheating events due to dynamics in data center configuration and workload, giving operators enough time to react. However, such forecasting is very challenging due to data center scales and complexity. Moreover, such a physical system is influenced by cyber effects, including workload scheduling in servers. We propose ThermoCast, a novel thermal forecasting model to predict the temperatures surrounding the servers in a data center, based on continuous streams of temperature and airflow measurements. Our approach is (a) capable of capturing cyber- physical interactions and automatically learning them from data; (b) computationally and physically scalable to data center scales; (c) able to provide online prediction with real-time sensor mea- surements. The paper’s main contributions are: (i) We provide a systematic approach to integrate physical laws and sensor observa- tions in a data center; (ii) We provide an algorithm that uses sensor data to learn the parameters of a data center’s cyber-physical sys- tem. In turn, this ability enables us to reduce model complexity compared to full-fledged fluid dynamics models, while maintain- ing forecast accuracy; (iii) Unlike previous simulation-based stud- ies, we perform experiments in a production data center. Using real data traces, we show that ThermoCast forecasts temperature 2× better than a machine learning approach solely driven by data, and can successfully predict thermal alarms 4.2 minutes ahead of time.</td>
</tr>

<tr id="bib_li2011thermocast" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2011thermocast,
  author = {Li, Lei and Liang, Chieh-Jan Mike and Liu, Jie and Nath, Suman and Terzis, Andreas and Faloutsos, Christos},
  title = {ThermoCast: A Cyber-Physical Forecasting Model for Data Centers},
  booktitle = {Proceeding of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  publisher = {ACM},
  year = {2011}
}
</pre></td>
</tr><tr id="li2011time" class="entry">
	<td>Lei Li and B. Aditya Prakash, <a href="pubs/li2011time.pdf"><i>"Time Series Clustering: Complex is Simpler!"</i></a>, In Proceedings of the 28th International Conference on Machine Learning (ICML), Bellevue, Washington, 2011.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2011time','bibtex')">[BibTeX]</a>
   <a href="pubs/li2011time - Time Series Clustering_ Complex is Simpler!.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/clds-r347.zip">[Software]</a>
	 <a href="pubs/li2011time-slides.pdf">[PPT]</a>
	  <a href="software/li2011time - Time Series Clustering_ Complex is Simpler!.zip">&#91;Dataset1-mocap16&#93;</a>  <a href="software/mocap35-rfoot_amc.zip">&#91;Dataset2-mocap35&#93;</a> 
	</p>
	</td>
</tr>


<tr id="bib_li2011time" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2011time,
  author = {Li, Lei and Prakash, B. Aditya},
  title = {Time Series Clustering: Complex is Simpler!},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML)},
  year = {2011}
}
</pre></td>
</tr><tr id="liu2011mobile" class="entry">
	<td>Siyuan Liu, Lei Li, Christos Faloutsos and Lionel Ni, <a href="pubs/liu2011mobile.pdf"><i>"Mobile Phone Graph Evolution: Findings, Model and Interpretation"</i></a>, In IEEE International Conference on Data Mining, workshop on Data Mining Technologies for Computational Collective Intelligence, 2011.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('liu2011mobile','bibtex')">[BibTeX]</a>
   <a href="pubs/liu2011mobile - Mobile Phone Graph Evolution_ Findings, Model and Interpretation.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_liu2011mobile" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{liu2011mobile,
  author = {Liu, Siyuan and Li, Lei and Faloutsos, Christos and Ni, Lionel},
  title = {Mobile Phone Graph Evolution: Findings, Model and Interpretation},
  booktitle = {IEEE International Conference on Data Mining, workshop on Data Mining Technologies for Computational Collective Intelligence},
  year = {2011}
}
</pre></td>
</tr><tr id="sakurai2011windmine" class="entry">
	<td>Yasushi Sakurai, Lei Li, Yasuko Matsubara and Christos Faloutsos, <a href="pubs/sakurai2011windmine.pdf"><i>"WindMine: Fast and Effective Mining of Web-click Sequences"</i></a>, In SIAM International Conference on Data Mining (SDM), 2011.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sakurai2011windmine','bibtex')">[BibTeX]</a>
   <a href="pubs/sakurai2011windmine - WindMine_ Fast and Effective Mining of Web-click Sequences.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_sakurai2011windmine" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sakurai2011windmine,
  author = {Sakurai, Yasushi and Li, Lei and Matsubara, Yasuko and Faloutsos, Christos},
  title = {WindMine: Fast and Effective Mining of Web-click Sequences},
  booktitle = {SIAM International Conference on Data Mining (SDM)},
  year = {2011}
}
</pre></td>
</tr><tr id="henderson2010metric" class="entry">
	<td>Keith Henderson, Tina Eliassi-Rad, Christos Faloutsos, Leman Akoglu, Lei Li, Koji Maruhashi, B. Aditya Prakash and Hanghang Tong, <a href="pubs/henderson2010metric.pdf"><i>"Metric forensics: a multi-level approach for mining volatile graphs"</i></a>, In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), New York, NY, USA, pp. 163-172. ACM, 2010.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('henderson2010metric','bibtex')">[BibTeX]</a>
   <a href="pubs/henderson2010metric - Metric forensics_ a multi-level approach for mining volatile graphs.pdf">&#91;PDF&#93;</a>
   <a href="https://doi.org/10.1145/1835804.1835828">[DOI]</a>
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_henderson2010metric" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{henderson2010metric,
  author = {Henderson, Keith and Eliassi-Rad, Tina and Faloutsos, Christos and Akoglu, Leman and Li, Lei and Maruhashi, Koji and Prakash, B. Aditya and Tong, Hanghang},
  title = {Metric forensics: a multi-level approach for mining volatile graphs},
  booktitle = {Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  publisher = {ACM},
  year = {2010},
  pages = {163--172},
  doi = {https://doi.org/10.1145/1835804.1835828}
}
</pre></td>
</tr><tr id="li2010fast" class="entry">
	<td>Lei Li, <a href="pubs/li2010fast.pdf"><i>"Fast Algorithms for Time Series Mining"</i></a>, In 26th IEEE International Conference on Data Engineering, PHD Workshop, pp. 341-344., 2010.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2010fast','bibtex')">[BibTeX]</a>
   <a href="pubs/li2010fast - Fast Algorithms for Time Series Mining.pdf">&#91;PDF&#93;</a>
  
  
	
	 <a href="pubs/li-icde10-slides.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>


<tr id="bib_li2010fast" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2010fast,
  author = {Li, Lei},
  title = {Fast Algorithms for Time Series Mining},
  booktitle = {26th IEEE International Conference on Data Engineering, PHD Workshop},
  year = {2010},
  pages = {341--344}
}
</pre></td>
</tr><tr id="li2010efficient" class="entry">
	<td>Lei Li, Bin Fu and Christos Faloutsos, <a href="pubs/li2010efficient.pdf"><i>"Efficient Parallel Learning of Hidden Markov chain Models on SMPs"</i></a>, IEICE Transactions on Information and Systems, Volume E93.D(6), pp. 1330-1342., 2010.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2010efficient','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2010efficient','bibtex')">[BibTeX]</a>
   <a href="pubs/li2010efficient - Efficient Parallel Learning of Hidden Markov Chain Models on SMPs.pdf">&#91;PDF&#93;</a>
  
  
  
  
   
	</p>
	</td>
</tr>

<tr id="abs_li2010efficient" class="abstract noshow">
	<td><b>Abstract</b>: Quad-core cpus have been a common desktop configuration for today’s office. The increasing number of processors on a single chip opens new opportunity for parallel computing. Our goal is to make use of the multi-core as well as multi-processor architectures to speed up large-scale data mining algorithms. In this paper, we present a general par- allel learning framework, Cut-And-Stitch, for training hidden Markov chain models. Particularly, we propose two model-specific variants, CAS-LDS for learning linear dynamical systems (LDS) and CAS-HMM for learning hidden Markov models (HMM). Our main contribution is a novel method to handle the data dependencies due to the chain structure of hidden variables, so as to parallelize the EM-based parameter learning algorithm. We imple- ment CAS-LDS and CAS-HMM using OpenMP on two supercomputers and a quad-core commercial desktop. The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the traditional serial version.</td>
</tr>

<tr id="bib_li2010efficient" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@article{li2010efficient,
  author = {Li, Lei and Fu, Bin and Faloutsos, Christos},
  title = {Efficient Parallel Learning of Hidden Markov chain Models on SMPs},
  journal = {IEICE Transactions on Information and Systems},
  year = {2010},
  volume = {E93.D},
  number = {6},
  pages = {1330--1342}
}
</pre></td>
</tr>
<tr id="li2010bolero" class="entry">
	<td>Lei Li, James McCann, Nancy Pollard and Christos Faloutsos, <a href="pubs/li2010bolero.pdf"><i>"BoLeRO: a principled technique for including bone length constraints in motion capture occlusion filling"</i></a>, In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA), Aire-la-Ville, Switzerland, Switzerland, pp. 179-188. Eurographics Association, 2010.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2010bolero','bibtex')">[BibTeX]</a>
   <a href="pubs/li2010bolero - BoLeRO_ a principled technique for including bone length constraints in motion capture occlusion filling.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/bolero-r349.zip">[Software]</a>
	
	  <a href="pubs/BoLeRO-final-v1_xvid.avi">&#91;demo&#93;</a> 
	</p>
	</td>
</tr>


<tr id="bib_li2010bolero" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2010bolero,
  author = {Li, Lei and McCann, James and Pollard, Nancy and Faloutsos, Christos},
  title = {BoLeRO: a principled technique for including bone length constraints in motion capture occlusion filling},
  booktitle = {Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)},
  publisher = {Eurographics Association},
  year = {2010},
  pages = {179--188}
}
</pre></td>
</tr><tr id="li2010parsimonious" class="entry">
	<td>Lei Li, B. Aditya Prakash and Christos Faloutsos, <a href="pubs/li2010parsimonious.pdf"><i>"Parsimonious linear fingerprinting for time series"</i></a>, In the Proceedings of the Very Large Data Bases Endowment (VLDB), Volume 3, pp. 385-396. VLDB Endowment, 2010.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2010parsimonious','bibtex')">[BibTeX]</a>
   <a href="pubs/li2010parsimonious - Parsimonious linear fingerprinting for time series.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/plif-r345.zip">[Software]</a>
	 <a href="pubs/li-vldb10-plif-slides.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>


<tr id="bib_li2010parsimonious" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2010parsimonious,
  author = {Li, Lei and Prakash, B. Aditya and Faloutsos, Christos},
  title = {Parsimonious linear fingerprinting for time series},
  booktitle = {the Proceedings of the Very Large Data Bases Endowment (VLDB)},
  publisher = {VLDB Endowment},
  year = {2010},
  volume = {3},
  pages = {385--396}
}
</pre></td>
</tr><tr id="guo2009tailoring" class="entry">
	<td>Fan Guo, Lei Li and Christos Faloutsos, <a href="pubs/guo2009tailoring.pdf"><i>"Tailoring click models to user goals"</i></a>, In Proceedings of the 2009 workshop on Web Search Click Data, New York, NY, USA, pp. 88-92. ACM, 2009.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('guo2009tailoring','bibtex')">[BibTeX]</a>
  
   <a href="https://doi.org/10.1145/1507509.1507523">[DOI]</a>
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_guo2009tailoring" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{guo2009tailoring,
  author = {Guo, Fan and Li, Lei and Faloutsos, Christos},
  title = {Tailoring click models to user goals},
  booktitle = {Proceedings of the 2009 workshop on Web Search Click Data},
  publisher = {ACM},
  year = {2009},
  pages = {88--92},
  doi = {https://doi.org/10.1145/1507509.1507523}
}
</pre></td>
</tr><tr id="li2009dynammo" class="entry">
	<td>Lei Li, James McCann, Nancy Pollard and Christos Faloutsos, <a href="pubs/li2009dynammo.pdf"><i>"DynaMMo: Mining and Summarization of Coevolving Sequences with Missing Values"</i></a>, In Proceeding of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), New York, NY, USA ACM, 2009.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2009dynammo','bibtex')">[BibTeX]</a>
   <a href="pubs/li2009dynammo - DynaMMo_ Mining and Summarization of Coevolving Sequences with Missing Values.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/dynammo-r346.zip">[Software]</a>
	 <a href="pubs/li-kdd09-dynammo-slides.pdf">[PPT]</a>
	  <a href="https:/github.com/lileicc/dynammo">&#91;Source repo&#93;</a> 
	</p>
	</td>
</tr>


<tr id="bib_li2009dynammo" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2009dynammo,
  author = {Li, Lei and McCann, James and Pollard, Nancy and Faloutsos, Christos},
  title = {DynaMMo: Mining and Summarization of Coevolving Sequences with Missing Values},
  booktitle = {Proceeding of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  publisher = {ACM},
  year = {2009}
}
</pre></td>
</tr><tr id="chen2009adaptive" class="entry">
	<td>Zheng Chen, Lei Li, Chenxi Lin, Qiaoling Liu, Jian Wang and Benyu Zhang, <a href="pubs/chen2009adaptive.pdf"><i>"Adaptive grouping in a file network"</i></a>(US 7,634,471), 2009.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2009adaptive','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2009adaptive" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2009adaptive,
  author = {Chen, Zheng and Li, Lei and Lin, Chenxi and Liu, Qiaoling and Wang, Jian and Zhang, Benyu},
  title = {Adaptive grouping in a file network},
  year = {2009},
  number = {US 7,634,471}
}
</pre></td>
</tr>
<tr id="chen2009system" class="entry">
	<td>Zheng Chen, Lei Li, Chenxi Lin, Qiaoling Liu, Jian Wang and Benyu Zhang, <a href="pubs/chen2009system.pdf"><i>"System and method for exploring a semantic file network"</i></a>(US 7,624,130), 2009.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2009system','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2009system" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2009system,
  author = {Chen, Zheng and Li, Lei and Lin, Chenxi and Liu, Qiaoling and Wang, Jian and Zhang, Benyu},
  title = {System and method for exploring a semantic file network},
  year = {2009},
  number = {US 7,624,130}
}
</pre></td>
</tr>
<tr id="chen2009extracting" class="entry">
	<td>Zheng Chen, Lei Li, Chenxi Lin, Qiaoling Liu, Jian Wang and Benyu Zhang, <a href="pubs/chen2009extracting.pdf"><i>"Extracting Semantic Attributes"</i></a>(US 7,502,785), 2009.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('chen2009extracting','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_chen2009extracting" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@patent{chen2009extracting,
  author = {Chen, Zheng and Li, Lei and Lin, Chenxi and Liu, Qiaoling and Wang, Jian and Zhang, Benyu},
  title = {Extracting Semantic Attributes},
  year = {2009},
  number = {US 7,502,785}
}
</pre></td>
</tr>
<tr id="li2008cut" class="entry">
	<td>Lei Li, Wenjie Fu, Fan Guo, Todd C. Mowry and Christos Faloutsos, <a href="pubs/li2008cut.pdf"><i>"Cut-and-Stitch: efficient parallel learning of linear dynamical systems on smps"</i></a>, In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), New York, NY, USA, pp. 471-479. ACM, 2008.
	
	<p class="infolinks"><a href="javascript:toggleInfo('li2008cut','abstract')">[Abstract]</a> <a href="javascript:toggleInfo('li2008cut','bibtex')">[BibTeX]</a>
   <a href="pubs/li2008cut - Cut-and-stitch_ efficient parallel learning of linear dynamical systems on smps.pdf">&#91;PDF&#93;</a>
  
  
	 <a href="software/paralearn.0.1.zip">[Software]</a>
	 <a href="pub/li-2008-cut-slides.pdf">[PPT]</a>
	 
	</p>
	</td>
</tr>

<tr id="abs_li2008cut" class="abstract noshow">
	<td><b>Abstract</b>: Multi-core processors with ever increasing number of cores per chip are becoming prevalent in modern parallel computing. Our goal is to make use of the multi-core as well as multi-processor architectures to speed up data mining algorithms. Specifically, we present a parallel algorithm for approximate learning of Linear Dynamical Systems (LDS), also known as Kalman Filters (KF). LDSs are widely used in time series analysis such as motion capture modeling and visual tracking etc. We propose Cut-And-Stitch (CAS), a novel method to handle the data dependencies due to the chain structure of hidden variables in LDS, so as to parallelize the EM- based parameter learning algorithm. We implement the algorithm using OpenMP on both a supercomputer and a quad-core commercial desktop. The experimental results show that parallel algorithms using Cut-And-Stitch achieve comparable accuracy and almost linear speedups over the serial version. In addition, Cut-And-Stitch can be generalized to other models with similar linear structures such as Hidden Markov Models (HMM) and Switching Kalman Filters (SKF).</td>
</tr>

<tr id="bib_li2008cut" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2008cut,
  author = {Li, Lei and Fu, Wenjie and Guo, Fan and Mowry, Todd C. and Faloutsos, Christos},
  title = {Cut-and-Stitch: efficient parallel learning of linear dynamical systems on smps},
  booktitle = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD)},
  publisher = {ACM},
  year = {2008},
  pages = {471--479}
}
</pre></td>
</tr><tr id="li2008laziness" class="entry">
	<td>Lei Li, James McCann, Christos Faloutsos and Nancy Pollard, <a href="pubs/li2008laziness.pdf"><i>"Laziness is a virtue: Motion stitching using effort minimization"</i></a>, In The 29th Annual Conference of the European Association for Computer Graphics (EG), Short Paper Proceedings, 2008.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2008laziness','bibtex')">[BibTeX]</a>
   <a href="pubs/li2008laziness - Laziness is a virtue_ Motion stitching using effort minimization.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_li2008laziness" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2008laziness,
  author = {Li, Lei and McCann, James and Faloutsos, Christos and Pollard, Nancy},
  title = {Laziness is a virtue: Motion stitching using effort minimization},
  booktitle = {The 29th Annual Conference of the European Association for Computer Graphics (EG), Short Paper Proceedings},
  year = {2008}
}
</pre></td>
</tr><tr id="sakurai2008efficient" class="entry">
	<td>Yasushi Sakurai, Rosalynn Chong, Lei Li and Christos Faloutsos, <a href="pubs/sakurai2008efficient.pdf"><i>"Efficient Distribution Mining and Classification"</i></a>, In SIAM International Conference on Data Mining (SDM), pp. 632-643., 2008.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sakurai2008efficient','bibtex')">[BibTeX]</a>
   
  
  
	
	
	  <a href="http:/www.siam.org/proceedings/datamining/2008/dm08_58_sakurai.pdf">&#91;&#93;</a> 
	</p>
	</td>
</tr>


<tr id="bib_sakurai2008efficient" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{sakurai2008efficient,
  author = {Sakurai, Yasushi and Chong, Rosalynn and Li, Lei and Faloutsos, Christos},
  title = {Efficient Distribution Mining and Classification},
  booktitle = {SIAM International Conference on Data Mining (SDM)},
  year = {2008},
  pages = {632--643}
}
</pre></td>
</tr><tr id="xu2008inferring" class="entry">
	<td>Wanhong Xu, Xi Zhou and Lei Li, <a href="pubs/xu2008inferring.pdf"><i>"Inferring privacy information via social relations"</i></a>, In IEEE 24th International Conference on Data Engineering workshops, pp. 525-530., 2008.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('xu2008inferring','bibtex')">[BibTeX]</a>
  
   <a href="https://doi.org/10.1109/ICDEW.2008.4498373">[DOI]</a>
  
	
	
	
	</p>
	</td>
</tr>


<tr id="bib_xu2008inferring" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{xu2008inferring,
  author = {Xu, Wanhong and Zhou, Xi and Li, Lei},
  title = {Inferring privacy information via social relations},
  booktitle = {IEEE 24th International Conference on Data Engineering workshops},
  year = {2008},
  pages = {525--530},
  doi = {https://doi.org/10.1109/ICDEW.2008.4498373}
}
</pre></td>
</tr><tr id="guo2008c" class="entry">
	<td>Fan Guo, Lei Li, Christos Faloutsos and Eric P. Xing, <a href="pubs/guo2008c.pdf"><i>"C-DEM: a Multi-modal Query System for Drosophila Embryo Databases"</i></a>, In the Proceedings of the Very Large Data Bases Endowment (VLDB), Volume 1, pp. 1508-1511. VLDB Endowment, 2008.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('guo2008c','bibtex')">[BibTeX]</a>
   <a href="pubs/guo2008c - C-DEM_ a multi-modal query system for Drosophila Embryo databases.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_guo2008c" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{guo2008c,
  author = {Guo, Fan and Li, Lei and Faloutsos, Christos and Xing, Eric P.},
  title = {C-DEM: a Multi-modal Query System for Drosophila Embryo Databases},
  booktitle = {the Proceedings of the Very Large Data Bases Endowment (VLDB)},
  publisher = {VLDB Endowment},
  year = {2008},
  volume = {1},
  pages = {1508--1511}
}
</pre></td>
</tr><tr id="li2006providing" class="entry">
	<td>Lei Li, Qiaoling Liu, Yunfeng Tao, Lei Zhang, Jian Zhou and Yong Yu, <a href="pubs/li2006providing.pdf"><i>"Providing an Uncertainty Reasoning Service for Semantic Web Application"</i></a>, In Asia-Pacific Web Conference, pp. 628-639., 2006.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('li2006providing','bibtex')">[BibTeX]</a>
   <a href="pubs/li2006providing - Providing an Uncertainty Reasoning Service for Semantic Web Application.pdf">&#91;PDF&#93;</a>
  
  
	
	
	 
	</p>
	</td>
</tr>


<tr id="bib_li2006providing" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{li2006providing,
  author = {Li, Lei and Liu, Qiaoling and Tao, Yunfeng and Zhang, Lei and Zhou, Jian and Yu, Yong},
  title = {Providing an Uncertainty Reasoning Service for Semantic Web Application},
  booktitle = {Asia-Pacific Web Conference},
  year = {2006},
  pages = {628--639}
}
</pre></td>
</tr><tr id="baiDraftself" class="entry">
	<td>Hongxiao Bai, Mingxuan Wang, Hai Zhao and Lei Li, <a href="pubs/baiDraftself.pdf"><i>"Self-Training with Heterogeneous Teachers Improves Neural Machine Translation"</i></a>, Draft.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('baiDraftself','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_baiDraftself" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{baiDraftself,
  author = {Hongxiao Bai and Mingxuan Wang and Hai Zhao and Lei Li},
  title = {Self-Training with Heterogeneous Teachers Improves Neural Machine Translation},
  year = {Draft}
}
</pre></td>
</tr>
<tr id="baoDraftpnat" class="entry">
	<td>Yu Bao, Hao Zhou, Jiangtao Feng, Mingxuan Wang, Shujian Huang, Jiajun Chen and Lei Li, <a href="pubs/baoDraftpnat.pdf"><i>"PNAT: Non-autoregressive Transformer by Position Learning"</i></a>, Draft.
	<p class="note">in submission</p>
	<p class="infolinks"> <a href="javascript:toggleInfo('baoDraftpnat','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_baoDraftpnat" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{baoDraftpnat,
  author = {Yu Bao and Hao Zhou and Jiangtao Feng and Mingxuan Wang and Shujian Huang and Jiajun Chen and Lei Li},
  title = {PNAT: Non-autoregressive Transformer by Position Learning},
  year = {Draft},
  note = {in submission}
}
</pre></td>
</tr>
<tr id="liDraftbidirectional" class="entry">
	<td>Mingwei Li, Qingyuan Jiang, Yi He, Lei Li and Wujun Li, <a href="pubs/liDraftbidirectional.pdf"><i>"Bidirectional Attentive Convolutional Neural Network for Near-Duplicate Video Retrieval"</i></a>, Draft.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('liDraftbidirectional','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_liDraftbidirectional" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{liDraftbidirectional,
  author = {Mingwei Li and Qingyuan Jiang and Yi He and Lei Li and Wujun Li},
  title = {Bidirectional Attentive Convolutional Neural Network for Near-Duplicate Video Retrieval},
  year = {Draft}
}
</pre></td>
</tr>
<tr id="sunDraftdiving" class="entry">
	<td>Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen and Lei Li, <a href="pubs/sunDraftdiving.pdf"><i>"Diving into Document-Level Neural Machine Translation"</i></a>, Draft.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('sunDraftdiving','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_sunDraftdiving" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{sunDraftdiving,
  author = {Zewei Sun and Mingxuan Wang and Hao Zhou and Chengqi Zhao and Shujian Huang and Jiajun Chen and Lei Li},
  title = {Diving into Document-Level Neural Machine Translation},
  year = {Draft}
}
</pre></td>
</tr>
<tr id="tianDraftconversational" class="entry">
	<td>Youzhi Tian, Zhou Yu, Cheng Yang, Hang Li and Lei Li, <a href="pubs/tianDraftconversational.pdf"><i>"Conversational Contextualized Multimodal Representation Learning"</i></a>, Draft.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('tianDraftconversational','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_tianDraftconversational" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{tianDraftconversational,
  author = {Youzhi Tian and Zhou Yu and Cheng Yang and Hang Li and Lei Li},
  title = {Conversational Contextualized Multimodal Representation Learning},
  year = {Draft}
}
</pre></td>
</tr>
<tr id="yanDraftcross" class="entry">
	<td>An Yan, Xin Wang, Jiangtao Feng, Lei Li and William Yang Wang, <a href="pubs/yanDraftcross.pdf"><i>"Cross-Lingual Vision-Language Navigation"</i></a>, Draft.
	
	<p class="infolinks"> <a href="javascript:toggleInfo('yanDraftcross','bibtex')">[BibTeX]</a>
  
  
  
  
  
  
	</p>
	</td>
</tr>


<tr id="bib_yanDraftcross" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@unpublished{yanDraftcross,
  author = {An Yan and Xin Wang and Jiangtao Feng and Lei Li and William Yang Wang},
  title = {Cross-Lingual Vision-Language Navigation},
  year = {Draft}
}
</pre></td>
</tr>
</tbody>
</table>
<p>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 2021/09/11.</small> Return to <a href="./">Home</a>
</p>
<!-- File generated by JabRef ; Export Filter written by Mark Schenk, (C) 2008, Modified by Lei Li (C) 2011,2017 -->
</body>
</html>