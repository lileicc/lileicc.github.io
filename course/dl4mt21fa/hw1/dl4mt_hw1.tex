\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{UCSB CMPSC 291K}
\newcommand\hwnumber{1}                  % <-- homework number
%\newcommand\name{Lei Li}                % <-- Name of person #1

\pagestyle{fancyplain}
\headheight 35pt
\lhead{Instructor: Lei Li}
\chead{\textbf{\Large DL4MT Homework \hwnumber}}
\rhead{\course \\ September 27, 2021}%\today}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}

\section*{Sign-up for Language-in-10-mins Survey and Presentation}
Please sign up at \url{https://tinyurl.com/4m8yjkuv} (link to google doc). You need UCSB account to sign-in and edit.
Spread evenly across the dates and avoid conflict on language-of-choice (no duplicate language). 
Please fill by Tuesday 9/28. The first presentation session is to start Monday 10/4.  

\section*{Problem 1: Probability Basics (10')} 

We are playing dart. There are two darts in a box. We are told that one dart is more accurate than the other. 
The accurate one will shoot on the target board with a Normal distribution $N(8, 1)$, while the defective one will land on the target with a Normal distribution $N(4, 2)$.
We would like to pick the accurate one, but we can not distinguish one from the other. They look identical. 
Unfortunately we randomly pick one and fire it towards the target. The location turns out to be $x$ ($0 \leq x \leq 10.9$).



\begin{enumerate}
  \item $x=6.0$, what is the probability of being accurate?

  \item What is the probability of the picked dart being the accurate one? (in terms of x)

\end{enumerate}



\section*{Problem 2: Logistic Regression (25')} 
Consider a single layer neural network, which takes a real vector input x, and output a class label in $K$ categories. 
\[f(x) = \mathrm{Softmax}(w\cdot x + b)
    \]
Given data $D=\{(x_1,y_1), \dots, (x_N, y_N)\}$, please derive the following
\begin{enumerate}
    \item What is a proper loss function and its empirical risk?
    \item We adopt the SGD method for learning the parameter estimation, please derive the gradient and update rule.
    \item Please write down the update rule for AdaGrad optimization method.
    \item What if the dimension of x is extremely large (e.g. one million), while we only have a few samples (e.g. 10 thousand) to train?
    \item Now we want to use the Newton's method to learn the parameters, please derive the update rule.
\end{enumerate}    


\section*{Problem 3: BPE Tokenization (35')} 
You are going to implment Byte-Pair-Encoding tokenization method in python.  
The program takes two commandline arguments: the first is an integer S after option \texttt{-s} to indicate vocabulary size. 
The second is a filename while contains a raw corpus after option \texttt{-i}. The file is in unicode format.  
The BPE vocabulary should be output to a file named \texttt{output.txt}, each line of the file should contain exactly one token (no extra space).
The tokens should be sorted according to alphbetical order (or unicode ascending order). 
The program should be named as \texttt{bpe\_tokenize.py}.
You may use basic python package (for example the \texttt{argparse} library for parsing commandline arguments), but not external third-party library.

\paragraph{Sample Commandline:}
\begin{verbatim}
  python bpe_tokenize.py -s 30 -i input.txt
\end{verbatim}

\paragraph{input.txt}
\begin{verbatim}
  Lettuce is a rich source of vitamin K and vitamin A .
\end{verbatim}

\paragraph{output.txt}
\begin{verbatim}
  .</w>
  A</w>
  K</w>
  L
  a
  a</w>
  c
  ce</w>
  d</w>
  e
  e</w>
  f</w>
  h</w>
  i
  m
  n
  n</w>
  o
  r
  s
  s</w>
  t
  u
  v
  vi
  vit
  vita
  vitam
  vitami
  vitamin</w>
\end{verbatim}

\section*{Problem 4: Skewed BLEU (30')} 
You are going to implement a program in Python to calculate the following skewed BLEU for a language.

\[\mathrm{BLEU}_{skew} = \min(1, e^{2(1-r/c)}) \cdot p_1^\frac{1}{6} \cdot p_2^\frac{1}{3} \cdot p_3^\frac{1}{6} \cdot p_4^\frac{1}{3}\]
Where $p_n$ is precision of n-gram, defined as 
\[p_n=\frac{num.of.correct.token.ngram}{total.output.ngram}
    \]
Your program should take two commandline arguments which specifies the input filenames for reference (after option \texttt{-r}) and candidate sentences (after option \texttt{-c}), and output a single numerical number to a file named \texttt{output.txt}.
Each line of the reference and prediction files contains corresponding sentences of reference and predictions. 
Both files will contain the same number of  lines.
The file is in Unicode encoding (utf-8). The tokens are already seperated by space characters. Notice that the input language is not necessarily English.
Your program should be named as \texttt{skewed\_bleu.py}.
You may use basic python and numpy package, but may not use pytorch or tensorflow or other external third-party package.
The results are checked up to three significant digits. 

\paragraph{Sample Commandline:}
\begin{verbatim}
  python skewed_bleu.py -r ref.txt -c cand.txt
\end{verbatim}

\paragraph{ref.txt}
\begin{verbatim}
  A SpaceX rocket was launched into a space orbit Wednesday evening .
\end{verbatim}

\paragraph{cand.txt}
\begin{verbatim}
  SpaceX launched a mission Wednesday evening into a space orbit .
\end{verbatim}

\paragraph{output.txt}
\begin{verbatim}
  0.2312
\end{verbatim}


\end{document}