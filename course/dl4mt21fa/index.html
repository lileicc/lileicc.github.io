<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" type="text/css" href="../../style/origo.css"
      media="all">
    <meta name="author" content="Lei Li">
    <title>DL4MT</title>
  </head>
  <body>
    <h1 align="center">&nbsp;291K Deep Learning for Machine Translation
      (Fall 2021)<br>
    </h1>
    <h2>Course Description </h2>
    This course will cover deep learning methods for neural machine
    translation and text generation. It will cover neural models for
    discrete sequences including Long-short term memory, Transformer,
    and encoder-decoder frameworks for generating language. It includes
    training objectives and optimization algorithms for learning those
    models. It will also include strategy such as back-translation,
    knowledge distillation. It will cover multilingual machine
    translation, low-resource translation, and speech translation, as
    well as engineering techniques to speed-up computation for MT
    models. <br>
    <h2>Instructor</h2>
    <p><a href="https://www.cs.ucsb.edu/~lilei">Lei
        Li</a><br>
    </p>
    <h2>Time and Location</h2>
    <p>Monday/Wednesday 11:00am-12:50pm&nbsp; Phelps 3526 (also on zoom)<br>
    </p>
    <h2>Office hour: <br>
    </h2>
    Via zoom with instructor by appointment.<br>
    <h2>Textbook</h2>
    <ul>
      <li>
        <meta charset="utf-8">
        (Optional) Neural Machine Translation,&nbsp;Philipp
        Koehn,&nbsp;ISBN-10: 1108497322,&nbsp;Publisher: Cambridge
        University Press. preprint version available <a
          moz-do-not-send="true" href="https://arxiv.org/abs/1709.07809">online</a>.<br>
      </li>
      <li>(Optional) Deep Learning,
        <meta charset="utf-8">
        Ian Goodfellow and Yoshua Bengio and Aaron Courville, Publisher:
        MIT Press. available <a moz-do-not-send="true"
          href="https://www.deeplearningbook.org/">online</a>.</li>
      <li>(Optional) Dive into Deep Learning, Aston Zhang, Zachary
        Lipton, Mu Li, Alexander Smola. available <a
          moz-do-not-send="true" href="https://d2l.ai/">online</a>. <br>
      </li>
      <li>(Optional) Linguistic Fundamentals for Natural Language
        Processing: 100 Essentials from Morphology and Syntax. Emily
        Bender. Publisher: Morgan &amp; Claypool.<br>
      </li>
    </ul>
    <h2>Prerequisites</h2>
    <p>
      <meta charset="utf-8">
      Prerequisites:&nbsp;130A or 130B;&nbsp;165A or 165B.<br>
    </p>
    <h2>Grading</h2>
    <ul>
      <li>Homework<br>
      </li>
      <ul>
        <li>HW1-3: separate assignments (10% each)</li>
        <li>HW4: In-class Presentation <a 
            href="LanguagePresentation.html">Language in 10 mins (10%)<br>
          </a></li>
        <li>HW5: MT Blog (15%)</li>
        <li>Turn-in homework at Gradescope: <a 
            href="https://www.gradescope.com/courses/319418">https://www.gradescope.com/courses/319418</a><br>
        </li>
      </ul>
      <li>Project: 40%</li>
      <li>Participation in active discussion (5)<br>
      </li>
    </ul>
    <h2>Discussion Forum</h2>
    <p>Piazza: <a 
        href="https://piazza.com/class/ksousnwx3cl1ux">
        https://piazza.com/class/ksousnwx3cl1ux </a><br>
        Piazza is main channel of communication. Questions can be posted and discussed here. 
    </p>
    <h2>Policy</h2>
    <p>Please read the following <a 
        href="course_policy.html">Link</a> carefully!<br>
    </p>
    <h2>Syllabus</h2>
    <table>
      <thead> <tr>
          <td>#<br>
          </td>
          <td>Date<br>
          </td>
          <td>Lecture Topic<br>
          </td>
          <td>Reading<br>
          </td>
          <td>Homework<br>
          </td>
        </tr>
      </thead> <tbody>
        <tr>
          <td>1<br>
          </td>
          <td>M 9/27<br>
          </td>
          <td><a href="lecture1intro.pdf">Introduction to MT, History, Probability, Statsitical MT</a><br>
          </td>
          <td><a moz-do-not-send="true"
              href="Weaver_1949_Translation.pdf">Weaver 1949</a>, <a
              moz-do-not-send="true"
              href="https://aclanthology.org/J93-2003.pdf">Brown 1993</a>
          </td>
          <td>HW1</td>
        </tr>
        <tr>
          <td>2<br>
          </td>
          <td>W 9/29<br>
          </td>
          <td>Data, Vocabulary and Evaluation<br>
          </td>
          <td><a moz-do-not-send="true"
              href="https://aclanthology.org/P02-1040/">Papineni 2002
              BLEU</a><br>
            <a moz-do-not-send="true"
              href="https://aclanthology.org/W18-6319/">Post 2018
              SacreBLEU</a><br>
            <a moz-do-not-send="true"
              href="https://arxiv.org/abs/2104.14478">Freitag 2021 Human
              evaluation</a><br>
            <a moz-do-not-send="true"
              href="https://aclanthology.org/P16-1162/">Sennrich 2016
              BPE</a><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>3<br>
          </td>
          <td>M 10/4<br>
          </td>
          <td>Basic Neural Network Layers, Embedding, Feedforward,
            Softmax<br>
          </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>4<br>
          </td>
          <td>W 10/6<br>
          </td>
          <td>CNN, LSTM, Encoder-decoder <br>
          </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>5<br>
          </td>
          <td>M 10/11<br>
          </td>
          <td>Transformer </td>
          <td><br>
          </td>
          <td>HW1 due, HW2<br>
          </td>
        </tr>
        <tr>
          <td>6<br>
          </td>
          <td>W 10/13 </td>
          <td>Pre-training Language Models, BERT, GPT, T5<br>
          </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>7<br>
          </td>
          <td>M 10/18 </td>
          <td>Learning with monolingual data, Back translation, Dual
            Learning, Mirror Generative Model<br>
          </td>
          <td><br>
          </td>
          <td>Project Proposal Due </td>
        </tr>
        <tr>
          <td>8<br>
          </td>
          <td>W 10/20 </td>
          <td>Low-resource and Unsupervised NMT, Incorporating
            pre-trained language model into NMT&nbsp; </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>9<br>
          </td>
          <td>M 10/25 </td>
          <td>Multilingual NMT, Pre-training, Augmentation,&nbsp; </td>
          <td><br>
          </td>
          <td>HW2 due, HW3 </td>
        </tr>
        <tr>
          <td>10<br>
          </td>
          <td>W 10/27 </td>
          <td>Improving Structure Capacity, continual learning, adapter,
            language-specific sub-network </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>11<br>
          </td>
          <td>M 11/1 </td>
          <td>Data mining for MT</td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>12<br>
          </td>
          <td>W 11/3 </td>
          <td>Speech Translation </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>13<br>
          </td>
          <td>M 11/8 </td>
          <td>Pre-training for Speech Translation </td>
          <td><br>
          </td>
          <td>HW3 due<br>
          </td>
        </tr>
        <tr>
          <td>14<br>
          </td>
          <td>W 11/10 </td>
          <td>Speed-up inference: Parallel Generation </td>
          <td><br>
          </td>
          <td>Project midterm report due </td>
        </tr>
        <tr>
          <td>15<br>
          </td>
          <td>M 11/15 </td>
          <td>Scaling Training and Hardware acceleration for NMT</td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>16<br>
          </td>
          <td>W 11/17 </td>
          <td>Advanced vocabulary and embedding learning </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>17<br>
          </td>
          <td>M 11/22 </td>
          <td>Interactive Machine Translation, Computer-aided
            Translation</td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>18<br>
          </td>
          <td>W 11/24 </td>
          <td>Applications to other tasks, drug discovery, material </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>19<br>
          </td>
          <td>M 11/29 </td>
          <td>Industrial Presentation: Lessons from Building Successful
            MT Products </td>
          <td><br>
          </td>
          <td><br>
          </td>
        </tr>
        <tr>
          <td>20<br>
          </td>
          <td>W 12/1 </td>
          <td>(tentative) Project Poster Presentation</td>
          <td><br>
          </td>
          <td>Final project report due </td>
        </tr>
      </tbody>
    </table>
    <br>
  </body>
</html>
